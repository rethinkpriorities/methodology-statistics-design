---
title: "Vignette: General (non-ML 'regression') modeling workflow"
format:
  html:
    theme: cosmo
    code-fold: true
    code-tools: true
    toc: true
    number-sections: true
    citations-hover: true
    footnotes-hover: true
execute:
    freeze: auto # re-render only when source changes
    warning: false
    message: false
    error: true
comments:
    hypothesis: true
---



```{r, include=FALSE}


#or instead source(here("code", "methods_setup.R"))

library(here)
library(pacman)
library(dplyr)

p_load(rsample, install = FALSE) # Why p_load these? (Can create issues with dependencies)
p_load(ggplot2, forcats, install = FALSE)
p_load(parsnip, tune, dials, install = FALSE) # If loading multiple packages, better to use p_load with install = FALSE option
p_load(conflicted, workflowsets,  install=FALSE)
p_load(purrr)
library(stringr)
p_load(kableExtra, janitor, install=FALSE)

p_load(dynverse, install=FALSE)

conflicted::conflict_prefer("set_label", "sjlabelled")
conflicted::conflict_prefer("add_footnote", "kableExtra")
conflicted::conflict_prefer("sample_n", "tidylog")

set_label <- sjlabelled::set_label

filter <- dplyr::filter
ungroup <- dplyr::ungroup
mutate <- dplyr::mutate
  
p_load(devtools)
source_url("https://raw.githubusercontent.com/daaronr/dr-rstuff/master/functions/baseoptions.R")


#github api yelled at me, and I had to do this: https://gist.github.com/Z3tt/3dab3535007acf108391649766409421

devtools::install_github("rethinkpriorities/rp-r-package", force = TRUE)
library(rethinkpriorities)

devtools::install_github("rethinkpriorities/r-noodling-package")
library(rnoodling)

```

# Introduction


This vignette 

- shows how I define, build, and report (tables and plots) a set of 'models' in a (fairly) tidy, organized way  ...  

-  for the simple intuitive/ad-hoc descriptive and causally suggestive models I did for the of the 2020 [EA Forum post](https://forum.effectivealtruism.org/posts/nb6tQ5MRRpXydJQFq/ea-survey-2020-series-donation-data) and [chapter](https://rethinkpriorities.github.io/ea_data_public/eas_rps_own_style/eas_donations.html)  and other EA forum posts posts


- As a code example 
     - How the computation works and 'what is producing what' ... so that you can recreate it in your own context


- To give a little bit of insight into the modeling choices and approaches.

## Reading in data from a repo

The code below reads data in directly from the RP private github repo. You need to have authorization set up for this to work.^[Ideally, for replicablity, one reads data in directly from it's earliest source, such as an API for the survey site where it was hosted.]

```{r input, include=FALSE}

df <- rethinkpriorities::read_file_from_repo(
  repo = "ea-data",
  path = "data/edited_data/eas_all_s_rl_imp.Rdata",
  user = "rethinkpriorities",
  token_key = "github-API",
  private = TRUE
)

```


Next we sample from this data and remove labels, to make the process quicker and easier.

```{r simplify_for_tidymodel}

df <- df %>%
  labelled::remove_attributes("label") %>%  # Labels don't work with tidymodels :/, sadly
  ungroup() %>%
dplyr::sample_n(2000)

```


# Modeling discussion 


::: {.callout-note collapse="true"}

## Brief: our descriptive model 'selection'

In our *descriptive* modeling we do *not* remove 'insignificant' features from the model (as in stepwise regression), nor do we shrink the coefficients towards zero (as in Ridge and Lasso models). Under the assumptions of the classical linear model and its simple extensions the coefficients we present here would be unbiased or consistent. (However, we admit that the strong assumptions of this model, particularly those embodying exogeneity, are likely to fail in important ways in the current non-experimental setting.)
:::

::: {.callout-note collapse="true"}

## We retain those features of most direct interest

... (such as 'how introduced to EA') and/or theoretical importance (such as income),
^[Classical economic theory suggests that most goods are 'normal', with the amount consumed increasing in income. Empirical evidence supports this for charitable giving; [recent work](https://econofact.org/are-rich-people-really-less-generous) suggests that *share of income* is relatively constant across the income distribution, implying that wealthier people give more in absolute terms.] and 'controls' (especially time-in-EA and survey-year) that might allow us to better interpret the features of interest.^[See further discussion in post.]

:::

# Choosing features and modeling targets, defining these lists/objects {.unnumbered}

**We focus on three key outcomes:**

1.  Amount donated (converted to US dollars)^[Here we focus on the average of last-year's and next year's donation for each individual, where both are present, and otherwise we use whichever one is present.... Further discussion in post.]

2.  Donation as a share of income^[Where income is missing or where income was reported as 0 we impute it based on student status and country.]

3.  Whether donated more than 1000 USD

**We construct several 'feature sets':**

-   "Key demographics, student status, and geography", used in all models 
-   "Career/Economics": (Income, employment status, top-6 university)

<!-- ^["Top-6 university" refers to whether the individual lists any of the six universities (Oxford, Stanford, Harvard, CalTech, MIT, Cambridge) appearing in the top-10 of all of USNWR, QS, and THE rankings. However, university was not asked in the 2018 survey ] -->
    
-   "Pledges/commitments:" Whether ever taken a 'Giving What We Can Pledge', whether 'Earning to Give' <!-- feat_gwwc_etg -->

-   "Controls" for age, time-in-EA, and survey-year (used in all models)
^[We refer to the latter as "controls" because they aid our interpretation of other features of interest, as noted above. However, these are *also* of independent interest.]



**In the code below, we define these as objects!**

We define the lists of the different 'features' we care about as character vectors, to put into the models later. The idea is 'all decisions are specified and discussed up top', for better organization and control.

First we define the 'targets': the binary outcomes, numerical outcomes, all outcomes, and a subset of these outcomes for more involved analyses. 

```{r targets, warning=FALSE}

#targets:
bin_out <- c("d_don_1k", "d_don_10pct")

num_out <- c('donation_usd', 'don_av2_yr', 'l_don_usd', "l_don_av_2yr", "don_share_inc_imp_bc5k", "donation_plan_usd")
targets <- c(bin_out, num_out)
targets_short <- c("don_av2_yr", "don_share_inc_imp_bc5k", "d_don_1k") 

#Note -- don_av2_yr is the right one for qpoisson as it already expresses things in exponents. l_don_av2_yr was the one to use in the loglinear model, which we are not emphasizing

targets_short_names <- c("Log (Avg don +1)", "Don/Income", "Donated 1k+")
```

Next, we define  the 'features of interest' and the 'controls'

```{r}

#features and controls
geog <- c("where_live_cat", "city_cat")
key_demog <- c("ln_age", "not_male_cat", "student_cat", "race_cat", geog)
key_demog_n <- c("age_d2sd", "not_male_cat", "student_cat", "race_cat", geog)

feat_income_employ <- c("ln_income_c_imp_bc5k", "d_pt_employment", "d_not_employed", "d_top6_uni")

#Note -income_c_imp_diqr has been adjusted to  with 5k minimum

feat_income_employ_n <- c("income_c_imp_diqr", "d_pt_employment", "d_not_employed", "d_top6_uni")


feat_gwwc_etg <- c("d_gwwc_ever_0", "d_career_etg")

controls <- c("ln_years_involved", "year_f") #note this assumes those 2009 or earlier started in 2009

controls_n <- c("years_involved_d2sd", "year_f") #note this assumes those 2009 or earlier started in 2009

robust_controls <- c("ln_years_involved_post_med",  "ln_age_if_older", "ln_income_c_imp_if_richer")

robust_controls_n <- c("years_inv_d2sd_post_med",  "age_d2sd_post_med", "income_c_imp_diqr_if_richer")



```


```{r key_eas_all_labels}

key_eas_all_labels <- c( #note these are converted to a list with as.list before assigning them
    donation_usd = "Donation (USD)",
    l_don_usd = "Log Don. (USD)",
    l_don_av_2yr = "Log Don. 'avg.'",
    ln_age = "Log age",
    don_av2_yr = "Don. 'avg'",
    donation_plan_usd = "Don. plan (USD)")
```

# Ad-hoc data cleaning, recipes

We impute a few missings, etc, just for modeling.^[Use `recipe` package for this if it makes it more organized, but it's not necessary to use recipe if we are not doing machine learning, as leaks are not an issue, and we only need to do the imputation once. On the other hand, in non-prediction models we have a (bad?) tendency to assign a causal interpretation to particular features, and we need to be careful about how imputation might affect this.]

Below, some imputations and constructed features (for functional form interpretation) that I did in the 'actual analysis'.


```{r impute_norm_features, warning=FALSE}

#We impute variables where missing and normalizing all variables to be mean-zero and to be on the same scale.

year <- 2020 

diqr <- function(x) {
  (x - mean(x, na.rm=TRUE))/IQR(x, na.rm=TRUE)
}

gtmed <- function(x) {
  x*(x>med(x))
}

eas_all_s <- df %>%
  mutate(
    #(re) code scaling and 2-part splits for the modeling sample (2018-20, reporting donations)
    age_d2sd = arm::rescale(age), #Todo (automate this with `mutate(across)` thing)
    years_involved_d2sd = arm::rescale(years_involved),
    years_inv_d2sd_post_med = gtmed(years_involved_d2sd),
    income_c_imp_diqr = diqr(income_c_imp_bc5k),
    income_c_imp_diqr_if_richer = gtmed(income_c_imp_diqr),
    ln_income_c_imp_if_richer= gtmed(ln_income_c_imp_bc5k)
  ) %>%
  rowwise() %>%
  mutate(don_share_inc_imp_bc5k = min(don_share_inc_imp_bc5k, 1)) %>%  #recode about 84 values so the range is between 0-1 for frac. logit to work
  ungroup() %>%
  dplyr::select(all_of(c(num_out, bin_out, controls, key_demog, feat_income_employ, feat_gwwc_etg, robust_controls)),
                income_c, income_c_imp, income_c_imp_bc5k, income_c_imp_diqr, income_c_imp_diqr_if_richer, first_hear_ea_lump, years_involved, age,
                contains("d2sd"), contains("iqr")) %>% #I have added first_hear_ea_lump back even though we don't use it here because we want to use it in ML; I hope it doesn't mess anything up
  # years_involved included to put in sumstats
  labelled::set_variable_labels(.labels = as.list(key_eas_all_labels), .strict=FALSE)


  #Recode missing as 0 for all dummies, as a 'NA category' for categoricals
  #also for normalized variables; i.e., set missings to the mean
eas_all_s_rl <- eas_all_s %>%
    mutate(across(matches("d_not_just_white"), missing_to_zero))

eas_all_s_rl_imp <- eas_all_s_rl %>%
      mutate(across(matches("d2sd|diqr"), missing_to_zero)) %>%
    labelled::set_variable_labels(.labels = as.list(key_eas_all_labels), .strict=FALSE)
#TODO: (future) -- check for sensitivity to this imputation vs dropping these obs


```


## Peeking at data for sanity checks 

We report summary statistics on a selection of these features and target outcomes below...


```{r don_inc_career_tab}
(
  don_inc_career_tab <- eas_all_s_rl_imp %>%
            filter(!is.na(don_av2_yr)) %>%
      mutate(`Earn-to-give` = as.factor(d_career_etg),
           `GwwC` = as.factor(d_gwwc_ever_0)) %>%
  ungroup() %>%
 dplyr::select(starts_with("don"), starts_with("d_don"), starts_with("inc"), -income_c_imp_diqr,
               d_pt_employment, d_not_employed,
               `Earn-to-give`, `GwwC`) %>%
  dplyr::select(-starts_with("l_"), -d_don_10pct, -ends_with("d2sd"),
                -matches("_if_|_post_")) %>%
    .summ(title = "Donations, income, career, most recent years,  (subset: reporting 0+ donation)",
          digits = c(0,0,1,1,2,1),
          labels=TRUE,
          logical.labels = c("No", "Yes"),
          factor.counts = FALSE,
           out="kable") %>%
  kable_styling()
)

```

Above, we report donations, income, career, and GWWC pledge rates. For an actual analysis, I would report a bit more here.


# Constructing and specifying models {.unnumbered}


For simplicity, I will focus on *linear* models, at least for now, although our target outcomes don't really warrant this.


### Construct lists of features etc. {-}

We construct a list (of lists) of features to use in a few models. We consider three sets of features:

1. Baseline, 
2. with Robust controls
3. Baseline with Earning to Give and GWWC

```{r feat_list}
# Define models (For LINEAR models ... used only in appendix, but some of these are reused in other models)
#-----------------------------------------------------

feat_list = list(
  c(key_demog, feat_income_employ, controls),
  c(key_demog, feat_income_employ, controls, robust_controls),
  c(key_demog, feat_income_employ, feat_gwwc_etg, controls) )

feat_names = c("Baseline", "Robust controls",  "Base + EtG & GWWC")
```

The first 2 elements of this list of lists of features is displayed below

```{r}
feat_list[1:2]
```

Next we define some more objects:

`rhs_vars_list`: repeating the feature list to have all three lists for all three outcome variables; a total of 9 lists.
`outcome_vars_list`: repeating the outcome variables to have all for all feature lists 
`dfs`: ... the data frame 9 times

```{r vars_list}

rhs_vars_list <- rep(feat_list, length(targets_short))

outcome_vars_list <- rep(as.list(targets_short), each=length(feat_list))

dfs <- rep(list(eas_all_s_rl_imp), length(outcome_vars_list))

```



### Make 'model data frames'

Using the `make_model_df` command, put all the above to make a 'model dataframe', which is a tibble of tibbles and lists ... specifying the outcome, the linear model formulas, and the data frames. 

```{r model-prep-linear, warning=FALSE}

## Create dataframe for modeling
(linear_models <- make_model_df(rhs_vars_list, outcome_vars_list, dfs))

#For example ` linear_models[2][[1,1]]` is the formula `r  linear_models[2][[1,1]]` 

```


### Fit linear models

```{r fit_linear_models}

linear_models <- linear_models %>%
  mutate(
    lm_fit = fit_models(
      linear_models, "formulas", "dfs", fun = fit_lm)
    )
#warning `using type = "numeric" with a factor response will be ignored‘-’ not meaningful for factor`
# @DR: Why are these models being fit on binary outcomes? DR, @OM: It is fit on all the outcomes including the binary ones, no? However, we haven't reported it yet. Anyways, I think there is still a norm of considering 'linear probability models' in Economics, and arguments on its behalf, at least as a robustness check.

# Extract coefficients, fitted and residuals
model_feat_names <- rep(c(feat_names), times= length(targets_short))
model_oc_names <- rep(c(targets_short_names), each= length(feat_names))
model_names <- paste(model_oc_names, model_feat_names, sep = ": ")

```



# Report model results

(tables and plots of results; latest years combined) 

We put together forest plots of (normalized) coefficients from the distinct set of models outlined above, where these can be compared on the same scales. Specifically, we consider,

-   for each of the three key outcomes ('amount donated (averaged)',
    'donation as a share of income', 'donated over 1000 USD'),
-   models with three specific sets of features, yielding nine models in total (plus robustness checks in the appendix).

The feature sets, which we will refer to in the forest plots below, are:

