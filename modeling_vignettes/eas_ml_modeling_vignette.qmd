# Setup 

## Packages and functions

Reinstall key packages so this can be standalone?^[Note: not all these packages may be needed; let's search only the ones that are and drop the rest]

```{r, include=FALSE}

library(here)
library(pacman)
library(dplyr)

p_load(rsample)
p_load(parsnip, tune, dials)
p_load(conflicted)
conflicted::conflict_prefer("set_label", "sjlabelled")
conflicted::conflict_prefer("add_footnote", "kableExtra")
conflicted::conflict_prefer("sample_n", "tidylog")

set_label <- sjlabelled::set_label

filter <- dplyr::filter
ungroup <- dplyr::ungroup
mutate <- dplyr::mutate

source(here("code", "packages.R"))
#github api yelled at me, and I had to do this: https://gist.github.com/Z3tt/3dab3535007acf108391649766409421


devtools::install_github("rethinkpriorities/rp-r-package", force = TRUE)
library(rethinkpriorities)

devtools::install_github("rethinkpriorities/r-noodling-package")
library(rnoodling)

# No longer needed because it's all in RP nowsource(here("code", "modeling_functions.R")) 


```


:::{.callout-note collapse="true"}

## Skipping here, for now; parallel cores, etc. 

Skipping here, for now; this may help processing larger jobs, so we may want to revisit it.

```
cores <- parallel::detectCores()
if (!grepl("mingw32", R.Version()$platform)) {
  library(doMC)
  registerDoMC(cores = cores)
} else {
  library(doParallel)
  cl <- makePSOCKcluster(cores)
  registerDoParallel(cl)
}
```

:::



## Reading in data from a repo

The code below reads data in directly from the RP private github repo. You need to have authorization set up for this to work.^[Note this data was already edited in an analysis file -- we should move that over to one of the R 'build files']

```{r input, include=FALSE}

df <- rethinkpriorities::read_file_from_repo(
  repo = "ea-data",
  path = "data/edited_data/eas_all_s_rl_imp.Rdata",
  user = "rethinkpriorities",
  token_key = "github-API",
  private = TRUE
)

```


Next we sample from this data and remove labels, to make it process quicker and easier

```{r simplify_for_tidymodel}

df <- df %>%
  labelled::remove_attributes("label") %>%  # Labels don't work with tidymodels :/, sadly
  ungroup() %>%
dplyr::sample_n(2000)

```


```{r}

df

```

## Some ad-hoc data cleaning and filtering, to enable modeling

Normally, we should do the main data manipulation steps separate from the analysis.^[In the present case this would be in `build/eas_cross_year_harmonisation.R`.] However, if there are processing steps that are very specific to a modeling exercise, it seems OK to do them at this stage; but we should remain vigilant.^[This does not include data-based imputation and standardizing variables, which are done with the `recipe` package below.]

First we remove columns with only missing values, to clean up what we have to consider, and because this may mess up processing.^[However, this is a case where this 'might as well have been done at the 'build the data' stage ... so I'll move it there.]

```{r}

# Remove variables (columns) that are all missing
df <- df %>% dplyr::select(where( ~!all(is.na(.x)) ))  %>%
  ungroup() 

```


Next, another 'Big global filtering step' ... .^[This should remove any people who did not answer the donation question... on which (all our) outcome variables are based. But actually, something is weird here -- it removes very few observations, so I'm not sure what is going on, need to doublecheck it], ^[Of course, as the decision to answer the donation question is nonrandom, this is not uncontroversial. There may be some selection issues here. Alternate models could consider, e.g., 1. Non-responses as zeroes, 2. The binary outcome 'reported a positive donation', 3. Some explicit selection model.]

```{r no-missing-don}

df <- df %>% 
  dplyr::filter(!is.na(d_don_1k))
```


Next, I define an 'optional filter', which we may use below.^[I define this 'filter object' using fancy 'quosure' syntax here, rather than actually creating another slice of the data as an object. This is in part to avoid clutter.] `income_filter` will remove individuals with an income below 500000.^[`income_c_imp_bc5k` actually does a particular imputation for missing or near-zero incomes. I use this imputed variable here for consistency, but it is not important here, as we are removing the *highest* income people only.]

```{r}

income_filter <- quo(income_c_imp_bc5k < 500000)

```


:::{.callout-note collapse="true"}

## I often like to 'set modeling choice objects up top' 

Specifying character vectors or list objects (or doing this in sourced scripts). 

This usually includes...

- 'filters' or slices of the data, and 
- choices of variables (columns) to include in different specifications of models ... perhaps as a list object.

Why 'do this up top'? It makes it clearer what 'all the messy code below does', and it gives you 'central control', allowing you to change everything in one place.  And you can also refer to these objects in many ways, including in inline text an in visualizations, to communicate what is being done. 

Aside: The 'choices of variables' thing is more important in models based on a particular theoretical structure, where we have different sets of pre and post variables and are worried about 'leakage' ... or where we are aiming at a causal interpretation and are worried about things like  'bad controls/colliders'. 
:::


Finally, for factor variables, we set base categories to the most common values.^[This could have been done in the building process, but different procedures and reporting may want to use these categories differently, so it's OK to do it in the moment.]


```{r}

df <- df %>% mutate(
  across(where(is.factor), #note the `across(where...` means 'do the function for all columns where the condition holds, and change the value, keeping the column name constant'
    ~ forcats::fct_infreq(.x))) # here's the abbreviated notation for a function, with a tilde ... hte `.x` refers to the object that is the function argument, here a column (name)

```

# Machine learning environment 'splits' {#mlsplits}

## Create training and test data 


Before we start, a 'seed' so we get the same 'random draw' whenever we run this stuff.

```{r}
seed <- 1
set.seed(seed)
```


For ML modeling, we need to setaside some data to test our model's ultimate performance. We thus 'split' our data sets using the `rsample` package tools.   3/4 of data is used for 'training' (fitting the parameters of our model), and the rest for testing.^[Why this particular split? I'm not sure, maybe it's a rule of thumb.]

```{r init_split}

init_split <- rsample::initial_split(df, prop = 3/4) #3/4 of data goes for training, rest for testing

```


:::{.callout-note collapse="true"}

## What kind of object is `init_split`? 

Typing it into the console just reports the counts of observations in the training and testing sets. If we look into its structure with `init_split %>% str()` we see a list of four things; 

`data`  is the dataframe itself, 
  
`in_id` seems to keep an indexing record of the training data, 
  
`out_id` is empty, and 

`id` seems to keep track of 'what this thing is' (but I'm not sure)

:::

Next, we assign the training data and testing data objects

```{r train-test}

train <- rsample::training(init_split)
test <- rsample::testing(init_split)

```

I next create the 'non-highest-income' subset of the data, using the `income_filter` quosure object defined above to the testing and training data.^[Consider: in some cases we might want to filter *before* making training/test, but there are pros and cons] 

```{r train-test_filter}

train_filter <- train %>% filter(!!income_filter)
test_filter <- test %>% filter(!!income_filter)

```

In the full model we compare both the unfiltered and filtered data. For simplicity, I'll use only the latter here.^[Why this filter? I suspected that our methods may be sensitive to outliers, particularly with some of the functional forms. I could instead try to adjust the *method* of course, considering the real criteria of interest. This may be just an interim solution.] 

## Create partitions (folds) for validation 'tuning' step


```{r}
# Cross validation splits ####

cv <- rsample::vfold_cv(train) # 10 fold
cv_filter <- vfold_cv(train_filter)

```


In a machine learning context, especially if we care only about prediction for its own sake, 'more features (columns) always help' ... as long as these represent pre-outcome data and not a 'leak'.

:::{.callout-note collapse="true"}

## ML, overfitting, regularization, tuning

In standard econometrics 'OLS' approaches, we learn that including more variables may *worsen* a model's predictive power (out-of-sample fit). However, in  ML contexts, to avoid this 'overfitting', we impose a 'penalty' on the  (normalized) coefficient magnitudes (absolute value, squared or both). We also 'tune' this penalty to achieve the best performance in the cross-validation partitions. If this is done right, more columns can only help our predictive power, achieving athe model that 'predicts best out of sample'. However we *need to be very careful in interpreting these coefficients*.
:::

So we do 'regularization' ('trimming', 'penalization', etc.) to avoid 'overfitting'.

:::{.callout-note collapse="true"}

## But how much of this regularization is optimal, and what is the best way of doing it? 

It seems to depend on the situation and structure of the data generating process. ML procedures try to compute the best-performing regularization 'hyperparameters' *using the data*. But (again because of the overfitting issue) we can't judge the performance of an approach by predicting on the same data that it fit the model on. **n-fold cross-validation** tries to get around this problem by splitting the data up into a number of partitions, fitting the model, with a certain tuning parameter, on 'all but one fold', and then testing it's performance on that left-out fold. 

This is iterated many times until we are getting good enough performance, i.e., we converge (?) on what seems to be the near-best set of tuning hyperparameters,
:::

So, we use this n-fold cross-validation to 'tune our parameters' to achieve the best tradeoffs between complexity and overfitting, to optimize our prediction. To enable this, we also need to define 'folds' to split up the training data, which we do below.

The `rsample::vfold_cv` function does this resampling (with a default of ten folds).^[Why ten? Maybe that's a conventional rule of thumb?] 

```{r}

cv <- rsample::vfold_cv(train) # 10 fold
cv_filter <- vfold_cv(train_filter)

```

It creates ten partitions (data frames with some indexing objects and meta-data on the process)

```{r cv-show}

cv

cv[[1,1]]

```

Each of these classes 90% of the data for 'analysis' and about 10% for 'assessment':



# Defining and creating the formula objects ('equations' to model)

First I make a list of 'right hand side variables' here,^[I previously labeled these `control_vars`. But 'control variables' has a particular meaning in the context of causal inference and interpretation. In other work that *is* aiming at causal interpretation, I separate `control_vars` from the key variable of (causal) interest. But here 'they are all equal' so I just call it `rhs_vars`] columns being used in prediction, to shorten later code, avoiding duplication, and centralize things.^[I know I said this is good to do 'at the top', but this is the part where we are starting to actually get into the modeling, so it seems OK.]

```{r rhs_vars}

rhs_vars <- c("ln_years_involved", "year_f", "ln_age", "not_male_cat", "student_cat", "race_cat", "where_live_cat",
                  "city_cat", "d_pt_employment", "d_not_employed", "d_career_etg", "ln_years_involved_post_med", "ln_income_c_imp_bc5k",
                  "first_hear_ea_lump")

```



Next we create three formula objects,^[It might be more tidy to do this imputting a list of outcome variables too, and generating a list of formulas to use later.] each with the same set of rhs variables, but a different 'outcome' variable.^[`make_formula` uses `stats::reformulate` to collapse lists into a formula object.] 

:::{.callout-note collapse="true"}
## Outcomes and models

Each of these outcomes has a different character:
`l_don_av_2yr_f`: a strictly positive continuous variable (log average donation) 
`don_share_inc_imp_f`: donation as a share of (imputed) income, between 0 and 1
`d_don_1k`: A binary for 'whether donated \$1000 or more. 

These will suggest different modeling approaches and interpretations.

:::

```{r}

##Consider: -- Medium importance: Consider constructing this starting with lists defined in donations_20 and adding/subtracting things (but could this cause problems?)


l_don_av_2yr_f <- rethinkpriorities::make_formula("l_don_av_2yr", rhs_vars) #shortcut for stats::reformulate to make a formal out of rhs and lhs

don_share_inc_imp_f <- make_formula("don_share_inc_imp_bc5k", rhs_vars)

d_don_1k_f <- make_formula("d_don_1k", rhs_vars)

```


These look like ... 

```{r}
l_don_av_2yr_f
```

etc.

## 'Recipes' for imputing and standardizing data in an ML environment 

As noted, in a machine learning context, especially if we care only about prediction for its own sake, 'more features (columns) always help'  Thus, we try to fit a model that allows many, many variables, imputing these where there are missing values.^[Imputing them based on non-leaky variables only, of course, not based on any post-outcome measures.] This should get us the model that 'predicts best out of sample'. However we *need to be very careful in interpreting these coefficients*.

For ML procedures to work, we almost always need to standardize each continuous (predictor) column, subtracting its mean and dividing by it's estimated standard deviation.^[This is basically because we need to 'penalize each coefficient fairly'. Note that we can also divide by any scalar for interpretation, as long as we do this the same for all columns. This comes up later.]


**Recipe package:** We can't merely 'do all the imputing and standardation on the full data set once'; this would not yield valid metrics for our tuning. Because of the nature of the cross-validation procedure, we need to do the above steps *separately for each iteration*, imputing and standardizing *using only the non-excluded observations in the partition* (or 'fold').

```{r}


preprocess_func <- function(formula, data = train){
  require(recipes)

  # Function to save time in creating recipes for different outcomes
  #? Todo -- add to rethinkpriorities package
  
  recipes::recipe(formula, data=data) %>% #formula is a 'y~x1+x2` thing, defining rhs and lhs variables
    recipes::step_impute_median(all_numeric_predictors()) %>% #rem: replaces missing values with medians

    # Create NA feature
    recipes::step_unknown(all_nominal_predictors()) %>%
    recipes::step_scale(all_numeric_predictors(), factor=2) %>% #the 2sd Gelman adjustment
    #Removing because redundant: step_impute_mode(all_nominal_predictors(), -all_outcomes()) %>%
    recipes::step_zv(all_predictors()) %>% #cut any predictors with zero variance
    recipes::step_dummy(all_nominal_predictors())
}
```




