# Donations {#eas_donations}

**Note**: This is largely meant as a year-independent template. '...' indicates a place we may want to fill in more details^[either merging in a sentence from a previous post, adding an updated link, or briefly characterizing the result 'manually'.] See our discussion of what should go in each chapter [here](https://github.com/rethinkpriorities/ea-data/issues/49). Note that we aim to keep the commentary here very limited. The EA forum post will do a bit more commenting... feel free to insert, but hide any notes on this output that we may want to integrate into the EA forum post. We will try to move all 'generic' statistical and methods discussions to our [methods discussion book](https://rethinkpriorities.github.io/methodology-statistics-design/introduction-and-overview.html), and link this here.

<!-- cut chunks up top because these are already in index -->

This report is summarized and discussed in the EA forum post ...

::: {.rmdnote}
**Linked to forum post**: [Link](https://forum.effectivealtruism.org/posts/nb6tQ5MRRpXydJQFq/ea-survey-2020-series-donation-data); EA Survey `r year_n` Series: Donation Data
:::

```{r plots_folder}

# Folder to save plots in
plots_folder <- here("analysis", "plots", "donations_20")

```


*I'd love your feedback  :*

::: {.foldable}


I'd love to get your feedback on this report. (cut ...)

Ways of leaving feedback.

Connection to EA forum post described...
:::

<!-- bookdown_end -->

## Introduction and summary {#don_introduction}

Very brief:  Statement about the importance of charitable giving and the data collection here. (cut ...), what we present, ...
linked outline here, e.g.,

...

-   the [total magnitude of EA giving](#total_mag) and its relationship
    to non-EA giving, ...
-   [EA's donation plans versus realized donations](#plan-actual) (and
    [future plans](#next-year)).

Our [modeling work](#modeling) work considers how donations (total,
share-of-income, and 'donated over 1000 USD') jointly relates to a range
of characteristics. We present:

-   ['descriptive' results](#descriptive): focusing on
    [demographics](#demog_mod), [employment and careers](#mod_emp), and
    the 'continuous features' age, time-in-EA, income, and year of
    survey.

-   ['Predictive' models](#predictive-models), allowing the 'machine learning'
    models themselves to choose which features seem to be most important
    for predicting donations.

<!-- Note to add to the EA forum post here on the existence of this bookdown -->

::: {.rmdnote}
In the narrative below, we simply refer to "donations" rather than "reported donations" for brevity. Unless otherwise mentioned, all figures simply add, average, or otherwise summarize individual responses from the EA Survey years mentioned.
:::

```{=html}
<!--
# Scales, breaks, and theme moved to code/plotting_functions_parameters_eas.R, sourced in main.R
-->
```
```{=html}
<!--
-->
```
```{r}

# Construct charity-specific aggregations
# TODO: do for eas_all, move to build side

eas_new <- eas_new %>%
  mutate(
    num_named_dons  = rowSums(!is.na(select(., one_of(all_chars)))),
    dev_don = rowSums(across(all_of(dev_health_chars)), na.rm = TRUE),
    d_dev_don = dev_don > 0,
    animal_don = rowSums(across(all_of(animal_chars)), na.rm = TRUE),
    d_animal_don = animal_don>0,
    ea_meta_don = rowSums(across(all_of(ea_meta_chars)), na.rm = TRUE),
    d_ea_meta_don = ea_meta_don>0,
    lt_ai_don = rowSums(across(all_of(lt_ai_chars)), na.rm = TRUE),
    d_lt_ai_don = lt_ai_don>0,
    other_don = rowSums(across(all_of(other_chars)), na.rm = TRUE),
    d_other_don = other_don>0
    ) %>%
  mutate_at(.vars =where_don_vars,
            funs(ifelse(num_named_dons==0, NA, .))
         )

eas_new %<>% labelled::set_variable_labels(.labels = as.list(all_char_labels), .strict=FALSE)

```

```{r key-don-statistics }

pct_tot <- function(x, df=eas_new) {
  x/NROW(df)*100
}

num_don <- sum(eas_new$donation_c>0, na.rm=TRUE)
num_na_don <- sum(is.na(eas_new$donation_c))
zero_don <- sum(eas_new$donation_c==0, na.rm=TRUE)

tot_don <- sum(eas_new$donation_c, na.rm=TRUE)

#for all years, for USA nonstudents only
tot_don_all_usa <- sum(eas_all$donation_c[eas_all$d_live_usa==1 & eas_all$d_student==0], na.rm=TRUE)

tot_inc_all_usa <- sum(eas_all$income_c_imp_bc5k[eas_all$d_live_usa==1 & eas_all$d_student==0], na.rm=TRUE)

tot_share_don_us_nonstudent <- tot_don_all_usa/tot_inc_all_usa

tot_don_dev <- sum(eas_new$dev_don, na.rm=TRUE)
tot_don_animal <- sum(eas_new$animal_don, na.rm=TRUE)
tot_don_ea_meta <- sum(eas_new$ea_meta_don, na.rm=TRUE)
tot_don_lt_ai <- sum(eas_new$lt_ai_don, na.rm=TRUE)

med_don <- median(eas_new$donation_c, na.rm=TRUE)
mean_don <- mean(eas_new$donation_c, na.rm=TRUE)
mean_don_not_new <- mean(eas_new$donation_c[eas_new$year_involved_n!=year_s], na.rm=TRUE)

mean_don_ly <- mean(eas_all$donation_c[eas_all$year==(year_n-1)], na.rm=TRUE)

mean_don_ly_not_new <- mean(
  eas_all$donation_c[eas_all$year==(year_n-1) & as.numeric(eas_all$year_involved)!=(year_n-1)],
  na.rm=TRUE)


plan_donate_ly_c <- filter(eas_all, year == year_n-1) %>% pull(donation_plan_c)

mean_plan_ly <- mean(plan_donate_ly_c, na.rm=TRUE) #plan in 2018 for 2019
med_plan_ly <- median(plan_donate_ly_c, na.rm=TRUE)

med_not_new <- median(eas_new$donation_c[eas_new$year_involved_n!=year_s], na.rm=TRUE)

top_1p3don <- eas_new %>% select(donation_c) %>% slice_max(donation_c, prop =.013) %>% sum()
top_1p3share <- top_1p3don/tot_don

```

### Summary, key results and numbers {#sum-results}


*Responses, total and average donation*

-   `r op(pct_tot(num_don))`% of EAs in the `r year_s` survey reported
    making a charitable donation in `r year_n -1`

-   `r op(pct_tot(zero_don))`% reported making zero donations, and

-   `r op(pct_tot(num_na_don))`% did not respond to this question.

    -   Thus, of those who responded,
        `r  op(100*num_don/(num_don+zero_don))`% reported making a
        donation in the prior year.

-   Participants reported total donations of `r op(tot_don)` USD in
    `r year_n -1` (cf 16.1M USD in `r year_n -2`).

\

*Trends*

-   The number of survey participants went from
    `r nrow(subset(eas_all, year_n==year_n-1))` in `r year_n-1`
    (`r nrow(subset(eas_all, year_n==year_n-1 & !is.na(donation_c)))` of
    whom answered the donation question) to
    `r nrow(subset(eas_all, year_n==year_n))`
    (`r nrow(subset(eas_all, year_n==year_n & !is.na(donation_c)))`
    answering the donation question) in `r year_n`.


-   Over the past years, we see ... (trend in median or mean donation amounts reported).^[All figures here refer to survey responses, so we won't write 'reported in the survey' each time. These (`r year_n-1`-20) numbers exclude a single survey donation response in the billions that was ruled to be implausible. A total of `r eas_new$troll_drops_n[1]` observations were dropped for implausible income, donations, or ages. Averages are for those who answered the donation question(s), including those who reported donating zero. Nonresponses are not counted in these statistics except where specifically mentioned. Unless otherwise mentioned, all figures simply add, average or otherwise summarize individual responses from the EA Survey years mentioned.]

-   Median annual donation in `r year_n - 1`: `r op(med_don)` USD

    -  <!--  Cf 683.92 USD in 2018 .... --> Cf prior year ...

-   Mean (reported) annual donation for `r year_n-1` was `r op(mean_don)` USD (cf `r op(mean_don_ly)` for 2018)

-   or `r op(mean_don_not_new)` USD excluding those who joined in
    `r year_s`

-   (cf `r op(mean_don_ly_not_new)` USD for `r year_n-2` excluding those who
    joined in `r year_n-1`).

-   Median annual donation in `r year_n-1` excluding those who joined EA
    in `r year_s` was `r op(med_not_new)` USD (cf. 990 USD for the
    comparable median for 2018/2019 and 832 USD for 2017/2018). (See
    ['donation and income trends in
    EA](#donation-and-income-trends-in-ea)' for more details).

\

*Donation as shares of income, distribution of donations*

-   In `r year_n-1` 1.3% of donors accounted for \$`r op(top_1p3don)` in
    donations or `r op(top_1p3share*100, d=2)`% of the survey total.

    -   (Cf ...)  <!-- [in 2018 1.3% of donors accounted for 57% of
        donations](https://forum.effectivealtruism.org/posts/29xPsh2MKkYGCuJhS/ea-survey-2019-series-donation-data).) -->

```{r don-income-share-statistics }

med_don_share <-  median(eas_new$don_share_inc_imp, na.rm = TRUE)
med_don_share_imp_bc <- median(eas_new$don_share_inc_imp_bc5k, na.rm = TRUE)

med_don_share_ly <-  median(eas_lastyr$don_share_inc_imp, na.rm = TRUE)
med_don_share_imp_bc_ly <- median(eas_lastyr$don_share_inc_imp_bc5k, na.rm = TRUE)

earn_filter <- quos(d_student==0, income_c>10000)

med_don_share_imp_ns_10k <- eas_new %>%
  filter(!!!earn_filter) %>%
    summarise(med=median(don_share_inc_imp, na.rm = TRUE))

tot_inc <- sum(eas_new$income_c, na.rm=TRUE)

tot_inc_imp_bc <- sum(eas_new$income_c_imp_bc5k, na.rm=TRUE)

share_don_gt_10pct <- sum(eas_new$don_share_inc_imp>=.1, na.rm = TRUE)/sum(!is.na(eas_new$don_share_inc_imp))

share_don_gt_10pct_imp <- sum(eas_new$don_share_inc_imp_bc5k>=.1, na.rm = TRUE)/sum(!is.na(eas_new$don_share_inc_imp_bc5k))

share_don_gt_5pct_imp <- sum(eas_new$don_share_inc_imp_bc5k>=.05, na.rm = TRUE)/sum(!is.na(eas_new$don_share_inc_imp_bc5k))

share_don_gt_10pct_earn <- eas_new %>%
  filter(!!!earn_filter) %>%
      transmute(share_don_gt_10pct =  sum(don_share_inc_imp>=.1, na.rm = TRUE)/sum(!is.na(don_share_inc_imp)) ) %>%
    unlist %>%  .[1]


#don gt 10pct ... by gender

#eas_new %>%
#     mutate(d_don_gte10_imp = don_share_inc_imp>=.1) %>%
#     tabyl(gender_manual, d_don_gte10_imp) %>% tabylstuff()

```

-   The median percentage of income donated in `r year_n-1` was
    `r op(med_don_share*100)`% (cf `r op(med_don_share_ly*100)`% in
    `r year_n-2`).

-   However, if we impute "0 and missing incomes" at "group medians for student-status and country",^[Many respondents do not reveal their income, or report zero or implausibly small incomes (if we consider income to include transfers and family support); among these, many *do* report donations. To get a meaningful measure of average shares of income donated (and other stats) *including* these individuals, we need to put some measure reflecting yearly spending power in the denominator. We thus make a rough imputation, selecting the average income for individuals from their same country and same student-status who *do* report an income value. To avoid sensitivity to outliers, countries with small numbers of participants are lumped together into an "other" group for this imputation. Where this (or reported income) falls below 5000 USD, we 'bottom-code' it this as at 5000 USD. (Note that we hope to improve this imputation in future work, incorporating features such as age in the imputation.)
] the median percentage of income donated was `r op(med_don_share_imp_bc*100)`% for `r year_n-1`.

-   *Mean* share of *total* (imputed) income donated was
    `r op(tot_don/tot_inc_imp_bc*100)`% (imputing income where below 5k
    or missing) or `r op(tot_don/tot_inc*100)`% without imputation.

<!-- DR: note, I checked there indeed are 3 'median' obs donating exactly 3 percent of income in 2019 -->

-   `r op(share_don_gt_10pct_imp*100)`% of EAs who answered the donation
    question reported donating 10% or more of their income in
    `r year_n-1` (if we impute income; otherwise
    `r op(share_don_gt_10pct*100)`% without imputation; this compares to
    ..., without imputation).

-   The median percent of income donated by full-time-employed
    non-students who earned more than \$10,000 was
    `r op(med_don_share_imp_ns_10k[[1]]*100)`%, and of this group
    `r op(share_don_gt_10pct_earn*100)`% donated 10% of their income or
    more in `r year_n-2` (cf 3.38% and 24% in 2018).

-   Overall, those taking the EA survey tend to report donating a
    substantially greater share of income than [those in the general US
    population](#usa-don) -- ([web
    link](https://rethinkpriorities.github.io/ea_data_public/eas_donations.html#usa-don)).

```{r oct_don_fmla, warning=FALSE}

don_tot_freq <- eas_all %>%
  dplyr::filter(year==max(year)) %>%
  summarise(across(c(all_of(donate_charity_names)), ~sum(as.numeric(.x) > 0, na.rm = TRUE)))  %>% slice(1) %>%
  unlist(., use.names=TRUE)


pct_don <- function(x) {
  sum(don_tot_freq[x])/sum(don_tot_freq)*100
}

pct_ddon <- function(x) {
  op(
    sum(x != 0, na.rm=TRUE)/sum(notNA(x), na.rm=TRUE)*100
  )
}

```

```{r don_stats, warning=FALSE, results='hide', error=TRUE}

don_stats <- eas_new %>%
        filter(num_named_dons>0) %>%
  select(all_of(where_don_vars)) %>%
  vtable::sumtable(
                summ=c('notNA(x)', 'sum(x != 0)', 'sum(x != 0)/notNA(x)', 'mean(x)', 'sd(x)', 'pctile(x)[50]', 'pctile(x)[90]'),
                summ.names = c('Number of Responses', 'Number reporting donation to cause', 'Share of reporters donating to cause', "Mean donation of reporters (including 0's)", 'Sd', "Median", "90th pct"),
                digits=c(0,0,2,0,0,0,0),
                simple.kable = TRUE,
                labels = all_char_labels2,  #it's a horrible workaround but we need to have the order of these the same as the table order ... I think it's a flaw of sumtable
                title = "Donations by category (where indicated)",
                out="kable") %>%
  kable_styling()



#todo (low-priority) -- replace with .summ hijacked command

n_rep_char <- sum(eas_new$num_named_dons>0, na.rm=TRUE)

```

```{r warning=FALSE, results='hide'}

don_stats_by_gwwc <- eas_new %>%
        mutate(`GWWC Pledge` = case_when(
          action_gwwc==1 ~ "Yes",
          action_gwwc==0 ~ "No"
        )) %>%
        filter(num_named_dons>0) %>%
  select(all_of(where_don_vars),
         `GWWC Pledge`) %>%
  vtable::sumtable(group = "GWWC Pledge",
                   group.test=TRUE,
                 summ=c('notNA(x)','sum(x != 0)/notNA(x)', 'mean(x)', 'sqrt(var(x)/length(x))',
                        'pctile(x)[50]'),
                summ.names = c('N Responses', 'Share positive', 'Mean', "Median"),
                digits=c(0,2, 0,0,0),
                simple.kable = TRUE,
                labels = all_char_labels2,  #it's a horrible workaround but we need to have the order of these the same as the table order ... I think it's a flaw of sumtable
                title = "Donations by category (where indicated), by GWWC", out="kable")  %>%
      row_spec(1:1, bold = TRUE) %>%
  kable_styling()

```

```{r,  results='hide' }

ddon_stats_by_gwwc <- eas_new %>%
        mutate(`GWWC Pledge` = case_when(
          action_gwwc==1 ~ "Yes",
          action_gwwc==0 ~ "No"
        )) %>%
        filter(num_named_dons>0) %>%
  select(all_of(where_don_dummies),
         `GWWC Pledge`) %>%
  vtable::sumtable(group = "GWWC Pledge",
                   group.test=TRUE,
                 summ=c('notNA(x)','sum(x != 0)/notNA(x)'),
                summ.names = c('N Responses', 'Donated to... ?'),
                digits=c(0,2),
                simple.kable = TRUE,
                labels = all_char_labels2,  #it's a horrible workaround but we need to have the order of these the same as the table order ... I think it's a flaw of sumtable
                title = "Binary: Indicated donating to category, by GWWC",
                out="kable") %>%
        row_spec(1:1, bold = TRUE) %>%
  kable_styling()


#  .kable() %>%
 # .kable_styling("striped")

#todo (low-priority) -- replace with .summ hijacked command

```

\

*Where EAs donated*

-   While `r op(pct_tot(num_don+zero_don))`% of respondents answered the
    donation question, only `r op(pct_tot(n_rep_char))`% answered at
    least one question about *where* they donated.

<!-- Todo: can we get a better default presentation of numbers and in line code so that I don't have to use this "op()" function all the time? -->

-   Among these, the charity that the most EAs stated that they donated
    to was (...) <!-- the Against Malaria Foundation (AMF)-->  with
    `r max(don_tot_freq)[[1]]` reported donations (out of a total of
    `r sum(don_tot_freq)` reported donations).

<!--  `r str_replace_all(names(don_tot_freq)[which.max(don_tot_freq)], "donate_|_", " ")` -->

-   Global Poverty charities continue to attract (...) the largest
    counts and amounts of donations. `r  pct_ddon(eas_new$dev_don)`% of
    those who answered the relevant question reported donating to this
    category. `r op(pct_don(dev_health_chars))`% of the total 'where
    donated' reports were to global poverty charities. We sum
    `r op(tot_don_dev)` USD in total donations reported as specifically
    going to global poverty charities.

    -   This compares to `r pct_ddon(eas_new$animal_don)`% reporting
        donating, `r op(pct_don(animal_chars))`% of donations and $\$$
        `r op(tot_don_animal)` total donated for animal charities,

        -   `r pct_ddon(eas_new$ea_meta_don)`%,
            `r op(pct_don(ea_meta_chars))`% and $\$$
            `r op(tot_don_ea_meta)` for EA movement/meta charities,
        -   and `r pct_ddon(eas_new$lt_ai_don)`%,
            `r op(pct_don(lt_ai_chars))`% and $\$$ `r op(tot_don_lt_ai)`
            for long term and AI charities, respectively.

\

*Donations vs plans*

-   ... [Evidence is mixed](#plan-actual) on whether EAs' donations in a
    year tend to exceed or fall short of the amount they planned to
    donate (as they reported in previous surveys).

<!-- TODO cut or soft-code and condense For the small share
    that *can* be tracked across years, donations tend to exceed plans
    (by around 60 USD at median, but over 1000 USD at mean). However,
    the *overall distribution* of donations for a particular year
    (including all respondents) tends to fall short of the distribution
    of planned donations (by about 450 USD at median and over 2000 at
    mean).

-   While at median [EAs tend to report](#next-year) planning to donate
    the same amount this *next year* that they donate in each particular
    year, the average (mean) plan for next year is ... significantly
    larger.

-->

<!-- EAS2019_draft10.do tabstat donate_cause_meta_2018_c donate_cause_cause_pri_2018_c donate_cause_poverty_2018_c donate_cause_animal_welfare_2018 donate_cause_far_future_2018_c, stat(sum mean median     ) -->
\

*Descriptive and predictive models*

-   Our [descriptive models](#descriptives) suggest that:

... <!--  update results here, or perhaps only link the forum -->

```{=html}
<!--
    -   age, being in a named 'top EA' big city, having taken the GWWC pledge, and an Earning-to-Give career are positively associated with donations,
    -   while being 'not employed' (and to a lesser extent non-male gender and student status are negatively associated with this;
    -   donation are roughly proportionally associated with income (approximately 'unit elastic'),
    -   as well as with age and 'time in EA' (with elasticities around 0.54 to 0.63, respectively).
-->
```
-   Our [predictive (ML) models](#predictive-models) highlight the
    importance of

... <!--  update results here, or perhaps only link the forum -->

<!-- income and (to a lesser extent) age (each positively related to donation incidence and amounts).-->

<!--    -   as well as certain sources of 'where heard of EA'. -->

-   ... model performance, brief note about technical modeling choice
    changes if any

<!-- These models [perform moderately well](#model-perf), particularly in predicting 'whether donated 1k USD or more' (here it attains about 74% accuracy compared to 54% accuracy from simply 'guessing the most common outcome'). -->

### Link: Why does the EA Survey ask about donations? {#toc .unnumbered}

See discussion in [2020 EA Forum post](https://forum.effectivealtruism.org/posts/nb6tQ5MRRpXydJQFq/ea-survey-2020-series-donation-data#Why_does_the_EA_Survey_ask_about_donations_).

## Total EA donations, magnitudes in context {#total_mag }

**Considering the magnitude of the donations...**

The \$`r op(tot_don)` USD in donations reported above seems likely to be
a small share of total EA-affiliated giving, ...

See our fairly lengthy discussion and benchmarking of this in the [2020
EA Forum post](https://forum.effectivealtruism.org/posts/nb6tQ5MRRpXydJQFq/ea-survey-2020-series-donation-data#Considering_the_magnitude_of_the_donations___) ... <!-- (link, mention any updates to this if necessary.) -->

## Career paths: Earning-to-give {#career-etg}

[Caveat about changes in question text (mention and link ...)]

```{=html}
<!--
"We were requested to change the question between 2018/2019 and 2020... but looking at non-students (who are largely already in their careers), the responses across years may still be comparable and appear to show a slight decline in E2G"

In the tables and graphs below, the apparent steep drop in the number indicating ETG from the 2019 to the 2020 survey seems likely to be overstated (as a result of a requested change in the question language and options provided).^[*Question texts*

2018: "What broad career path are you planning to follow?" [4 options]

2019: "If you had to guess, which broad career path(s) are you planning to follow?" [9 options]

2020: "Which of the following best describes your current career?" [11 options]

Note that the changing composition of EA survey respondents may also affect this.
]
-->
```
<!-- Still, the responses for *non-students* might be less sensitive to the changes in the survey question as they are more likely to be *in* a career path as their 'current career'. These responses also suggest some decline in EtG. -->

Changes for students and nonstudents (bearing in mind the above
caveats):

```{r}

etg_rates_all <- eas_all %>%
  filter(year>2014) %>%
  group_by(year) %>%
  summarise( "Count" = n(),
             "Share ETG" = mean(as.numeric(d_career_etg))
             )

etg_rates_ns <- eas_all %>%
    filter(year>2014) %>%
  filter(d_student==0) %>%
  group_by(year) %>%
   summarise( "Count" = n(),
             "Share ETG" = mean(as.numeric(d_career_etg))
             )

(
etg_rates_tab <- bind_cols(etg_rates_all, etg_rates_ns[-1]) %>%
  magrittr::set_names(c("Year", "All responses", "Share EtG", "Nonstudents", "Nonstudents: Share EtG")) %>%
  kable(caption = "Rates of 'Earning-to-give' by year and student status (see caveats)", digits=3) %>%
  .kable_styling()
)


#todo - medium priority: combine the above tables into a single table: overall, just for students  with just n,

```

```{r etg_rates_plot}

(etg_rates_plot <- eas_all %>%
  group_by(year, d_student) %>%
      filter(year>2014) %>%
  filter(!is.na(d_student)) %>%

        #@oska (low-med priority todo): we should functionalize these mutations for computing se and CIs (or find someone who has done). We do it again and again, and the code is bulky
        #maybe incorporate my se_bin function
        #@oska todo ... also functionalize or otherwise preserve a good version of this graph

  # Calculate standard error, confidence bands and change student factor levels
  summarise(
         m_etg = mean(as.numeric(d_career_etg)),
         se = se_bin(d_career_etg)) %>%
    mutate(
         etg_low = m_etg - 1.96*se,
         etg_high = m_etg + 1.96*se,
         d_student = as.factor(if_else(d_student == 0, "Non-student", "Student")),
         year = as.factor(year)) %>%

ggplot(aes(x=year, y=m_etg, colour = d_student, group = d_student))  +
  geom_pointrange(aes(ymin = etg_low,
                      ymax = etg_high),
                  position = position_dodge(width=0.5)) + # Ensure that bars don't overlap
  geom_line(position = position_dodge(width=0.5)) +
  xlab("Mean (and 95% CI) response share in 'Earning-to-give'") +
  ylab("Share of sample") +
  scale_color_discrete("") + # Remove legend title
   scale_y_continuous(labels = scales::percent_format(accuracy = 1L), limits=c(0,NA), oob = scales::squish) + # Change y-axis to percentages
  theme(legend.position = c(0.9, 0.95),
        #legend.background = element_rect(fill=alpha('blue', 0.001)),
        legend.key = element_blank())
)
```

The decline in ETG is less dramatic among non-students (over 23% of non-student respondents still report ETG as their 'current career'), but it nonetheless appears to be fairly strong and consistent from 2017-present.^[We do not include 2014 in the above tables and plots because of very low response rates to the student status and EtG-relevant questions.
]

## Donation totals: descriptives {#descriptives}
^[Note, we report on amounts 'planning [to] save in `r year_n` to donate later in an [appendix section](#save-to-don)'.]

### Overall donations, totals by groups {.unnumbered}

Below, we present a histogram of positive reported `r year_n-2`
donations by all respondents. Note that:

-   the horizontal axis is on a logarithmic scale,
-   `r op(sum(eas_new$donation_c==0, na.rm=TRUE)/nrow(eas_new)*100)`% of
    the `r op(nrow(eas_new))` total respondents reported donating zero,
    and
-   `r op(sum(is.na(eas_new$donation_c))/nrow(eas_new)*100)`% of the
    total respondents did not report their donation amount.

```{=html}
<!--
# plotting-funcs
# The plotting functions have been moved into `code/plotting_functions.R`

#Todo -- Medium priority: add or align a cumulative donation amounts histogram

-->
```

```{r don-hist}

donation_c <- eas_new$donation_c

require(scales)

don_breaks <- c(50, 100, 200, 300, 500,  1000, 2500, 5000, 10000, 25000, 50000, 100000, 250000, 500000, 1000000, 2500000)

eas_new %<>%
  rowwise() %>%
      mutate(donation_c_50 = max(donation_c, 50)) %>%
  ungroup

(
  donhist_tyly <- eas_new %>%
    hist_plot_lscale(eas_new$donation_c_50, breaks = don_breaks ) +
    geom_vline_mean(donation_c) +
    geom_vline_med(donation_c) +
    geom_vline_90(donation_c) +
    labs(title="Histogram of prior year's Donations (latest survey)", x="last year's $ Donations (bottom-coded at 50)", y = "Number of respondents")
)

# Todo (medium importance): Overlay a display of 'overall percentage shares' ... so we know where the 80th and 90th percentile are, etc.

```

\

In `r year_n-2` we reported [link/quote] (Or make this into a concise
bulleted comparison)

...

We compare the results for `r year_n` (for `r year_n-1` donations):

-   Median donation (of those reporting): `r op(op(med_don))` USD
-   Donation of \$1000 puts you in the
    `r op(ecdf(eas_new$donation_c)(1000)*100)`th percentile. 
- Being in the top 10% requires donating `r op(quantile(eas_new$donation_c, .90, na.rm=TRUE))`
-   Being in the top 1% means donating
    `r op(quantile(eas_new$donation_c, .99, na.rm=TRUE))` USD.

... <!-- comments (or leave out) -->

<!-- As in previous years, the mean far exceeds the median, (and falls close to the 90th percentile!); a very small number of very large donations dwarf the size of most others. We illustrate this in the 'treemap' plot below, which divides the total reported contributions into groups by size-of-contribution. -->

\

```{r don_share_by_size}

require(treemapify)

geom_treemap_opts <- list(treemapify::geom_treemap(alpha = 0.7),
  geom_treemap_text(fontface = "italic", colour = "white", place = "centre",
                    grow = TRUE, min.size = 1 ),
  theme(legend.position = "none",
        plot.title = element_text(hjust = 0.5))
  )

(
  don_share_by_size <- eas_new %>%
    geom_tree_tot_by_split(donation_c, donation_c_split, title = "Share of total last year's donation amount, by donation size")
)

```

... discussion or cut

```{=html}
<!-- Over a third of total reported contributions reported for 2019 come from contributions over 500,000 USD, with another 20% coming from contributions between 25k and 100k. Contributions of under 2500 USD represent less than 5% of the total.
-->
```
\

**'Which career paths are driving donation totals?'**

\

*Some caveats:*

::: {.foldable}

Caveats: The figures below exclude `r op(sum(is.na(eas_new$career_)))`
participants who provided no response to the career question (...
doublecheck this), `r op(sum(is.na(eas_new$career_))/NROW(eas_new))` of
the sample. These participants reported a total of
$\text{\$}$`r format( sum(eas_new$donation_c[is.na(eas_new$career_)], na.rm=TRUE), big.mark="," )`
in donations which makes up
`r op(sum(eas_new$donation_c[is.na(eas_new$career_)], na.rm=TRUE)/sum(eas_new$donation_c, na.rm=TRUE)*100)`%
of the total reported donations for `r year_n-1` .
:::

-   Totals
-   Overall shares

```{r don_by_career}
#library(treemapify)


  don_by_career_XXX <- eas_new %>%
    geom_tree_tot_by_split(donation_c, career_, title= "Share of last year's donations by career path")


(
  don_by_career <- eas_new %>%
    select(career_, donation_c) %>%
              filter(!is.na(career_)) %>%
  group_by(career_) %>%
      filter(!is.na(career_)) %>%
  summarise(total_don = sum(donation_c, na.rm=TRUE),
            n = n()) %>%
  mutate(don_share = round(total_don/sum(total_don)*100),
         freq = n/sum(!is.na(eas_new$career_))
         ) %>%

  ggplot(aes(area = total_don , fill=freq,
             # Include percentage of total donation
             label = paste(career_,
                           paste0(don_share, "%"),
                           paste0("(Pop:", round(freq*100) , "%)"),
                                  sep = "\n"))) +
  geom_treemap_opts +
   # theme(legend.position = "bottom") + #todo -- add title to legend explaining that it's the survey pop; get better colors for this
  scale_fill_continuous(name = "Frequency",
                        label = scales::percent, trans = "reverse") +
labs(title= "Share of last year's donations by career path",  subtitle = "(Share of survey population in parentheses; darker = larger share)")
)


```

```{r}

(
  career_tab <- eas_new %>%
    mutate(Career = na_if(career_, "na")) %>%
      filter(!is.na(Career)) %>%
    tabyl_ow(Career) %>%
    adorn_totals() %>%
    rename_with(snakecase::to_title_case) %>%
    .kable(caption = "Shares in each career path", padding = 0, digits=c(0,0,2)) %>%
    .kable_styling()
)

```

\
<!-- ... softcode description, or skip. It's pretty obvious Those reporting 'for profit-earning to give' career paths represent the largest share, nearly half of the total donations, despite making up  only about 15% of the sample (of those answering this question). Those with 'for profit' careers who do not say they are earning to give donate about 15% of the total, roughly in proportion to their 12% share of the sample. However all of these differences may reflect differences in income and wealth levels, as well as differences in underlying characteristics of people who choose different career paths.

Direct work does not seem to be obviously coming at the expense of donations. Those pursuing careers working at EA-affiliated non-profits account for a somewhat higher share of donations (12%) than their (8%) share of the sample. (However, we do not know how much these particular EAs *would* have given had they chosen a different career.)

-->

We put this in perspective, considering that income levels are different
between these career paths:

```{r plotting_helpers}

grp_sum <- function(df, xvar, yvar, groupvar) {
  df %>%
      dplyr::select({{xvar}}, {{yvar}}, {{groupvar}}) %>%
      group_by({{groupvar}}) %>%
      drop_na({{xvar}}, {{yvar}}, {{groupvar}}) %>%
      summarise(
                      mn_y = mean({{yvar}}),
                      mn_x = mean({{xvar}}),
                      med_y = median({{yvar}}),
                      med_x = median({{xvar}}),
                      se_y = sd({{yvar}}, na.rm=TRUE)/sqrt(length({{yvar}})),
                      se_x = sd({{xvar}}, na.rm=TRUE)/sqrt(length({{xvar}}))
                      ) %>%
      group_by({{groupvar}}) %>%
    # Calculate confidence intervals
      mutate(
              lower = max(0, mn_y - 1.96*se_y),
              upper = mn_y + 1.96*se_y
              )
}

plot_grp <- function(df, groupvar, labsize=4, labangle=90, force = 1, fp = 1, mo=10, bp=1, arrow=NULL) {
  df %>%
    ggplot(aes(x=mn_x, y=mn_y, label = {{groupvar}})) +
  geom_point() +
  geom_abline(intercept = 0, slope = 0.1, colour="violetred1") +
  geom_smooth(method=lm, alpha=0.7) +
  geom_errorbar(aes(ymin = lower,
                      ymax = upper), alpha=0.7) +
  scale_y_continuous( oob = scales::squish) +
    scale_x_continuous( oob = scales::squish) +
  ggrepel::geom_text_repel(
    size = labsize, angle = labangle, max.overlaps=mo, force=1, force_pull = fp,
                           box.padding = bp,
                           arrow = arrow,
    color="brown", alpha=0.75)
    }

```

```{r}
(
  don_inc_career_plot <- eas_new %>%
     mutate(Career = na_if(career_, "na")) %>%
      filter(!is.na(Career)) %>%
  grp_sum(income_c_imp_bc5k, donation_c, Career) %>%
  plot_grp(Career, labsize=3) +
  xlab("Mean income in USD (imputed if <5k/missing)") +
  ylab("Mean donations, CIs") +
      scale_y_continuous(limits=c(-10000, 30000),  oob = scales::squish)
)

```

The plot above depicts mean income and mean donations by 'career group',
with 95% CI's for the latter. We superimpose a 'line of best fit' (blue,
with smoothed 95% intervals for this rough fit) and a '10% of income
donation' line (red). Unsurprisingly, for-profit 'not-EtG' are below the
fitted line, and 'for-profit EtG' above this line, although 95% CIs are
fairly wide. We also note that among people in non-profit careers, there
are similar average incomes whether or not the non-profit is EA-aligned,
but the non-profit *EA* people seem to donate somewhat more (although
the CI's do overlap).

\

Next, we present reported donation amounts by income groupings.^[Here we impute income where it is missing or below 5000 USD. However, the figure *removes* observations where income as well one of either country or student status is missing; in these cases income cannot be simply imputed from these.
]

```{r don_share_by_income}
#p_load(treemapify)

(
  don_share_by_income <- eas_new %>%
    select(donation_c, income_c_imp_bc_k, income_c_imp_split) %>%
    filter(!is.na(income_c_imp_bc_k)) %>%
  group_by(income_c_imp_split) %>%

    summarise(total_don = sum(donation_c, na.rm=TRUE),
            n = n()) %>%

  mutate(don_share = round(total_don/sum(total_don)*100),
         freq = n/sum(!is.na(eas_new$income_c_imp_split))) %>%

  ggplot(aes(area = total_don, fill= freq,
             # Include percentage of total donation
             label = paste(income_c_imp_split,
                           paste0(don_share, "%"),
                           paste0("(Pop:", (round(freq*100, 1)) , "%)"),
                           sep = "\n"))) +
  geom_treemap_opts +
    scale_fill_continuous(name = "Frequency",
                        label = scales::percent, trans = "reverse") +
labs(title= "Share of last year's donations by income groups",  subtitle = "(Share of survey population in parentheses; darker = larger share)")
)

earn_tab <- eas_new %>%
tabyl_ow_plus(income_c_imp_split)


```

Compare the above graph to the 'donations by donations size' graph...

<!-- ... or cut The largest *earners* (the
`r op(nrow(subset(eas_new, eas_new$income_c_imp_bc_k>=1000)))` people
earning 1 million USD or more) represent 35% of the donations (cf the
largest *donors* represent 36% of the donations). However, the
second-highest earners, the
`r op(nrow(subset(eas_new, eas_new$income_c_imp_bc_k>=500 & eas_new$income_c_imp_bc_k<1000)))`
people earning between 500k and 1 million USD represent only 6% of the
donations (cf 20% from the second-highest *donation* group). In fact,
the second largest share of total `r year_n` donations come from the
second-largest (in *population*) income-group in our sample, the
`r op(nrow(subset(eas_new, eas_new$income_c_imp_bc_k>=50 & eas_new$income_c_imp_bc_k<100)))`
people earning between 50K and 100K USD.

-->


\

**Finally, we report donation totals by country.**
<!--(see also the table ... above):-->

First for `r year_n-1` donations alone:

```{r don_share_by_country}
#p_load(treemapify)

(
  don_share_country <- eas_new %>% select(donation_c, country_big) %>%
  group_by(country_big) %>%
    summarise(total_don = sum(donation_c, na.rm=TRUE),
            n = n()) %>%
  mutate(don_share = round(total_don/sum(total_don)*100),
         freq = n/sum(!is.na(eas_new$country))) %>%
    ungroup() %>%
  filter(don_share != 0 & !is.na(country_big)) %>%
  ggplot(aes(area = total_don, fill= freq,
             # Include percentage of total donation
            label = paste(country_big,
                           paste0(don_share, "%"),
                           paste0("(Pop:", op(round(freq*100, 0)) , "%)"),
                           sep = "\n"))) +
    geom_treemap_opts +
     #scale_fill_continuous(name = "Frequency", label = scales::percent, trans = "reverse") +
     scale_fill_continuous(name = "Frequency",
                        label = scales::percent, trans = "reverse") +
labs(title= "Share of last year's donations by country",   subtitle = "(Share of survey population in parentheses; darker = larger share)")
)

#; darker = larger share


```

\

Next, pooling across all years of the EA survey (without any weighting
or adjustment):

```{r don-share-country-all_years}
(
  don_share_country_all_years <- eas_all %>% select(donation_c, country, year) %>%
     filter(!is.na(country)) %>%
  group_by(country) %>%

    summarise(total_don = sum(donation_c, na.rm=TRUE),
            n = n()) %>%
    ungroup() %>%
  mutate(don_share = round(total_don/sum(total_don)*100),
         freq = n/sum(!is.na(eas_all$country))) %>%

  filter(don_share > 0.1) %>%
  mutate(country = snakecase::to_title_case(country)) %>%
  ggplot(aes(area = total_don, fill= freq,
             # Include percentage of total donation
label = paste(country,
                           paste0(don_share, "%"),
                           paste0("(Pop:", op(round(freq*100, 0)) , "%)"),
                           sep = "\n"))) +
  geom_treemap_opts +
       scale_fill_continuous(name = "Frequency",
                        label = scales::percent, trans = "reverse") +
  theme(legend.position = "none",
        plot.title = element_text(hjust = 0.5)) +
  labs(title= "Share of total (all years) donation amounts by country", subtitle = "(Share of survey population in parentheses; darker = larger share)")
)

```

\

And again, 'Winsorizing' donations at 100K USD (setting larger donations
at this value), to reduce the impact of outliers:

```{r don-share-country-all_years}

(
  don_share_country_all_years_w <- eas_all %>% select(donation_c, country, year) %>%
     filter(!is.na(country)) %>%
    rowwise() %>%
    mutate(donation_c_w = min(donation_c, 100000)) %>%
    ungroup() %>%
  group_by(country) %>%

    summarise(total_don_w = sum(donation_c_w, na.rm=TRUE),
            n = n()) %>%
    ungroup() %>%
  mutate(don_share = round(total_don_w/sum(total_don_w)*100),
         freq = n/sum(!is.na(eas_all$country))) %>%

  filter(don_share > 0.1) %>%
  mutate(country = snakecase::to_title_case(country)) %>%
  ggplot(aes(area = total_don_w, fill= freq,
             # Include percentage of total donation
label = paste(country,
                           paste0(don_share, "%"),
                           paste0("(Pop:", op(round(freq*100, 0)) , "%)"),
                           sep = "\n"))) +
  geom_treemap_opts +
       scale_fill_continuous(name = "Frequency",
                        label = scales::percent, trans = "reverse") +
  theme(legend.position = "none",
        plot.title = element_text(hjust = 0.5)) +
  labs(title= "Share of 100k-Winsorised donations by country; all years", subtitle = "(Share of survey population in parentheses; darker = larger share)")
)

#TODO - @oska -- UK and USA in all-caps above

```

\

<!-- We report the shares (0-1) of the total survey population coming from
each country below: -->

```{r country_tab}

#TODO - @oska -- capitalization below
#TODO - @oska -- sort by shares below

(
  country_tab <- eas_all %>%
    group_by(country_big) %>%
    filter(year>2014) %>%
   mutate(
          year_2020 = case_when(
            year==2020 ~ "2019 share.",
            TRUE ~ "pre-2019 share."
          ),
          `Country` = str_to_title(country_big),
          ) %>%
tabyl(`Country`, year_2020) %>%
 adorn_percentages("col")  %>%
  .kable(digits=2, caption="Shares (0-1) of survey population by country; larger countries only", label=TRUE) %>%
    .kable_styling()
)

```

\

<!-- Next, a year-by-year animation of the shares of donations from
each country: -->

```{r animated-tree-map}

#d_anim <- "Y"

#library(gganimate)

anim_filename <- here(plots_folder, "animated_tree_plot.gif")

if (exists("d_anim")) {
if (d_anim == "Y") {

animated_dons_country <- eas_all %>% select(year, donation_c, country_big) %>%
  group_by(year, country_big) %>%
  filter(year>2014) %>%
  summarise(total_don = sum(donation_c, na.rm=TRUE)) %>%
  mutate(don_share = round(total_don/sum(total_don)*100)) %>%
  ggplot(aes(area = total_don, fill= country_big,
           # Include percentage of total donation
           label = paste(country_big, paste0(don_share, "%"), sep = "\n"))) +
  geom_treemap_opts +
  ggtitle("Share of total last year's reported donation amounts by country")


anim <- animated_dons_country + transition_states(year,
                                      state_length = 3) +
  ggtitle("Share of total {closest_state} reported donation amounts by country")

gganimate::anim_save(anim_filename, anim)

anim

}
else{
  knitr::include_graphics(anim_filename)
}
}

if (!exists("d_anim")){
  knitr::include_graphics(anim_filename)
}
#Todo (medium importance): slo

```

<!--- In `r year_n-1`, the largest summed donation amount came from

-   the UK (about 11% of the sample but 41% of the donations)

-   and the USA (30% of the sample and 37% of the donations).

Across all years: ...

-   the USA represents the largest amount of donations,

-   with the UK a close second,

-->

Summarize or skip discussion of this here....

<!--
Again, the UK 'punches far above its weight.'  Note that the UK share may be *understated*, if UK donors claim the matching 'Gift Aid' but do not report it as part of their donation.^[In the UK the government's ['Gift Aid' policy](https://www.gov.uk/donating-to-charity/gift-aid) supplements all reported donations made by UK taxpayers by an additional 25%.
]


Again, these raw difference may reflect differences in income and life circumstances among survey respondents from different countries. The outsized UK share also seems to be driven by a few large outlying donations -- when we Winsorise donations at 100K USD, the UK no longer overperforms.
-->

 We return to presenting a 'controlled descriptive picture' in our [modeling work](#descriptive).^[We have shown 'where the donations were in `r year_n-1`' (and across years). However, we are *not* suggesting that this provides direct evidence of differences in EA generosity by country.]

### Donation (shares) vs. income and GWWC pledge {#donshares}


Quick notes or recap ... (or skip)

<!-- [2018
post](https://forum.effectivealtruism.org/posts/29xPsh2MKkYGCuJhS/ea-survey-2019-series-donation-data#Percentages_of_Income_Donated):

> We also looked at the percentages of pre-tax income that EAs were
> donating, based on the 1,563 EAs who disclosed both income and
> donation data. As in previous years, most EAs were donating
> significantly less than the 10% Giving What We Can Pledge... However,
> as the graph below shows, there is a marked 'bump' in the donors
> giving at around the 10% figure, perhaps due to the Giving What We Can
> Pledge target around this amount, or due to the figure's wider
> popularity as a target (e.g. in tithing).\

-->

The histograms beloware first only for those with positive reported incomes, and next with the previously-discussed income imputation. The blue vertical line depicts the share of *total* (imputed) income donated by all
respondents, with the green line depicting the median and the red line
the 90th percentile.  ...

<!-- These plots show similar patterns as in 2018. -->

```{r don-share-income-hist}

scale_x_set <- list(scale_x_continuous(limits=c(0,0.35), n.breaks=20))

(
  don_share_inc_imp_hist <- eas_new %>%
    hist_plot(don_share_inc_imp) +
    geom_vline_med(eas_new$don_share_inc_imp, tgap=0.01) +
    geom_vline_mean(tot_don/tot_inc, tgap=0.01, label = "Overall share") +
        geom_vline_90(eas_new$don_share_inc_imp, tgap=0.005) +
    scale_x_set +
    labs(title="2019 Donations/Income (no imputing)", x="2019 Donations/income", y="Number of respondents") +
     ylim(0, 300)
)


##Todo -- Medium priority: mean is missing
# todo -- low priority: make the above histogram bigger, it's smaller than the rest

don_share_inc_imp_hist_imp <- eas_new %>%
    hist_plot(don_share_inc_imp_bc5k) +
    geom_vline_mean(tot_don/tot_inc_imp_bc, tgap=0.01, label = "Overall share") +
    geom_vline_med(eas_new$don_share_inc_imp_bc5k, tgap=0.005) +
        geom_vline_90(eas_new$don_share_inc_imp_bc5k, tgap=0.005) +
    scale_x_set +
    labs(title="2019 Donations/Income (with imputing)", x="2019 Donations/income (with imputing)", y = "Number of respondents") +
  ylim(0, 300)

don_share_inc_imp_hist_imp

#Todo -- Medium priority(@oska): convert to 'share of respondents', add cumulative plot

```


```{r}
don_share_inc_imp_hist_imp %>% ggplotly()
```


\

The noticeable spike at 10% likely reflects the GWWC pledge (we return
to this further below). As noted above,
`r op(share_don_gt_10pct_imp*100)`% of EAs reported a donation at or
above 10% of their (imputed) income in `r year_n-1`.
`r op(share_don_gt_5pct_imp*100)`% reported an amount at or above 5%.

```{=html}
<!-- TODOS:
- overlay (smoothed) histograms (low)
- compare/overlay prior years (medium to low, but would be very cool)
-- animate year to year (medium to low, but would be very cool)
-->
```
\

```{r don-income-gwwc-scatter}

#Donations and donation shares -- scatterplots by income and GWWC 'action'

p_load(ggpubr)

op_ax <- function(x) round(as.numeric(x), digits=2)

scale_y_don <- scale_y_log10(
    name = "Donation amount (bottom-coded at $50)",
    # labels = scales::dollar,
    labels = scales::label_number_si(prefix = "$"),
    n.breaks = 10,
    limits = c(50, NA)
  )

don_income_gwwc_sp <- eas_all %>%
  filter(year==2020) %>%
    ggpubr::ggscatter(
      x = "income_c_imp_bc_k", y = "donation_c_min50", color = "d_gwwc_ever", size = 0.8, xlab = "Income in $1k USD (imputed where missing or lt 5k)", repel = TRUE, palette = "jco", yscale = "log10", xscale = "log10", add = "loess", add.params = list(color = "black", fill = "lightgray"), conf.int = TRUE
    ) +
    labs(title = "Donations by income (log scales)") +
    scale_x_log10(name="Income in $1K USD (imputed if <5k/missing)", labels = op_ax, n.breaks=5, limits=(c(5,5000)))  +
    labs(colour = "Mentioned taking GWWC pledge") +
    scale_y_don  +
    theme(axis.text.x = element_text( angle = 90, vjust = 0.5, hjust = 1 ))


don_income_gwwc_sp_gwwc <- eas_all %>%
  filter(year==2020) %>%
  ggplot(aes(x = income_c_imp_bc_k, y = donation_c_min50, color = d_gwwc_ever)) +
  geom_point(size = 1, alpha = 0.7) + # draw the points
  geom_smooth(aes(method = 'loess',
                  fill = d_gwwc_ever)) + # @Oska -- note I am using  local smoothing here.
  scale_x_log10(name = "Income in $1K USD (imputed if below 5k/missing)", n.breaks = 5, limits = c(5, 5000)) +
  scale_y_log10(
    name = "Donation amount (bottom-coded at $50)",
    # labels = scales::dollar,
    labels = scales::label_number_si(prefix = "$"),
    n.breaks = 10,
    limits = c(50, NA)
  ) +
  scale_color_discrete(name = "GWWC pledge") +
  scale_fill_discrete(guide = "none") +
  theme(axis.text.x = element_text( angle = 90, vjust = 0.5, hjust = 1 ),
        legend.position = c(.87,.15),
        legend.background = element_rect(fill=alpha('blue', 0.01)))

##Todo -- Medium priority - clean up the above a bit more...  get the axes better so that we can really see the 'large mass in the middle a bit better. Maybe slightly smaller dots and bolder smoothed lines, perhaps different colors for the CI shading for each
# - perhaps use geom_pointdensity with different shapes to indicate regions of "larger mass"

# #TODO -- Add some layer to better capture the masses *exactly at* 10pct

# REVIEW
# We should note that this doesn't include those who donate nothing due to the log scale (pseudo log scale is a bit weird here as well)


```

```{r don-share-income-gwwc-scatter}

require(ggpointdensity)

don_share_income_by_X  <- eas_all %>%
  filter(year==2020) %>%
  mutate(income_c_imp_bc5k_k = income_c_imp_bc5k/1000) %>%
  rowwise() %>%
  mutate(don_share_inc_imp_bc5k = min(don_share_inc_imp_bc5k, 0.4)) %>%
  ungroup() %>%
   group_by(d_gwwc_ever_0) %>%
  mutate(med_gwwc = median(don_share_inc_imp_bc5k, na.rm=TRUE)) %>%
   ungroup() %>%
   group_by(engage_high_n) %>%
    mutate(med_eng = median(don_share_inc_imp_bc5k, na.rm=TRUE)) %>%
  ggplot(aes(x = income_c_imp_bc5k_k, y = don_share_inc_imp_bc5k)) +
  ggpointdensity::geom_pointdensity(adjust=0.25) +
  geom_smooth(method = "loess") +
 #geom_hline_med(y) +
  geom_hline(yintercept=0.1, linetype="dashed", size=0.5, color = "red") +
  scale_y_continuous(labels = scales::label_percent(accuracy = 1L)) +
  scale_x_log10(breaks = scales::log_breaks(n=7)) +
  scale_color_viridis_c("density of respondents") +
  xlab("Income in $1K USD (imputed if missing, bottom-code at 5k)") +
  theme(axis.title.x = element_text(size = 10)) +
  ylab("Donations/Income (top-code at 40%)")


don_share_income_by_engage_sp <- don_share_income_by_X +
       geom_hline(aes(yintercept=med_eng), linetype="dashed", size=0.5, color = "blue") +
   facet_wrap(~engage_high_n, nrow=3)  +
    ylab("Donations/Income (top-coded at 50%)") +
  labs(title="By 'High-engagement': last year's 'Don. shares of income' by income (w. imputing)")


don_share_income_by_gwwc_sp <- don_share_income_by_X +
       geom_hline(aes(yintercept=med_gwwc), linetype="dashed", size=0.5, color = "blue") +
   facet_wrap(~d_gwwc_ever_0)  +
  labs(title="By GWWC: last year's  'Don. share of income' by income (w/ imputing)")

```

\

**How do donations relate to income, and does this relationship differ
between those who mention that they took the Giving What We Can (10%)
pledge?**



<!-- We first simply plot reported donations against income, simply dividing
individuals (points) by whether they mention having taken the GWWC
pledge. -->

```{r}

don_income_gwwc_sp

```

\

We give a scatterplot of reported donations against income, faceted by
GWWC pledge, with separate locally-smoothed conditional means (and 95%
confidence intervals for these conditional means). (The figure below is
for `r year_n-1` donations only.)

```{r}
don_income_gwwc_sp_gwwc

```

...

<!--
Unsurprisingly, those with higher incomes, and those who took the GWWC
pledge tend to report donating more. On average, the GWWC pledgers
report giving more throughout the whole range of income, and the 95%
confidence intervals are distinct for most of the range.\*
-->


<!-- ...? \^[This
agrees with what we reported in `r year_n-1`:

"In the EA Survey 2019 data, the median percentage of income donated by
someone who had taken the GWWC Pledge was 8.87%, short of the 10%
target, though there could be some noise around how respondents reported
income and donations. Nevertheless, this of course could be influenced
by GWWC Pledge takers being students, not employed or only recently
having taken the Pledge. We addressed this question in more depth last
year (link): GWWC members donate more than non-GWWC members, both
absolutely and as a percentage of income but \~40% of self-reported GWWC
members were not reporting donation data that is consistent with keeping
their pledge, a trend most likely to be the result of attrition over
time."^[Note that the smaller group who did not respond to the GWWC pledge prompt but *did* provide a donation response seems to resemble the non-pledgers. We thus lump these groups together in the subsequent analysis.]
-->

\

Next we plot donations as *shares of income* against income for non-GWWC
pledgers (combined with non-responders) and GWWC pledgers. The median
for each group is given by the dashed blue line, and the dashed red line
represents 10 percent of income.

```{r}

don_share_income_by_gwwc_sp

```


...

<!--
The relationship between income and 'share of income donated' dips down
for the lowest incomes, but for the mass of 'substantial donors' the
curve is fairly flat, and then seems to increase at higher incomes. As
expected, GWWC pledgers tend to donate closer to 10% of income than do
the rest.

In each year substantially larger shares of those who report having made
a GWWC pledge report donating 10% or more. Below, we tabulate this by
donation year and by 'whether they report having ever made a GWWC
pledge, for individuals who report income over 5000 USD and who report
zero or positive donations:
-->

```{r}

(
  tab_don_by_year_pledge <- eas_all %>%
  filter(!is.na(d_don_10pct_bc5k) & year>=2015) %>%
  mutate(`Survey year` = year,
         d_don_plan_10pct = as.numeric(donation_plan_c/income_c_imp_bc5k >=0.1),
         d_don_plan_10pct = if_else(year<2018, NaN, d_don_plan_10pct)) %>%
  group_by(d_gwwc_ever_0, `Survey year`) %>%
  summarise(n = n(), "Donated 10% of income" = mean(d_don_10pct_bc5k),
            "Donated 10% of income (plan)" = mean(d_don_plan_10pct, na.rm=TRUE)
            ) %>%
  rename("Ever GWWC pledge" = d_gwwc_ever_0) %>%
  adorn_rounding(digits = 2) %>%
  kable(caption = "GWWC pledgers: Don.  10%+ of income by survey year (exclusions: see text)", label=TRUE) %>%
  .kable_styling()
)


```

Among those who report having ever taken a GWWC pledge (and who report
donations, and excluding those reporting incomes below 5000 USD) [... share who report donating over 10%, discussion and caveats]

<!-- less
than half report donating 10% in the past year. However, this may be an
underestimate, as some people are reporting having pledged for
*this/next* year, while donation reports are for the previous
year.^[Furthermore, this does not tell us that people are failing to meet an active pledge. The question asks about having *ever* taken the GWWC pledge'; some of these people might have *ended* their pledge at some point.
]
-->

Further discussion (or link), relation to previous posts and figures, linkk to supplements...

<!--
Our [2018
post](https://forum.effectivealtruism.org/posts/fZ3Y4iYwt36Pjmwhz/ea-survey-2018-series-do-ea-survey-takers-keep-their-gwwc#How_Many_Giving_What_We_Can_Members_Are_Keeping_Their_Pledge_)
report found a rate slightly higher than 50%.\*\* This is closer to the
above figure for 'plan to donate in the current year', which hovers
around
50%.^[The rmtes we report may also be lower than those reported in the 2018 post because here we exclude those earning less than 5000 USD
.], ^[
In the [online appendix](#don_eng) ([web
link](https://rethinkpriorities.github.io/ea_data_public/eas_donations.html#don_eng)
we also plot donations by income by self-reported level of engagement
(1-3 versus 4-5). Unsurprisingly, those who report greater engagement
tend to donate more.]
-->

```{=html}
<!-- Future work on this

see thread here: https://rethinkpriorities.slack.com/archives/G01962YABHB/p1630510886012800?thread_ts=1630508970.010100&cid=G01962YABHB

was considering doing it also by
-   Joined in 2019 (high)
-   By referrer (medium)

(maybe do some of this ... for largest referrers ... either faceted or animated)
-->
```
<!-- Todo, time permitting ...  xt consider *cumulative* distributions. -->

```{=html}
<!-- Todo Medium to high --  @oska:
tables/cumulative distribution plot of share of income donated (what percentage donating more than 1%, more than 5%, more than 10%, etc);
use imputed value ... perhaps overlay this on the previous scatterplots?-->
```
### Employment and student status {#emp-student}

<!-- do similar for 'all donations in all years' -->

We present income and donation statistics for those "statuses" with more than 50 respondents in the forest plot below (a full table of statistics for each group can be found in the [appendix](https://rethinkpriorities.github.io/ea_data_public/eas-donations.html#robust-appendix)).^[In stratifying donation and income statistics by employment/student status we exclude those who gave no information on this question (or who answered that they prefer not to answer). (These nonresponses make up `r op(propNA(eas_new$status_)*100)`% of the sample).
] In each of the forest plots in this subsection, the blue line presents a simple linear best-fit of these points, and the red line represents a 10% donation rate.

```{r sumstatvec}

se <- function(x) sqrt(var(x)/length(x))


sumstatvec <- c("{median}", "{p10}-{p90}", "{mean} [{se}] ({sd})")

```

```{r doninclabs}

doninclabs <- list(income_k_c ~ "Income in $1000 USD",
                   donation_c ~ "Last year's donation (in USD)",
                   donation_plan_c ~ "Latest planned donation")

```

```{r don-inc-by-student}


don_inc_by_student <-
eas_new %>%
  group_by(status_) %>%
  mutate(
    status_ = as.character(status_),
    large_group = case_when(
                          n()<50 ~ "Other",
                           TRUE ~ status_)
    ) %>%
  ungroup() %>%
  dplyr::select(income_k_c, donation_c, donation_plan_c, large_group) %>%
  tbl_summary(by = large_group,
              type = c(all_continuous()) ~ "continuous2",
      statistic = list(all_continuous() ~ sumstatvec),
        label = doninclabs,
                  missing = c("no") ) %>%
      bold_labels() %>%
    add_n() %>%
    add_overall()


#TODO: High -- fix the column labels
#todo (low) -- we use this several times and it's a good format; let's functionalise it
#Todo (medium): Bootstrapping the SE of the median would be nice, see, e.g., https://clayford.github.io/dwir/dwr_12_generating_data.html

```

```{r don-by-career}
library(ggrepel)
#
# 1.summarize donation and income (mean and 95pct CI for each) by status_
# 2. plot median (and mean) donation by income for each group (income lowest to highest)
# 3. fit a line/curve of donation by income for each group (do for ) -- replace with the regression line based on the population not the groups
# 4. Add error bars (for donations, not income) -- hard to do for median, though

#TODO -- High Priority: Make this nice in the ways discussed (@oska it seems you have already started this)

# why are the error bars not surrounding the point?
# make it pretty (use your judgment), fix labels, add median colored dot,


(
  don_inc_status_plot <- eas_new %>%
     mutate(
  status_ = str_replace_all(
    status_, c("_" = " ")
    )
  ) %>%
  grp_sum(income_c_imp_bc5k, donation_c, status_) %>%
  plot_grp(status_, labsize=3, fp=0.3, force=5, mo=20, bp=1.5,
           arrow = arrow(length = unit(0.02, "npc"))
           ) +
  xlab("Mean income in USD (imputed if < 5k/missing)") +
  ylab("Mean donations, 95% CIs") +
      scale_y_continuous(limits=c(-10000, 30000),  oob = scales::squish)
)

# Todo (low): Plot regression line for full pop
# Todo: HIGH -- get this to look nicer, label it better, add better axis breaks (every 5k for donation, every 20k for income)
#Todo (Medium) -- add plots for the medians
#Todo

```

Donations generally track income by this aggregation, with some groups possibly 'under-performing' or 'over-performing'; we return to this in our [descriptive modeling](#descriptive).^[Note that thus is reporting means and not medians. The 'self-employed' group clearly reflects outliers, and its upper CI is truncated at 30000 to save space.
]

### Donations by country {#don-by-country}

<!-- maybe we should skip ... 2018: by Region of Residence and by City of Residence -->

**Donations and income by country**

We report similar income and donation statistics for all countries with
more than 50 respondents:

```{=html}
<!-- Note: I suspect there is 1+ (?inaccurate) donation and income outlier in France that skews the overall mean. I removed it in the data cleaning

Todo  (low): all figures in 1000Usd?


#TODO -- High Priority --  Do a grouped plot instead of just numbers ... average and disperson of income and donations by country, or?
-->
```
```{r don-inc-by-ctry}

(
  don_income_by_ctry <-
    eas_new %>%
        dplyr::select(income_k_c, donation_c, donation_plan_c, country_big) %>%
        tbl_summary( by = country_big,
                     sort = all_categorical() ~ "frequency", #reverse this ordering or maybe reverse sort by average income
                      type = c(all_continuous()) ~ "continuous2",
      statistic = list(all_continuous() ~ sumstatvec),
        label = doninclabs,
            missing = c("no")
        ) %>%
    bold_labels() %>%
    add_n() %>%
    add_overall()
)

#todo (medium?): make a stem-leaf thing here

#todo  (High): add *medians* to the above

```

\

```{r}
  # don_inc_status_plot <- eas_new %>%
  # dplyr::select(status_, donation_c, income_k_c) %>%
  # group_by(status_) %>%
  #     drop_na(status_, donation_c, income_k_c) %>%
  #     summarise(across(c(donation_c, income_k_c),
  #                     list(mean=mean,
  #                          median=median,
  #                          se = ~sd(.x)/sqrt(length(.x))))) %>%
  #   group_by(status_) %>%
p_load(ggimage)

country_codes <- tibble(country = c("Australia", "Canada", "France", "Germany", "Netherlands", "Other", "United Kingdom", "USA"),
                        code = c("ac", "ca", "fr", "de", "nl", "yt", "gb", "us"))

(
  don_inc_country_plot <-  eas_new %>%
  grp_sum(income_c_imp_bc5k, donation_c, country_big) %>%
  left_join(., country_codes, by = c("country_big" = "country")) %>%
  plot_grp(country_big) +
  xlab("Mean income in USD (imputed if <5k/missing)") +
  ylab("Mean donations, CIs") +
     scale_y_continuous(limits=c(-3000, 30000),  oob = scales::squish)
)

#Note -- plotly seems to destroy country labels here

  #+ggimage::geom_flag()

```

Above, we plot donations and income by country of residence for the
countries with the largest number of EA respondents. We fit a simple
best-fit (least-squares) line in blue, and add a red line depicting a
10% donation rate. Again, donations generally track income, with some
under and over-performers (see later modeling).^[... update to new data The UK clearly contains
some notable donation outliers, leading to very large confidence
intervals for the UK mean (truncated above at 30000 USD).]

```{r don_by_country_box, eval=FALSE}

(
  don_by_country_viol_ly <-  eas_new %>%
       plot_box_pt_viol(donation_c, country_big, notch=TRUE) +
  labs(title = "Donation amounts by country (last year)")
)

(
  don_by_country_viol_all <-  eas_all %>%
           plot_box_pt_viol(donation_c, where_live_cat, notch=TRUE) +
  labs(title = "Donation amounts by country grouping (2013-2019)")
)


(
  don_by_yr_viol_all <-  eas_all %>%
    mutate(year=as.factor(year)) %>%
    plot_box_pt_viol(donation_c, year) +
  labs(title = "Donation amounts by year")
)


```

### Donations, age and years in EA {.unnumbered}

Next, we consider how donations may increase or decrease with
'time-in-EA' (i.e., 'tenure') as well as age.^[As discussed in [other
posts](https://forum.effectivealtruism.org/posts/4xczoALF6adpQk3TN/ea-survey-2020-engagement#__Selection_and_differential_attrition__focus_on_recent__2017_2019__recruits___)
and [bookdown
chapters](https://rethinkpriorities.github.io/ea_data_public/eas-engagement.html#why-do-we-care-about-engagement),
this may be reflecting differences in *who* stays in EA (and continues
responding to the survey) as much as it reflects how people themselves
change from year to year.]

**Note on Plotly**^[I turned off Plotly here to make individual's data points stand out a bit less here.  Will probably want to jitter them as well.]


<!-- use or skip? Maybe all of this should simply be in a (regression-type) model? -->

<!-- todo (medium): try redoing the comparable faceted plots we did for engagement, swapping in a donation variable? done a bit, clean and improve?-->

```{r don_by_tenure_facet_age}

  don_by_tenure_facet_age <- eas_new %>%
  filter(!is.na(age_ranges)) %>%
  ggplot() +
  aes(x = tenure, y = donation_c_min50) +
 geom_point(size = 0.15, colour = "#0c4c8a", position = position_jitter(seed = 42,  width = 0.1, height = 0.001)) +
  geom_smooth(span = 0.75) +
  scatter_theme +
  facet_grid(vars(), vars(age_ranges), scales = "free") +
labs(title = "Last year's donation by time in EA",
     subtitle = "Faceted by Age ranges") +
    labs(x = get_label(eas_new$tenure)) +
  scale_y_don

```

<!-- bookdown_start -->

```{r}

don_by_tenure_facet_age

```

<!-- ... or cut Donations appear positively associated with tenure for nearly all age
groups, with perhaps some flattening out after 5 or so years, for some
age groups. Donations also appear positively associated with age for
each level of tenure. We return to this in our [descriptive (and
causally-suggestive) models](#descriptive). -->

We next report the comparable chart for *donation as a share of income:*

```{r don_share_by_tenure_facet_age}

donshare_by_tenure_facet_age <- eas_new %>%
  filter(!is.na(age_approx_ranges)) %>%
  ggplot() +
  aes(x = tenure, y = don_share_inc_imp_bc5k) +
 geom_point(size = 0.15, colour = "#0c4c8a", position = position_jitter(seed = 42,  width = 0.1, height = 0.001)) +
  geom_smooth(span = 0.75) +
  scatter_theme +
  facet_grid(vars(), vars(age_approx_ranges), scales = "free") +
labs(title = "Last year's donation as share of (imputed) income by time in EA",
     subtitle = "Faceted by Age ranges") +
    labs(x = get_label(eas_new$tenure)) +
    ylab(element_blank()) +
  ylim(0, 0.3)


```

```{r}

donshare_by_tenure_facet_age

```

... ^[Discussion or link, mention and link analysis 'within-referrer'... ]

<!--
As a share of income, we again see donations positively associated with time in EA, at least for the older age groups.^[This also holds when we look within groups of 'referrers' (which link took a respondent to the survey.) We report a graph on this [in the online appendix as a robustness check](#don_share_by_tenure_facet_referrer) ([web link](https://rethinkpriorities.github.io/ea_data_public/eas_donations.html#don_share_by_tenure_facet_referrer)). This suggests that the association with tenure is not entirely driven by differences in the composition of those referred to the survey.
]
-->

### By referrer {.unnumbered}

Next, we consider how donations vary by 'which referrer' (i.e., which link) took an individual to the EA survey.  Again, the blue line gives linear fit (for group means), and the red line the slope for donating 10% of income.

```{r}

(
  don_inc_referrer_plot <-  eas_new %>%
  grp_sum(income_c_imp_bc5k, donation_c, referrer_cat) %>%
  plot_grp(referrer_cat) +
    scale_y_continuous(limits=c(0, 15000), oob = scales::squish) +
  xlab("Mean income by group in USD (imputed if <5k/missing)") +
    ylab("Mean donations by group, CIs") +
    ggtitle("Donation by income and referrer")
)

# (Todo?) I wonder if we should get rid of the blue line and gray line for this … or replace it with one from an individual-based regression

```

<!-- ... (for post only) At the referrer level we see no strong association of income and donation, however, these confidence intervals are very wide. While 80000
Hours and social media appear to be 'under-performers', for most groups
of referrers the confidence intervals are too wide to make very strong
inferences. (Furthermore, as always, these differences may reflect other
underlying differences between the samples collected from these
referrers, such as differences in 'time-in-EA'.) -->

## Donation and income for recent years {#donation-and-income-trends-in-ea}

<!-- we will revisit this in tables coming out of multi-variable modeling below -->

<!-- move to methods book -->

\

*Changing EAS:*

::: {.foldable}

As the composition of EAS responses change from year to year, it may be difficult to tell whether EAs (as individual or in total) have been donating more or less in recent years. We discuss this further in the  [online bookdown supplement chapter](https://rethinkpriorities.github.io/ea_data_public/sensitivity.html).

Because of these limitations:

-   we report both totals and averages below,
-   we advise caution in interpreting the amounts and changes
-   we [return to this in a controlled model](#descriptive), which is
    also subject to similar limitations,
-   we defer more detailed analysis of this question for future work.

:::

<!--

TODO: Move this discussion to the methods book and link it.

the EAS may also not be representative of the EA population cross-sectionally, i.e., it may over or under-represent certain demographic or interest-oriented subpopulations. Still, if this unrepresentativeness is 'constant from year to year', the total reported donations in each year will at least provide a reliable measure of how the donations of this (somewhat nonrepresentative) weighted average is changing from year to year. Note that the issue of cross-sectional representativeness (and changes in this) is equally relevant to questions of 'average donations' as it is to questions of 'total donations'.
]

(Further discussion of this can be found in the
[Bookdown](https://rethinkpriorities.github.io/ea_data_public/eas-donations.html#donation-and-income-trends-in-ea)
at this point).


::: {.foldable}

1.  If the EAS response rates (and the response rates to the donation question in particular) vary year-to-year in proportion to the total size of the EA population representative, we may want to simply focus on the totals, which should move in proportion to the *true* totals.\*

2.  On the other hand, suppose the number of responses to the EAS fluctuates from year to year *not* in proportion to the size of EA, but its composition *is* representative of the EA movement as a whole. In this case it may be more reliable to report mean or median donations of EAS respondents, and combine this with extrapolations and 'guesstimates' based on separate estimates of changes in the size of EA (themselves in informed by data including the EA survey).

3.  To the extent that the EAS response total is both fluctuating (independently of the size of EA) and nonrepresentative, we may be only able make statements about changes in donations among particular subgroups, and even then the within-subgroup composition may change.
:::

-->

```{=html}
<!--
-   Total, per-capita, per-income
-   Per-capita, focusing on the largest `referrer` only (do we have this variable in previous years?)
-   Changes in causes supported
-   Adjusted and unadjusted (for differential sample composition)
-->
```

The plot and tests below depict and consider the year-to-year changes in reported donations (subject to caveats noted in the fold above).^[We exclude the year 2014, because of the very low response rate to the donation question in this survey. ]  We first consider donation rates in each year for those who answer the
donation question (reporting 0 or positive amounts). <!-- We give the share
of positive responses, the mean, median, and 80th percentile donation,
and the standard deviation for each year: -->

```{r don_by_year_tab, eval=TRUE}
library(ggstatsplot)
library(pairwiseComparisons)

(
  don_by_year_tab <- eas_all %>%
    filter(year>=2015) %>%
    sumtab(donation_c, year, caption="Donations by year", digits=c(0, 0, 2, 0, 0, 0,0)) %>%
    kable_styling() #redundant but helps with parsing

)
```

We next consider a similar report with donation-nonresponses coded as 0.

\

*Discussion/details:*

::: {.foldable}


The above only considers people who *did* answer donation questions. At
an extreme we could consider all non-responses as reflecting people who
made (little or) no donations, for a lower bound on on donation rates.
As a compromise measure, probably a tighter lower bound, we might assume
that people willing to report their incomes are generally willing to
answer financial questions. Thus if they do not report their donations
it seems *more* reasonable to suspect that they did not donate in a big
way. We thus consider the subset of the above who reported their income,
considering similar statistics as above for a modified donation
variable, coded as '0' where the donation was not reported.

:::

```{r don0_by_year_tab}

(
don0_by_year_tab <- eas_all %>%
  filter(!is.na(income_c) & year>2014) %>%
  #rowwise() %>%
  #mutate(donation_c_0 = if_else(is.na(donation_c), 0, donation_c)) %>%
  #ungroup() %>%
  sumtab(donation_c_0, year, caption="Donations by year for those reporting income (missings coded as 0)", digits=c(0, 0, 2, 0, 0, 0, -1, 0)) %>%
    kable_styling()
)
#todo  same for GWWC people (member_gwwc needs reconciling)

#todo -- include a 'total donations row'. maybe plot/graph this stuff;


```

### Graphs: donation densities {-}

<!-- Next, we present a combined scatterplot, violin plot, and stem and leaf
plot, depicting the densities of donation amounts in each year. We
present this first with level outcomes (but log scales) and then for the
'log (donation+1)' outcome. -->

```{r don_by_year_viol_test, eval=TRUE}


(
don_by_year_viol_test <- eas_all %>%
  #select(donation_c_min50, year) %>%
  #select(donation_c, year) %>%
  mutate(year = year-1) %>%
  filter(year>2014) %>%
  ggbetweenstats(y = donation_c,
                           x = year,
                          ylab = "Donations (USD)",
                 # plot.type = "violin", # type of plot
                  type="parametric",
                  conf.level = 0.95,
                # pairwise.display = "significant",
                 #p.adjust.method = "hol",
                 #results.subtitle = "false",
    title = "Donations by year, 2016-Present"
                    ) +
    theme(legend.position="none") +
                     ylim(50, NA) +
  scale_y_continuous(trans = "pseudo_log",
                     breaks = don_breaks,
                     labels = scales::dollar_format())
)


(
don_by_year_viol_test_ldon <- eas_all %>%
  #select(donation_c_min50, year) %>%
  #select(donation_c, year) %>%
  mutate(year = year-1,
         ldon1 = log(donation_c+1)) %>%
  filter(year>2014) %>%
  ggbetweenstats(y = ldon1,
                           x = year,
                          ylab = "Log (Donations +1)",
                 # plot.type = "violin", # type of plot
                  type="parametric",
                  conf.level = 0.95,
                # pairwise.display = "significant",
                 #p.adjust.method = "hol",
                 #results.subtitle = "false",
    title = "(Log) donations by donation year, 2016-Last year"
                    ) +
    theme(legend.position="none")
)


#%>%  ggplotly()

#Below: replaced this with 'wilcox, the nonparametric test' ... but note that is on top of the tests given by the ggbetweenstats command

```

<!-- ... Year-to-year differences
While the linear plots and tests of donation amounts suggest no substantial or significant differences in overall donations between these years, the log specification does suggest some year-to-year differences, with (log) 2019 donations being significantly lower than 2017 donations, even after accounting for multiple comparisons. We return to this, to some extent, in our descriptive modeling.^[While this may merit further investigation, the differences in response composition across years make statistical inference challenging, and we thus do not make this our main focus. Cross-year differences could be inflated or masked by these changes.
]

-->


<!--Note that the results are similar whether or not we bottom-code at 50 USD.-->

## Which charities (causes and categories) are EAs donating to? {#which-charities}

Only a small share of respondents report *where* they are donating. We group this into several categories summarized below, reporting for only those
`r eas_new %>% filter(num_named_dons>0) %>% count %>% magrittr::extract2("n")`
respondents who indicated at least one category of donations.

```{r  message=FALSE}

#don-statistics-category-show,

#TODO - HIGH: add better cause labels to this, visualise it in a way that conveys the aggregate shares of donations counts and amounts
 #created near the top of this file

don_stats

```

<!-- ... or skip As in previous years, 'Global health and development' is the largest
category, both in terms of number of reported donations, and in terms of
mean (and median, and 90th percentile) donations (we give the mean
including zeroes, for those who reported *any* specific category of
donation). -->


Below, we depict the amounts and density of donations for each category.

\

*Details:*

::: {.foldable}
The vertical axis is  on a logarithmic scale. The width of the violin plot depicts the smoothed density.  In the box, the horizontal lines represent medians for each, lower and upper margins of the box 25th and 75th percentiles, "whisker" lines extends from the box to to the largest (smallest) value no further than 1.5 $\times$ the inter-quartile range, and large dots represent outlying points beyond the edge of the whiskers.

:::

```{r don_by_cause_viol}
##TODO --
#sort by reverse frequency of donations to a cause

#TODO: bottom code and change the scale on this, time permitted

(
  don_by_cause_viol <-  eas_new %>%
      filter(num_named_dons>0) %>%
      select(where_don_vars, action_gwwc_f) %>%
      gather(cause, don, -action_gwwc_f) %>%
      ggplot() +
      aes(cause, don) +
      geom_violin() +
      geom_boxplot() +
    ylab("Donation amount") +
    geom_point(size = 0.30, colour = "#0c4c8a", position = position_jitter(seed = 42,  width = 0.3, height = 0.01)) +
scatter_theme +
  scale_y_log10(labels = scales::label_number_si(prefix = "$"),
    n.breaks = 10) +
      scale_x_discrete(labels = function(x) str_wrap(all_char_labels, width = 10)) +
  labs(title = "Donation amounts by category: see description above")
)

#Todo (Low to medium) ... @oska: if it's easy-ish, maybe gganimate this one across years?
#@David: Kinda difficult to do this as the variables in where_don_vars don't seem to align with eas_all

#@oska -- it is there, in variables like `donate_[charity]_year` but it would require considerable data cleaning work. Will ask/see if it's worth it.

```

\

We also check whether donations to each cause (incidence and
amounts) vary by whether the person (ever) took a GWWC pledge.

<!-- Below, we present scatterplots + violin + box plots for donation (USD amounts) to
each category, split by GWWC pledge status. -->

```{r}
#TODO -- High Priority (@oska): clean up the below to be more readable, add the mean and a CI for the mean

(
  don_by_cause_viol_gwwc <-  eas_new %>%
      filter(num_named_dons>0 & !is.na(action_gwwc_f)) %>%
      select(where_don_vars, action_gwwc_f) %>%
      gather(cause, don, -action_gwwc_f) %>%
      ggplot() +
      aes(cause, don, color=action_gwwc_f) +
      scale_color_discrete(name="GWWC pledge",
                         labels=c("No", "Yes")) +
      geom_violin() +
      geom_boxplot(notch=TRUE) +
    geom_point(size = 0.30, colour = "#0c4c8a", position = position_jitter(seed = 42,  width = 0.3, height = 0.01)) +
scatter_theme +
  scale_y_log10() +
      scale_x_discrete(labels = function(x) str_wrap(all_char_labels, width = 10))
)

```

\

*Difference from previous plots :*

::: {.foldable}

Comparing this to the "Donation amounts by category..." plot, Here, the lower and upper margins of the (now 'notched') box present an estimate of 95% confidence interval for medians (for those reporting at least one category of donations and reporting GWWC status).

:::

<!-- ... We see that these
mainly overlap, but perhaps less so for "EA meta and organization",
which GWWC pledgers seem to give more to. -->

\

Below, we tabulate donations for each cause, by group.^[The final column in each table presents a statistical test forsignificant differences in means each donation category by GWWC status. (Note that 'no significance stars' implies a lack of statistical significance at the $p<0.10$ level in two-tailed tests).]

```{r}

don_stats_by_gwwc

```

\

Next, as above, but for donation *incidence*:^[I.e., for 'whether someone
reports a donation in a particular cause category')]

```{r ddon_stats_by_gwwc}

ddon_stats_by_gwwc

```

<!-- ... Here, the differences are substantial, and in some cases, statistically
significant (three stars indicates statistical significance at the
$p<0.01$) level in a two-tailed test). -->

```{r fisher_cats}
# #TODO -- High Priority (@oska): -- the below is a mess... we want both the frequency table and test for each of these ... but how to do it. I feel like I've done this before. maybe the function in rstuff `fisherme` would help?

# TODO (high-medium):  Once we get it to work, do similar plots and tests for different 'which cause' comparisons ...

fisher_cats <- eas_new %>% filter(num_named_dons>0) %>%
  dplyr::select(all_of(where_don_vars)) %>%
  lapply(janitor::fisher.test, y = eas_new$action_gwwc_f[eas_new$num_named_dons>0], simulate.p.value=TRUE)

```

<!-- Todo (High to medium... some work done above in fisher tests): What justifies the above statement? Need numbers and tests; the scatterplot isn't informative. Simple (Fisher?) test for differences in counts of donations, GWWC vs no-pledge, separately for each charity group, perhaps?

@Oska: make a table, for those reporting at least 1 charity `filter(num_named_dons>0)`,
... of the shares indicating a donation for each charity category, separately for action_gwwc_f=0 and action_gwwc_f=1.
... and do simple tests for significant differences (perhaps Fisher test or something adjusted for multiple comparisons)
-->

```{=html}
<!-- todo: how it differs between groups ? how it relates to cause prioritisation -->
```


\

*Discussion of results above:*

::: {.foldable}
... As suggested in the first of the two tables above, among those who report a charity category, those who took the GWWC pledge tend to give ...
:::

 <!-- as much or more on average to each category (other than perhaps Long Term & AI), although none of these individual differences meet conventional statistical significance in simple F-tests (note that these tests are fairly low-powered due to small sample sizes). As the second table illustrates, GWWC pledgers are more likely to have *donated* to each of these categories, and *this* difference *is* statistically significant in standard chi-sq tests for all categories except 'Global Health and Development'. This can be seen seen in the $\chi^2$ tests in the "Donated to category" table, as well as in uncorrected Fisher's exact tests: $p=$ `r op(fisher_cats$dev_don$p.value)` for 'Global Health and Development', $p<0.01$ for all other categories. -->

## Donations: plans/aspirations vs. actual (reported) amounts {#plan-actual}

<!-- **CODING NOTE**: All testing and setup for the sections below is done in the following chunks or the sourced code -->

**To consider:** ^[Should this section be repeated every year, done in an aggregated fashion across years, or skipped going forward (just referring back to 2019-20)]

```{r filter-shape-don, warning=FALSE}

#filtering and shaping functions

f_don_plan_by_year <- function(df=eas_all, years=latest_years) {
  #adjusting for comparing planned and actual donation for same year in question (but not always for 'same individuals')
  {df} %>%
  select(year, donation_c, donation_plan_c) %>%
  gather(donation_type, value, -year) %>%
  mutate(year = if_else(donation_type == "donation_plan_c", year, year-1)) %>%
  mutate(year = fct_rev(as.factor(year)),
         donation_type = fct_recode(donation_type,
                                    "Planned Donation" = "donation_plan_c",
                                    "Donation" = "donation_c")) %>%
  filter(year %in% years)
}

f_don_last_3 <- function(df=eas_all, years=last_3_years) {
  #this is for comparing to 'planned donation' (next year)
  df %>%
 dplyr::filter(year %in% years) %>%
  group_by(year) %>%
  select(year, donation_c, donation_plan_c) %>%
  gather(donation_type, value, -year)
}

f_next_d_don <- function(df=eas_all) {
  #same as f_don_last_3, but instead of gather it constructs a differenced variable `next_d_don`
  df %>%
 dplyr::filter(year %in% last_3_years) %>%
  select(year, donation_c, donation_plan_c) %>%
  transmute(next_d_don = donation_plan_c - donation_c)
}

```

```{r present-both-planned-2019-df, warning=FALSE}
#Construct key tibbles to use in comparing planned and actual for 'this year'

demographics <- c('age', 'gender', 'country', 'employ_status')

# Filtering for those present in both datasets
planned_actual_ly_ty <- eas_all %>%
  filter(year %in% c(year_n, year_n-1) & !is.na(ea_id)) %>%
  select(ea_id, donation_c, donation_plan_c, year) %>%
  distinct() %>%
  group_by(ea_id) %>% filter(n() == 2) %>% # Filter for those appearing in both years
  pivot_wider(names_from = "year",
              values_from = c("donation_c", "donation_plan_c")) %>%

  # Remove unnecessary columns
  select(-donation_plan_c_2020, -donation_c_2019) %>%
#  drop_na() %>% # Ensure that each participant had planned donation from 2019 and actual donation from 2020 # TODO - fix, this is dropping everything
  rename(donation_last_survey_year = donation_c_2020,
         planned_donation_prior_survey_year = donation_plan_c_2019) %>%
  #2022: renamed from 'donation_ly' and 'planned_donation_ly' for greater clarity
  # Add demographic information
  left_join(., select(eas_new, all_of(demographics), ea_id, action_gwwc, start_date, end_date, income_c), by = "ea_id")

#Convert to long format for ggplot
planned_actual_ly_ty_l <- planned_actual_ly_ty %>%
  group_by(ea_id) %>%
  gather(donation_type, value, donation_last_survey_year, planned_donation_prior_survey_year)

```

```{r functions-planned-actual-for-tests-LINKED}

## helper functions

f_ly_hyp <- function(df) { #2019 data for donation difference
  df %>%
     filter(donation_last_survey_year > 0 & planned_donation_prior_survey_year>0) %>% #positive don in each year
  transmute(don_diff = donation_last_survey_year - planned_donation_prior_survey_year) #only the difference is used; this adds an 'attribute' to this object
}

```

```{r linked_tests_setup_components}

#test_rep_don_diff_mn_19
#point hypothesis of 0 mean (+attribute)
  #1000 replications of the relevant 'data'
#test_rep_don_diff_med_19: as above but for median

#test_rep_next_d_don_mn_18_20
#for actual vs *next* year's plan (means)
#test_rep_next_d_don_med_18_20 ... (medians)

#### Linked tests: New Purr testing framework #####
# ...Alternate between testing mean and median = 0 ####
mean_zero_hyp <- list(null = "point", mu = 0)
med_zero_hyp <- list(null = "point", med = 0)
hyps <- list(mean_zero_hyp, med_zero_hyp)

# ...Stats to calculate #####
stat_mean <- list(list(stat = "mean"))
stat_median <- list(list(stat = "median"))

bs_1000 <- list(reps = 1000, type = "bootstrap")

#dataframes for testing 'current less next donation' and 'actual less planned donation'
df_next_don <- eas_all %>% f_next_d_don
df_don_diff <- planned_actual_ly_ty %>% f_ly_hyp

n <- 4 # Total number of tests ... mean and median for each dataframe (better to softcode this?)

responses <- c( rep("don_diff", n/2), rep("next_d_don", n/2))
p_value_directions <- rep("two_sided", n)

# ...  Functionalize ####
dfs <- list( rep(list(df_don_diff), n/2),
             rep(list(df_next_don), n/2))

linked_df_labels <- c(rep("Last year-this year linked responses", 2),
                      rep("Last 3 years all responses", 2))
linked_test_var_type <- c(rep("Actual vs Planned", 2),
                          rep("'Next year' vs Current", 2))

```

```{r linked_tests_tibble_run_extract}

linked_tests_df <- tibble(df = do.call(c, dfs), # Dataframes (needs tidying)

                          # Stats to calculate
                          stat = rep(c(stat_mean, stat_median), n/2),

                          # Hypotheses to test
                          hypothesis = rep(hyps, n/2),

                          # Samples to generate
                          gen = rep(list(bs_1000), n),

                          # Outcome variables
                          response = responses,

                          # Direction for p-value calculation
                          p_val_dir = rep("two_sided", n))

# .... actually run tests and collect pvalues etc ####
linked_tests_df <- linked_tests_df %>%
  mutate(results = pmap(., htest_infer_sim))
  #htest_infer_sim was defined in `hypothesis_test.R`; it runs the steps in the Infer testing package with options selected based on the content of the arguments.

linked_tests_results <- extract_hyp_results(linked_tests_df) %>% #extract and label key results for reporting and plotting
  mutate(data_label = linked_df_labels,
         data_type = linked_test_var_type)

```

```{r unlinked-tests-setup, echo=FALSE, warning=FALSE}

#### Unlinked tests #####
# previous way `archive/don_unlinked_tests_old_version.Rmd`

# .... Dataframes for unlinked tests ####

unlinked_data <-
  list(
      list(eas_all %>% dplyr::semi_join(planned_actual_ly_ty, by = "ea_id")  %>% f_don_plan_by_year %>% dplyr::filter(year==year_n-1)), #those present in both years, for year 2019 only
      list(eas_all %>% f_don_plan_by_year %>% dplyr::filter(year ==year_n-2 | year ==year_n-1)), #everyone (for 2018 and 2019 donation years),
      list(eas_all %>% dplyr::filter(d_gwwc_ever == "Yes") %>% f_don_plan_by_year %>% filter(year ==year_n-2 | year ==year_n-1)), #as above but for gwwc
                      list(eas_all %>% dplyr::filter(year_involved < 2019) %>% f_don_plan_by_year %>%  filter(year ==year_n-2 | year ==year_n-1)), #involved before 2019
                      list(eas_all %>% f_don_last_3)
    ) #actual vs next year (2018-2020)

unlinked_data_labels <- c("'Matched individuals'",
                          "Full sample (2018-19 donation years)",
                          "GwwC only (2018-19 don)",
                          "Involved before last year (2018-19 don)",
                          "most recent years, all responses")

unlinked_test_var_type <- c(rep("Actual - Planned", 4), "'Next year' - 'this year'")

#... Create formulas and choose test elements for Infer package####
unlinked_formula <- reformulate(response = "value", termlabels = "donation_type") #simple formula for test

n <- length(unlinked_data) #number of dataframes
perm_200 <- list(reps = 1000, type = "permute") #what tests, how many replications #@Oska -- why did we choose 200,  I don't remember. Would this be fast enough with 1000?
hyp_unlinked <- list(list(null = "independence"))

## Order for calculation
# @David: Know of any tidier ways to do this?

d_order <- c("Donation", "Planned Donation")
d_order_next <- c("donation_plan_c", "donation_c")

d_order_diff_means <- list(list(stat = "diff in means", order = d_order))
d_order_next_diff_means <- list(list(stat = "diff in means", order = d_order_next))

d_order_diff_medians <- list(list(stat = "diff in medians", order = d_order))
d_order_next_diff_medians <- list(list(stat = "diff in medians", order = d_order_next))

```

```{r unlinked-tests-tibbles}

# ... make a tibble of the relevant dataframes and 'test formula elements'  ####
unlinked_tests_df <- tibble(df = do.call(c, unlinked_data),

                            formula = rep(list(unlinked_formula), n),

                            hypothesis = rep(hyp_unlinked, n),

                            gen = rep(list(perm_200), n),

                            stat = c(rep(d_order_diff_means, n-1),
                                     d_order_next_diff_means),

                            p_val_dir = rep("two_sided", n))

```

```{r unlinked-tests-simulate-extract, echo=FALSE, warning=FALSE}

# .... Compute simulated dataframes of difference in means ####
unlinked_diff_in_means <- unlinked_tests_df %>%
  mutate(results = pmap(., htest_infer_sim))

unlinked_diff_in_means_results <- extract_hyp_results(unlinked_diff_in_means) %>%
  mutate(data_label = unlinked_data_labels,
         data_type = unlinked_test_var_type)

# ... Simulated dataframe for difference in medians (start with same as for means but change the 'stat') ####
## (Must be separate because of the way pmap works...)
unlinked_diff_in_medians <- unlinked_tests_df %>%
  mutate(stat = c(rep(d_order_diff_medians, n-1), d_order_next_diff_medians)) %>%
  mutate(results = pmap(., htest_infer_sim))

unlinked_diff_in_medians_results <- extract_hyp_results(unlinked_diff_in_medians) %>%
  mutate(data_label = unlinked_data_labels,
         data_type = unlinked_test_var_type)

```

```{r hyp-test-results}
# Column labels
rename_test_results <- c("Statistic" = "stat",
                         "Null type" = "null",
                         "Null value" = "null_value",
                         "Point estimate" = "point_estimate",
                         "CI Lower" = "lower_ci",
                         "CI Upper" = "upper_ci",
                         "P-value" = "p_value",
                         "Sample" = "data_label")

# This can be used for plotting
full_test_results <- dplyr::bind_rows(linked_tests_results, unlinked_diff_in_means_results, unlinked_diff_in_medians_results) %>%
  select(-c(order)) %>%
  mutate(across(c(stat, null, p_val_dir), ~ snakecase::to_sentence_case(.x)))

# This forms the basis for tables/displaying stats
full_test_results_clean <- full_test_results %>%
  select(-c(reps, type, formula, p_val_dir, response)) %>%
  rename(!!rename_test_results) %>%
  mutate(Statistic = str_replace_all(Statistic, c("means" = "Mean",
                                                  "medians" = "Median",
                                                  "Diff" = "Difference")))
```

```{r hyp-test-results-tables-linked}
#making tables

#For linked tests:
current_next_test_results_clean <- full_test_results_clean %>%
  filter(data_type == "'Next year' vs Current") %>%
  select(-c(data_type, null_dist))

planned_v_actual_test_results_clean <- full_test_results_clean %>%
  filter(data_type == "Actual vs Planned") %>%
  select(-c(data_type, null_dist))

planned_v_actual_test_table <- planned_v_actual_test_results_clean %>%
      select(-c(`Null value`, `Null type`, `Sample`)) %>%
  kable(caption = "Actual minus planned donations for last year, linked participants (this year and last year)",
        digits=c(0,0,0,3)) %>%
  kable_styling()

current_next_test_table <- current_next_test_results_clean %>%
      select(-c(`Null value`, `Null type`, `Sample`)) %>%
  kable(digits=c(0,0,0,0,3), caption = "Planned minus last year's donation, 2018-20, all participants who report donations") %>%
  kable_styling()

```

```{r hyp-test-results-tables-unlinked}

#making tables for UNLINKED tests:

planned_actual_unlinked_results_table <- full_test_results_clean %>%
  arrange(match(Sample,
                c("Full sample (Prior donation years)", "Involved before last year (Prior-year don)", "GwwC only (Prior-year don)", "'Matched individuals'"))) %>%
  filter(`Null type` == "Independence" & data_type == "Actual - Planned"
) %>%
      select(-c(data_type, null_dist, `Null value`, `Null type`)) %>%
    select(Sample, Statistic, everything()) %>%
  kable(caption = "Actual versus Planned donation distributions: permutation tests",
        digits=c(0,0,0,0,0,3)) %>%
  kable_styling()


next_current_unlinked_results_table <- full_test_results_clean %>%
  filter(`Null type` == "Independence" & data_type == "'Next year' - 'this year'"
) %>%
      select(-c(data_type, null_dist, `Null value`, `Null type`)) %>%
    select(Sample, Statistic, everything()) %>%
  kable(caption = "'Next year (plan)' - 'this year' donation distributions: permutation tests",
        digits=c(1,1,1,3)) %>%
  kable_styling()


```

<!-- Note: Actual vs Planned also provides a check on the data and the year-to-year trend, to see if it is driven response -->

Do people meet or exceed the amount they intended or planned to donate for the next year?

<!-- What factors relate to this? Our surveys provide some
evidence. -->
In recent surveys, we have asked "In [current year] how much do you currently plan to donate?". We also ask "in [previous year], roughly how much money did you donate?".

*Note: timings of surveys:*

::: {.foldable}

The EA surveys have been released at various points in the year:

In 2017, the survey was released in April; thus the 'plan' was reported only about 1/3 of the way through the year (or slightly later, depending on response time).

In 2018, the survey was released in May.

In 2019, it was released in August, about 3/4 of the way throughout the year.

Thus, for each of these years, the year-to-year comparison may tell us
something about whether people lived up to their plans.

:::

<!-- xxx? This could be
particularly relevant for the 2017 and 2018 surveys, but also relevant
for 2019-20, particularly if donations tend to be clustered at years'
end (e.g., Christmas giving, Giving Tuesday in November).\*

::: {.marginote}
\* This clustering seems to hold true for giving in the USA overall.
E.g., Charity Navigator (citing the 'Digital giving index') states that
[31% of annual giving occurred in the month of
December](https://www.charitynavigator.org/index.cfm?bay=content.view&cpid=360).
:::
-->

<!--
In our [2019
post](https://forum.effectivealtruism.org/posts/29xPsh2MKkYGCuJhS/ea-survey-2019-series-donation-data)
we wrote:

> We also asked respondents how much they planned to donate in 2019. ...
> The median planned donation for 2019 was 1,074.98 USD among all EAs,
> and 3,000 USD among full-time employed non-student EAs.

Below, we compare this 2019 report of planned-2019 donation to reports
from the `r year_n` EAS of *actual* 2019 donations. We report this for several
different groupings below, as well as for other pairings of surveys.
-->

### `r year_n-1` Planned vs. actual: Individuals present in both surveys {.unnumbered}

We first consider those `r op(NROW(planned_actual_ly_ty))` respondents who can be matched across the `r year_n-1` and `r year_n` surveys.^[Matched through an anonymized email]

The plots below cover only respondents who appear in both samples *and* provide planned and actual donation values. These individuals make up `r op(length(unique(planned_actual_ly_ty$ea_id))/length(unique(eas_new$ea_id))*100)`% of the total respondents that appear in the `r year_n` sample and `r op(length(unique(planned_actual_ly_ty$ea_id))/length(unique(c(eas_all$ea_id[eas_all$year==year_n-1], eas_new$ea_id)))*100)`% of the total respondents across `r year_n-1` and `r year_n`.^[These EAs, who happened to respond to the survey in both years and give a matchable email, may not be typical EAs; we discuss this further below.]

```{r planned-actual-2019-matched-plot-creation}
# Create plots for planned and actual donations matched across 2019

scales_point_density_min50 <- list(limits = c(50, 500000), trans = scales::pseudo_log_trans(base=10),
                     breaks = breaks,
                     labels = scales::dollar_format())

planned_actual_ly_ty_density <- planned_actual_ly_ty_l %>%
  rowwise() %>% mutate(value = max(value, 50)) %>% ungroup() %>%
  ggplot() + geom_density(aes(x = value, fill = donation_type), alpha = 0.5) +
  do.call(scale_x_continuous, scales_point_density_min50) +
  scale_y_continuous(breaks = density_breaks,
                     expand = c(0,0)) +
  ggtitle("Actual vs Planned last year's donations", subtitle = "Donations bottom-coded at $50; subset: those who can be matched across surveys)") +
  theme(legend.position = "bottom",
        legend.margin=margin(t = -0.6, unit='cm')) + # Shift legend position up
  xlab("") + ylab("Density") +
  scale_fill_discrete(name = "",
                      labels = to_title_case(unique(planned_actual_ly_ty_l$donation_type)))

# Define same parameters for x and y axis
scales_point_density <- list(limits = c(0, max_lim), trans = scales::pseudo_log_trans(base=10),
                     breaks = breaks,
                     labels = scales::dollar_format())


planned_actual_ly_ty_pointdensity <- planned_actual_ly_ty %>%
  rowwise() %>%
  mutate(planned_donation_prior_survey_year = max(planned_donation_prior_survey_year, 50),
  donation_last_survey_year = max(donation_last_survey_year, 50)) %>%
  ungroup() %>%
  ggplot(aes(y = donation_last_survey_year, x = planned_donation_prior_survey_year)) +
  ggpointdensity::geom_pointdensity(adjust = 0.25) +
  geom_abline(slope = 1,
              intercept=0,
              linetype = "dotted") +
   geom_smooth() +
  do.call(scale_x_continuous, scales_point_density_min50) +
  do.call(scale_y_continuous, scales_point_density_min50) +
  scale_color_viridis_c("Neighbours") +
  #scale_size_continuous("Income", labels = scales::label_number_si()) +
  ylab("Actual last years' Donation (bottom-coded @ $50)") +
  xlab("Planned last year's donation from prior year (bottom-coded @ $50)") +
  ggtitle("Planned & actual donations  (cross-survey matches)")

#@oska I added a geom_smooth. If you can get it to work with the income-size and legends looking good, let's put that back (TODO)
#We can also trim the right horizontal  axis perhaps (maybe that can be set more generally above?)

```

Below, we plot planned and actual `r year_n-1` donations for these respondents.^[As the distribution of values is highly negatively skewed, we present this on a logarithmic scale, with values from 0-50 USD 'bottom-coded' as 50 USD.
]

```{r present-both-planned-2019}

planned_actual_ly_ty_density

```

<!-- Reassuringly, these distributions largely overlap.--> We separate the above graph by whether the individual made a GWWC pledge:

```{r planned_actual_gwwc, eval=TRUE}

(
  planned_actual_gwwc <- planned_actual_ly_ty_l %>%
      rowwise() %>%
  mutate(value = max(value, 50)) %>%
  ungroup() %>%
  filter(!is.na(action_gwwc)) %>%
  mutate(action_gwwc = as.factor( if_else(action_gwwc == 1, "GWWC Pledge", "No GWWC Pledge") ) )%>%
  # mutate(value = value + 1) %>%
  ggplot() + geom_density(aes(x = value, fill = donation_type), alpha = 0.5)
  + scale_x_log10(labels = scales::label_number_si(prefix = "$"))
  + ggtitle("Actual vs Planned 2019 donations by 'made GWWC pledge'", subtitle="Linked individuals, log scale") +
    xlab("Donation value, bottom-coded at $50") +
    ylab("Density") + facet_grid(action_gwwc ~ . ) +
  scale_fill_discrete(name = "",
                      labels = to_title_case(unique(planned_actual_ly_ty_l$donation_type)))
)


#TODO (\@oska)  -- maybe do this specifically for a year in which there is a  large gap in timing -- perhaps 2018 is the best as it was <ay (1/2 the year) and we think it's a reliable data year

#TODO: Med-high -- test for difference in planned and actual (a 'shift') and ideally test for a difference in difference between GWWC and non-GWWC

```

<!-- The above graphs do not suggest large differences in these
distributions.\^[However, 'visual inspection' risks reading patterns
into noise, and vice-versa; this speaks for formal statistical testing.
In future modeling we may also wish to consider whether this could also
reflect 'regression to the mean'; those who report planning an unusually
high (low) amount may tend to actually donate a more typical amount,
i.e., a lower (higher) amount.
-->

We ran a series of simulation-based 'permutation tests' to consider
compare the medians and means of the *distributions* of planned and
actual donations for these linked individuals. Results are included in
the table "Actual minus planned donations for `r year_n-1`, linked participants
(`r year_n-1` - `r year_n`)" in the section [Planned vs. actual: All
respondents](#plan-actual-all). ... <!-- These tests are inconclusive (not statistically significant, with wide confidence intervals),-->

^[However, note that we are currently considering these *distributions*
rather than the individual differences for linked individuals'; we
present the latter in the next subsection.]

```{=html}
<!--
\*\* The overlap appears stronger for those who made a GWWC pledge. For the non-GWWC responses, we see:

-   for smaller donations (about 10-600 USD) 'actual donation' mass somewhat the exceeds 'planned donation' mass

-   for medium-large donations (about 600-5000 USD), the reverse, as 'planned donations' somewhat exceed 'actual donations', especially for non-GWWC people

-   for the largest donations, above about 8000 or so, the 'actual' again has a somewhat greater mass than planned

We do not present statistical tests on these small intervals, nor on the differences between GWWC and non-GWWC.
-->
```

#### Donations versus plans (same individuals, linked) {#don_v_plan .unnumbered}

While the graphs and figures above help us understand whether the *distribution* of planned and actual gifts differ, it does not tell us whether any *individual's* donation meets or exceeds his or her plan. As we are considering individuals present in *both* surveys, we can connect their donation responses across years.

The graph below shows the distribution over the *difference* in planned and actual `r year_n-1` donations for those matched across the years. Here a negative value corresponds to an actual donation being lower than planned.

<!-- Here we do not split by GWWC pledge, as the graphs are, once again, similar for both groups (this also allows us to reinclude those who did not answer the GWWC question). -->

```{r gap-actual-planned-2019}

#TODO [Medium-High] -- incorporate it in so it will work for split plots, for 'same year', etc.

m_dd <- planned_actual_ly_ty %>%
  transmute(don_diff = donation_last_survey_year - planned_donation_prior_survey_year) %>%
  ungroup() %>%
  dplyr::summarize(mn_dd=mean(don_diff, na.rm=TRUE),
                   med_dd = median(don_diff, na.rm=TRUE)
  )

(
  actual_planned_2019 <- planned_actual_ly_ty %>%
  transmute(don_diff = donation_last_survey_year - planned_donation_prior_survey_year) %>%
  ggplot(aes(x = don_diff)) +
  geom_density(alpha=0.5,
               fill="blue") +
  scale_x_continuous(trans = pseudo_log_trans(base=10),
                     breaks = c((-1)*breaks*2, breaks*2),
                     labels = label_number_si(prefix = "$")) +
  geom_vline(xintercept=m_dd$mn_dd, size=1.5, color="green") + # this code is lame, we can improve it
  geom_vline(xintercept=m_dd$med_dd, size=1.5, color="red") + # this code is lame, we can improve it
  geom_vline(xintercept=0) +
#  coord_flip() +
  labs(title = "Last year's donations: actual minus previously planned",
      caption = "Red line: Median, Green line: Mean" ) +
  xlab("Actual - planned for same year") +
  ylab("")
)

# TODO: keep improving this guy

```

Planned and actual donations are... highly correlated ($\rho =$
`r op(cor(planned_actual_ly_ty$donation_last_survey_year, planned_actual_ly_ty$planned_donation_prior_survey_year, use="pairwise.complete.obs"))`).

<!-- While substantial shares report substantially less or more than their
plan, this more or less balances out, with some tendency towards
donating *more* than planned.--> ...  In fact, the mean difference between
donation and plan is `r op(m_dd$mn_dd)` USD in excess of plan (the green
line), while the median of the differences is `r op(m_dd$med_dd)` USD.

Considering that the zeroes might have been quick and uncareful mis-responses, we repeat the same plot for those who report *positive* planned and actual donations in the consecutive years, and compare these for GWWC pledgers versus non-pledgers:

```{r}



(
  actual_planned_2019_no_0_bygwwc <- planned_actual_ly_ty %>%
    filter(!is.na(action_gwwc)) %>%
     filter(donation_last_survey_year>0 & planned_donation_prior_survey_year>0) %>%
      mutate(don_diff = donation_last_survey_year - planned_donation_prior_survey_year) %>%
ggplot(aes(x = don_diff, y = as.factor(action_gwwc), fill = factor(stat(quantile)))) +
  stat_density_ridges(
    geom = "density_ridges_gradient",
    calc_ecdf = TRUE,
    quantiles = 4, quantile_lines = TRUE
  ) +
  scale_fill_viridis_d(name = "Quartiles") +
    scale_x_continuous(trans = pseudo_log_trans(base=10),
                     breaks = c((-1)*breaks*2, breaks*2),
                     labels = label_number_si(prefix = "$")) +
  geom_vline(xintercept=0) +
    ggtitle("2019 donations (no zeroes): actual minus planned, by GWWC-pledge") +
  xlab("") +
  ylab("")
)


```

...

<!-- The results are similar -- substantial shares outperformed their plans,
and substantial shares underperformed, but the positives seem to
outweigh the negatives. For both GWWC and non-GWWC pledgers the median
donation exceeds the plan (by around 200 USD).

We do not see striking differences between the GWWC pledgers and non-pledgers by this measure.^[Although the plot loosely suggests that the distribution for GWWC pledgers is shifted slightly to the right as for non-pledgers, the shift appears small. We do not pursue further testing here for several reasons. 1. There are many other differences between these groups. 2. The cross-survey tracked sample is a small share of the total sample. 3. As seen earlier, the base distribution of the *levels* of donations (planned or actual) differs between GWWC and non-GWWC; thus there is no single measure of 'which group comes closer to their plan on average'.
]

-->

<!-- Repeat for another year -->

We next present a scatterplot of planned versus actual donations for
2019, for those who can be matched across surveys. In the figure below,
the brightness of a color indicates the density of respondents (number
of 'neighbors') with a particular combination of planned and actual
donations.

```{=html}
<!--
To do, future work

4.  Do across years (not 'for people present in every year' because there are few like that)

-   graph 'mean (and spread?) of p75 actual donations for year XX' versus 'planned donations for year XX'

Later, perhaps it's worth 'modeling' this a bit more formally.

Future work could involve: modeling donation and planned donation more carefully across all years

And further tests
- https://stats.stackexchange.com/questions/315601/chi-square-test-alternatives-to-test-for-shift-in-distributions
- Kolmogorov-Smirnov test (for shift in distribution)
- Mann-Whitney test (rank-sum)
- quantile regression or permutation test for difference in
   - medians
   - 75th percentiles of distribution

-->
```
```{r planned_actual_ly_ty_pointdensity}

planned_actual_ly_ty_pointdensity

```

```{=html}
<!--
Todo (low to medium) -- do something else with income or get rid of because width=size or number of obs in most people's heads.

Todo (medium) -- @Oska either animate or do separately for different years?

 add a regression line (perhaps outlier-insensitive) ...

 modeling question: what affects the 'slope of planned in actual' = function of (planned amount, time to realization, season, gwwc, cause prioritization, student status)
 -- alt -- model across years as 'percentage gap, actual-planned/planned, or  (actual-planned/planned)/(share of year remaining)

-->
```
Overall, the plot is more or less centered around the 45 degree line of
'plans=actual'. There are noticeable departures in each direction, but
these seem to balance out. Thus, we might loosely conclude that 'on
average
`r op(length(unique(planned_actual_ly_ty_pointdensity$data$ea_id)))`
individuals who can be matched across years tend to donate an amount
close to what they planned'. However, there may nonetheless be important
differences, so we test further.

Below, we plot donations for these linked individuals -- actual
donations are on the left, and planned donations are on the right. We
overlay a 'violin' density plot (widths depicts the frequencies).
Medians are depicted in red dots, and the boxes depict 25th and 75th
percentiles. The lines show each individual's donation (on the left)
connected to her plan (on the right). The plot also reports on a
Wilcoxon signed-rank test (for paired data).

```{r wilcoxon-matched-dons}
(
matched_dons_wilcoxon <- planned_actual_ly_ty_l %>%
  mutate(donation_type = to_title_case(donation_type)) %>%
ggstatsplot::ggwithinstats(
  x = donation_type,
  y = value,
  type = "nonparametric",
  paired = TRUE,
    point.path.args = list(alpha = 0.1,
                           linetype = "solid"),
) +
  do.call(scale_y_continuous, scales) +
  xlab("") + ylab("") +
  scale_fill_discrete(name = "")
)


w_signed_test_planned_actual <- wilcox.test(x = planned_actual_ly_ty$donation_last_survey_year, y = planned_actual_ly_ty$planned_donation_prior_survey_year,
              alternative = c("greater"),
            mu = 0, paired = TRUE, exact = NULL, correct = TRUE,
            conf.int = TRUE, conf.level = 0.95,
            tol.root = 1e-4, digits.rank = Inf)

#a Wilcoxon signed rank test of the null that the distribution of ... x - y (in the paired two sample case) is symmetric about mu is performed.

#Here the CI estimates 'the median of the difference between a sample from x and a sample from y.'

```

The nonparametric tests reported above find a statistically significant
difference: actual donations tend to exceed planned donations in this
sample, and this difference is unlikely to be due to chance. The
'pseudo-median' of this difference is estimated as
`r op(w_signed_test_planned_actual$estimate)` USD with 95% lower CI
bound `r op(w_signed_test_planned_actual$conf.int[1])`. The
"matched-pairs rank-biserial correlation" is also bounded between about
0.17 and 0.41, suggesting that "actual donation exceeds planned
donation" is more likely than "planned exceeds actual" (in the
population that this is drawn from).


```{r wilcoxon-matched-dons_no-0}

# For those who report a donation in each year...

planned_actual_ly_ty_no_0 <- planned_actual_ly_ty %>%
  filter(donation_last_survey_year>0 & planned_donation_prior_survey_year>0)

w_signed_test_planned_actual_no0s <- wilcox.test(x = planned_actual_ly_ty_no_0$donation_last_survey_year, y = planned_actual_ly_ty_no_0$planned_donation_prior_survey_year,
              alternative = c("greater"),
            mu = 0, paired = TRUE, exact = NULL, correct = TRUE,
            conf.int = TRUE, conf.level = 0.95,
            tol.root = 1e-4, digits.rank = Inf)

#a Wilcoxon signed rank test of the null that the distribution of ... x - y (in the paired two sample case) is symmetric about mu is performed.

#Here the CI estimates 'the median of the difference between a sample from x and a sample from y.'

```

We next present simulation-based tests for whether the mean and median
of the individual 'actual minus planned' donations exceeds or falls
below zero.

```{r}
planned_v_actual_test_table
```

<!-- Todo (@oska, medium-priority): add the graph of the simulated distribution for the median here, and then we can explain it a bit -->

```{r linked_ddm}

linked_ddmn <- linked_tests_results %>% filter(response=="don_diff" & stat=="mean")
linked_ddmed <- linked_tests_results %>% filter(response=="don_diff" & stat=="median")

```

The mean of 'actual minus planned' donations is
`r op(linked_ddmn %>% extract2("point_estimate"))` USD, with
simulation-based (bootstrapped) confidence intervals
[`r op(linked_ddmn %>% extract2("lower_ci"))`,
`r op( linked_ddmn %>% magrittr::extract2("upper_ci"))`], with
corresponding p-value
`r op(linked_ddmn %>% magrittr::extract2("p_value"))`. For the median of
this difference we have point estimate
`r linked_ddmed %>% extract2("point_estimate") %>% op()` USD, with
simulation-based (bootstrapped) confidence intervals
[`r op(linked_ddmed %>% extract2("lower_ci"))`,
`r op( linked_ddmed %>% magrittr::extract2("upper_ci"))`], with
corresponding p-value
`r  op(linked_ddmed %>% magrittr::extract2("p_value"))`. Thus, the
evidence points towards 'actual donations exceeding planned donations
for those EAs who can be linked across the past two years'. <!-- ... However, (unlike in the Wilcoxon signed-rank tests) the differences are not strongly statistically significant in these simulation-based tests. -->

### Planned vs. actual: All respondents (across relevant years) {#plan-actual-all .unnumbered}

Those who responded to both `r year_n-1` and `r year_n` surveys (and left emails both
times) might tend to be the more engaged EAs, suggesting that the above figures may be
biased towards more 'fulfilled plans'.^[In particular, having
fulfilled one's planned donation might make one more likely to want to
complete the follow-up survey, and perhaps more keen to provide one's
*donation* data in particular.]

Thus, we next overlay the planned and actual donations for *all
respondents* across both surveys.^[Here we compare the 'amounts planned
for the year of a survey' to the 'amounts reported for the previous
year, in the following year's survey'. We do this separately for each
available year.  While this offers us a larger sample, and may be less vulnerable to the bias just-mentioned, it brings up other sample selection issues, and
these comparisons should also be treated with some caution (see fold/footnote).]

::: {.foldable}

Indeed, '2019 respondents who entered a planned donation amount' (call
these '2019-planners') may not be precisely representative of the
population of interest. Still, we might at least seek an
'internally-valid' measure of the 'distribution of actual `r year_n-1` donations
for the 2019-planners', and compare their actual to their planned 2019
donations. This will still be imperfect: the composition of the `r year_n-1` and
2020 respondents may differ, as discussed elsewhere.

Thus, the distribution of 'reported `r year_n-1` donations for those who
completed the survey in `r year_n`' (call these '2020-reporters') may be
different from the distribution of the actual `r year_n-1` donations made by
2019-planners. The direction and nature of this bias is unclear; we
might get some hints at this by comparing the reported donations in 2018
and `r year_n-1` respectively by 2019-reporters and `r year_n`-reporters. If the
distribution of donations changes little from year to year, we might
worry less about this bias. We defer this for future work.

Below, we also report these measures for those who joined EA only before
2019 (plausibly a more stable group).
:::

<!-- If we have time/later,  ...  do the same for only those 'referrers' with similar numbers in each year. -->

<!-- @Oska; see above let's plot the distribution of reported donations for each year, or some statistics on this that gives us a sense of whether it's changing much. -->

```{r}



(
  dons_planned_across_all <-  eas_all %>%
  f_don_plan_by_year %>%
  ggplot(aes(x = value, y = as.factor(year), fill = donation_type)) +
  geom_density_ridges(alpha=.6,
                      color = "black",
                      quantile_fun = median,
                      quantile_lines = TRUE,
                      rel_min_height = 0.005)  +
  # geom_vline_med(x) +
  scales_set +
  ridge_bottom_opts +
  xlab("") +
  guides(fill = guide_legend(override.aes = list(linetype = 0))) +
    labs(title= "Density of planned and actual donations for each year",  subtitle = "Vertical lines: medians for the year and donation type",
         caption = "Donations bottom-coded at $50")
)

```

```{=html}
<!--

(above)
TODO @oska -- can you move these lists of 'display format parameters' elsewhere, to a list, or a set of lists, as I've done for other things, and then "call these" in the command? otherwise it's hard to read the code
-->
```

*Discussion of above chart:*

::: {.foldable}

... For `r year_n-1` (`r year_n` survey 'actual' and `r year_n-1` survey 'planned') and 2018 (`r year_n-1`
survey 'actual' and 2018 survey 'planned'), the histograms of planned
and actual donations line up approximately (although planned donations
tend to be a bit higher). However, for 2017 (2018 survey 'actual' and
2017 survey 'planned'), the planned donation distribution appears *far*
lower. This seems likely to result from a different response and a
different composition between the 2017 and 2018 responses.\^[There was
an increase in the sample size and the response rate to the donation
questions from 2017 to 2018 below. We report this in the [bookdown
'robustness'
appendix](https://rethinkpriorities.github.io/ea_data_public/eas-donations.html#robust-appendix).

... Note that the number of EA survey respondents also ... <!-- declined --> in `r year_n`,
from `r op(length(eas_all$ea_id[eas_all$year==2019]))` to
`r op(length(eas_all$ea_id[eas_all$year==2020]))`. If the EA survey
tends to select only the more engaged EAs, this would suggest that the
extent `r year_n-1` donations under-performed plans may be even *higher*. ]

(Thus we will *not* include 2017 in our "planned versus actual"
comparisons.)

As noted, for the remaining relevant donation years (2018 and `r year_n-1`) the
median donation is somewhat *lower than* the median planned donation,
suggesting *under-performance* relative to plans.

:::

**Over the most recent years, how do the distributions of planned versus actual donations differ?**

In spite of the caveats above, we consider and test whether the distribution of planned donations for a year exceeds or falls short of actual donations, pooling across recient years.
<!-- pooling the 2018-2019 and 2019-2020 data (to consider donations vs plans in 2018 and 2019). -->
We do this separately both overall, and  excluding those who joined EA only before `r year_n-1` (plausibly a more stable group).

<!-- Todo: I want to also do this for a large/consistent set of 'referrers' or 'first heard' group but this will take a lot of cleaning ... see eas_all %>% tabyl(first_hear_ea, year) -->

```{r dons-plan-actual-hist-19-20}

 #TODO -- medium-high priority: some depiction of quantiles/cutoffs within each smoothed histogram (for all the ones below, even the faceted ones). See, e.g., https://stackoverflow.com/questions/57563692/combining-facet-wrap-and-95-area-of-density-plots-using-ggplot2/57566951#57566951


dons_plan_hist_opts <- function(df) {
  df %>%
      ggplot(aes(x = value, fill = donation_type)) +
      geom_density(alpha=.35) +
  scales_set +
  #geom_vline_med(x) +
  ridge_bottom_opts +
    xlab("")
}

#crappy workaround here:
x <- eas_all %>%
  filter(year %in% last_3_years) %>% f_don_plan_by_year

xo <- eas_all %>% filter(year_involved<2019) %>%  filter(year %in% last_3_years) %>% f_don_plan_by_year

x_med_don <- median(x$value[x$donation_type== "Donation"], na.rm=TRUE)
x_med_don_plan <-  median(x$value[x$donation_type== "Planned Donation"], na.rm=TRUE)
xo_med_don <- median(x$value[xo$donation_type== "Donation"], na.rm=TRUE)
xo_med_don_plan <-  median(xo$value[x$donation_type== "Planned Donation"], na.rm=TRUE)


dons_planned_last_2_years <- eas_all %>%
      filter(year %in% last_3_years) %>%
    f_don_plan_by_year %>%
  rowwise() %>%
  mutate(value = max(value, 50)) %>%
  ungroup() %>%
  dons_plan_hist_opts +
  geom_vline(xintercept=x_med_don, size=.5, color="green") +
  geom_vline(xintercept=x_med_don_plan, size=.75, color="pink") +
ggtitle("2018-19")


dons_planned_last_2_years_no_new <- eas_all %>%
          filter(year_involved< year_n-1) %>%
      filter(year %in% last_3_years) %>%
        f_don_plan_by_year %>%
  dons_plan_hist_opts +
  geom_vline(xintercept=xo_med_don, size=.5, color="green") +
  geom_vline(xintercept=xo_med_don_plan, size=.75, color="pink") +
  ggtitle("2018-19")

(
dons_planned_last_2_years_arr_new <- ggarrange(dons_planned_last_2_years, dons_planned_last_2_years_no_new,
                                        ncol = 2, nrow=1,
                                        labels=c("All", "Involved pre-last-year"),
                                        label.x=0.0,
   label.y=0.85,
   legend="bottom",
 common.legend=TRUE                                 ) +
     labs(title= "Density of planned and actual donations, split by year-invo",  subtitle = "Vertical lines: medians for the group and donation type",
         caption = "Donations bottom-coded at $50")
)

 #todo (@oska) -- finish this; separate aligned plots for
 # - is it doing what I think? I think we want to use 2020 for the sums in  'actual' but not in 'planned'
 # - align the above, fix labels
 # - report some more stats within each as 'bars' or shading (geom lines could also be good)
 # - stat tests for each

```

Above, we give the density of planned and actual donations, split by
year-involved. Vertical lines represent medians for the group and
donation type (green=donation, pink=planned donations). Donations are
bottom-coded at 50 USD. Again, for both groups (where the survey entries
are *linked*) the *planned* donation distribution appears to be somewhat
higher than the actual distribution, although the difference is not
dramatic.

We present the results of simulation-based permutation tests below.
<!-- bookdown_start --> (Explanation in fold). <!-- bookdown_end -->

<!-- TODO (@oska, medium-highpriority): add the graph of the simulated distribution for the difference in median here for the full sample, with the CI's and point estimates overlaid, and then we can explain it a bit -->


*Explanation of permutation tests:*

::: {.foldable}

We use *permutation tests* for testing whether the median (and mean) of planned donations exceeded/fell short of the mean for actual donations, but using the data from different years' surveys (without connected individuals).

The null hypothesis (for `r year_n-1` donations) is that:

'there is no difference in the median donation, in our survey sample, between actual `r year_n-1` donations (reported in `r year_n`) and planned `r year_n-1` donations (reported in `r year_n-1`).'

Suppose we maintain the hypothesis that

'individuals appeared in the `r year_n-1` survey versus the `r year_n` survey 'as if randomly drawn'',  and we consider that under the null hypothesis "the distribution of planned and actual donations is identical" (this may be a stronger assumption than needed).

The permutation procedure repeatedly simulates a distribution of planned and actual donations that is consistent with this null, by using our original donation amount data and randomly re-assigning each observation to either 'planned' or 'actual'. For each 'simulated null distribution' we can then compute the targeted statistic (difference between mean/median donation between the two groups). We can plot this 'simulated null distribution of differences' and consider 'how often do we observe a difference as extreme as the 'point estimate' from our *actual* data'? This yields the p-values reported below.

The confidence intervals for the differences come from simply using the 95% interval range from the simulated distribution, shifted to be centered around our point estimate.
:::

The figure below presents the simulated null distribution of differences in medians arising from this procedure, for the full sample (`r year_n-2` - `r year_n-1` donation years).

NOTE: we lost the image here


*Explanation*:

::: {.foldable}

The red line gives the point estimate of 'differences in median between planned and actual' from our data. The grey bins present the '(simulated) distribution of differences between planned and actual, under the null hypothesis that planned and actual are drawn from the same distribution'.

The 'pink areas' depict the area of the simulated null distribution that is 'more extreme' than our point estimate; this iis the area represented by the 'p-value' of a two-tailed hypothesis test.

Finally the turquoise should represent a 95% CI for the actual median;
essentially this comes from a simulated distribution under the hypothesis that the true difference between the medians is exactly equal to our point estimate.

Note that this turquoise region just crosses the 0, again confirming that we 'just barely cannot reject the null', but that a Bayesian analysis would probably put a lot of probability mass on fairly
substantial differences.
:::

<!-- bookdown_end -->

In the table below we summarize the results of this test for means and
medians, and for four distinct subsamples.

```{r}

planned_actual_unlinked_results_table

# report on

#test_rep_mean_don_19_20
#test_rep_med_don_19_20

#test_rep_mean_don_19_20_gwwc
#test_rep_med_don_19_20_gwwc

#test_rep_mean_don_19_20_nonew
#test_rep_med_don_19_20_nonew

```

For the full sample, for the subset who have been in EA since before
`r year_n-1`, and for GWWC pledgers, both the mean and median donations [...] fall
short of the planned donations. Overall, the difference in medians is
bounded between about ... <!-- `r full_test_results_clean$year` 300 and 600 USD. -->
<!-- ... Each of these differences (for the medians) are strongly statistically
significant. Perhaps because of the large outliers, the differences in
means are much more widely bounded, and thus this comparison is largely
uninformative. -->

... re-state results... This contrasts with <!-- our results for individuals that can be *matched* across `r year_n-1` and `r year_n` surveys, for whom actual donations tend to *exceed* the reported plans in the prior year.-->^[As noted earlier, this contrast might be because people who *have* met or exceeded their donation plans are more likely to respond to surveys in subsequent years, and report donations in each. As discussed earlier, for the 'matched individuals' (those who can be linked from `r year_n-1` and `r year_n` surveys) the mean and median of the distribution of actual donations exceeds that of planned, but these differences are not statistical significant in the permutation tests. However, for the matched individuals, these differences are positive and significant (or close to significant) in tests that take advantage of the linked nature of this data and look at the individual differences themselves.
]

<!-- Given these contrasting findings, further work might be warranted. If
donations *do* tend to underperform plans, we might look for patterns of
underperformance that might suggest ways of improving this. -->

## Donations versus *next year's* plans {#next-year}

[...] In each of the years where it was asked, most respondents who answered the retrospective donation question also answered the 'planned for this year' question. We can see how these tend to relate; we may particularly consider whether `r year_n` donations are expected to be higher or lower than `r year_n-1`, in light of recent events.

<!-- ... the pandemic (cf the [Giving
Tuesday](https://www.givingtuesday.org/blog/givingtuesday-data-commons-report-donors-shifted-and-boosted-giving-in-response-to-crisis-in-2020/)
report suggesting growth in overall US charitable giving in 2020). -->

Below we overlay the distribution of 'last year's donations' and 'planned current year' donations for the most recent surveys.

```{=html}
<!--
Todo @Oska: let's redo something like `planned_actual_ly_ty_pointdensity` and `actual_planned_2019_no_0`

But
- across years (eas_all)
- for 'plans for this year' (y axis) vs 'actual last year' (x axis), but featured/overlaid or faceted by year (for the relevant years)

-->
```
```{r don-vs-next-year-plan}

next_don_lab <- c(donation_c = "Don: Last year", donation_plan_c  = "Don: This year (plan)")

(
  dons_v_next_last_2_years <- eas_all %>%
    f_don_last_3 %>% ggplot(aes(x = value, y = as.factor(year), fill = donation_type)) +
  geom_density_ridges(alpha=.6,
                      color = "black",
                      quantile_fun = median,
                      quantile_lines = TRUE)  +
  # geom_vline_med(x) +
  scales_set +
  ridge_bottom_opts +
  guides(fill = guide_legend(override.aes = list(linetype = 0))) +
    labs(title= "Density of last year vs current year planned donations",  subtitle = "Vertical lines: medians for the year and type of report") +
    scale_color_brewer(labels=next_don_lab) +
  scale_fill_brewer(labels=next_don_lab) +
  xlab("") +
    theme(legend.title=element_blank()) +
          guides(fill = guide_legend(reverse = TRUE))
)


  # Todo -  medium: It's still not clear what is going on from year to year... maybe try animated?

  #  Todo -- medium/high: get bars or color separation for quantiles (probability mass) within each histogram


```

<!-- ... re-characterize In each year, the median of planned donations exceeds that of actual
donations. The distribution of each of these appears fairly constant
across years, with no obvious substantial drop for 2020. -->

Next we plot this in two dimensions: for each individual we plot their planned current year's donation against their reported donation for the year prior to the survey.

```{r prepare_current_planned}

current_planned_eas <- eas_all %>%
  select(ea_id, donation_c, donation_plan_c, year) %>%
  # Add demographic information -- cut because it was crap
  #left_join(., select(eas_new_cy, all_of(demographics), ea_id, action_gwwc, start_date, end_date, income_c), by = "ea_id") %>% #remove likely duplicate entries (do that elsewhere too?) ; this also removes entries from years without any responses to these I guess
  distinct()

```

```{r current_planned_pointdensity}

(
current_planned_pointdensity <- current_planned_eas %>%
  # ggplot(aes(y = donation_2019, x = donation_plan_c)) +
  filter(year>=2018) %>%
  rowwise() %>%
  mutate(donation_c = max(50, donation_c),
         donation_plan_c = max(50, donation_plan_c)) %>%
  ungroup() %>%
  ggplot(aes(x = donation_c, y = donation_plan_c)) +
  ggpointdensity::geom_pointdensity(adjust = 0.25) +
   geom_smooth() +
  do.call(scale_x_continuous, scales_point_density_min50) +
  do.call(scale_y_continuous, scales_point_density_min50) +
  scale_color_viridis_c("Neighbours") +
  #scale_size_continuous("Income", labels = scales::label_number_si()) +
  ylab("Planned (next year's) Donation") +
  xlab("Actual (this year's) donation") +
   labs(title= "Last year's donation vs this year's (planned) donation",
       subtitle = "most recent years; donations 'bottom-coded' at 50 USD")
)

#TODO: --medium  importance -- facet or animate this across years'

```

The graph's implications <!-- ...  are not obvious. There is a large mass
*exactly* along the 45 degree line, where the donation amount planned
for the current year equals the amount reported for last year. There
seems to be some substantial mass where planned donations exceed actual
donations (above the 45 degree line), the smoothed curve is largely
positioned at or below this line, perhaps because of the very small and
zero entries for planned donations. -->

We repeat the above plot but only for those who report positive values
for both 'the previous year' and for 'planned for this year':

<!-- TODO: Move this one to the appendix because it looks almost identical to above -->

```{r current_planned_pointdensity-no0}

(
current_planned_pointdensity_no0 <- current_planned_eas %>%
  # ggplot(aes(y = donation_2019, x = donation_plan_c)) +
  filter(donation_c >0 & donation_plan_c>0) %>%
    filter(year_n-2>=2018) %>%
 rowwise() %>%
  mutate(donation_c = max(50, donation_c),
         donation_plan_c = max(50, donation_plan_c)) %>%
  ungroup() %>%
  ggplot(aes(x = donation_c, y = donation_plan_c)) +
  ggpointdensity::geom_pointdensity(adjust = 0.25) +
   geom_smooth() +
  do.call(scale_x_continuous, scales_point_density_min50) +
  do.call(scale_y_continuous, scales_point_density_min50) +
  scale_color_viridis_c("Neighbours") +
  #scale_size_continuous("Income", labels = scales::label_number_si()) +
  ylab("Planned (next year's) Donation") +
  xlab("Actual (this year's) donation") +
labs(title= "Last year's vs this year's (planned) donation",  subtitle = "For EAs reporting a positive amount for each",
     caption="Last twi tears, donations 'bottom-coded' at 50 USD" )
)

#TODO: --medium  importance -- facet or animate this across years'
```
### Simulation-based tests {-}

**(1) (Unpaired) differences in the medians and means of distributions**

```{r}
next_current_unlinked_results_table
```

<!-- ... recharacterize
The mean of the distribution of the current-years' planned donations is
higher than the mean for the previous year's, but we cannot reject
equality (the p value is far above the conventional statistical
threshold). However,-->

The median is <!-- statistically significantly --> higher
(by `r op(full_test_results_clean[[14,5]])` USD point estimate).

**(2) Differences by individual:**

```{r }

#report-test_rep_next_d_don_med_18_20-etc

current_next_test_table

```

Discussion: ...

<!-- ... The median difference is clearly 0 -- the great middle mass of
participants report the same donation planned for the current year as
for the previous one, and this is the case in all resampling
simulations. However, the mean difference is strongly and significantly
positive: if we consider the magnitude of the differences, people tend
to report a greater planned donation for this year than they reported
last year. However, this does *not* necessarily indicate over-optimism
and underperformance: it is possible that the individuals responding to
individual survey in this period in fact *did and do* increase their
donations from year to year. -->

## Model of EA donation behavior {#modeling}

### Modeling 'questions' and approaches: {.unnumbered}

We differentiate three categories of modeling: descriptive, predictive, and causal. We discuss these further [Link to methods bookdown here].


*The three categories, briefly:*

::: {.foldable}

The three categories are

1. Descriptive ('what relates to donation behavior'),
2. Predictive ('what will people with particular characteristics donate in the future'), and
3. Causal ('what factors actually *determine* the amount donated'; so
if we changed these factors, donations would change).

:::


*Brief: why might we care about causality here?*

::: {.foldable}

We may care about causality

- because we see potential to intervene and boost those variables that  cause greater giving, and/or because

- a better understanding of what actually *drives* donation behavior may yield additional insights, helping us understand the world better

:::


*Brief: why might we care about prediction here?:*

::: {.foldable}
We may care about prediction because

(... add brief discussion here)

:::



*Brief: our descriptive model 'selection'*

::: {.foldable}
In our *descriptive* modeling we do *not* remove 'insignificant' features from the model (as in stepwise regression), nor do we shrink the coefficients towards zero (as in Ridge and Lasso models). Under the assumptions of the classical linear model and its simple extensions the coefficients we present here would be unbiased or consistent. (However, we admit that the strong assumptions of this model, particularly those embodying exogeneity, are likely to fail in important ways in the current non-experimental setting.)
:::

We retain those features of most direct interest (such as 'how introduced to EA') and/or theoretical importance (such as income),^[Classical economic theory suggests that most goods are 'normal', with the amount consumed increasing in income. Empirical evidence supports this for charitable giving; [recent work](https://econofact.org/are-rich-people-really-less-generous) suggests that *share of income* is relatively constant across the income distribution, implying that wealthier people give more in absolute terms.] and 'controls' (especially time-in-EA and survey-year) that might allow us to better interpret the features of interest.^[As discussed in [other posts](https://forum.effectivealtruism.org/posts/tzFcqGmCA6ePeD5wm/ea-survey-2020-how-people-get-involved-in-ea) 'how people were introduced to EA' has changed over the years. We also expect those who have been involved in EA for a longer period of time to be more engaged, and perhaps donate more (possibly due to differential attrition). Thus, it seems reasonable that in tracking the association between donation and introducer, we might want to 'control for' (difference out or hold constant) the differences in 'time-in-EA' between these groups. Still, we admit that these specifications are not based on an explicit causal model or identification strategy.]

#### Choosing features and modeling targets {.unnumbered}

We construct several 'feature sets':

-   "Key demographics, student status, and geography", used in all models 
-   "Career/Economics": (Income, employment status, top-6 university)^["Top-6 university" refers to whether the individual lists any of the six universities (Oxford, Stanford, Harvard, CalTech, MIT, Cambridge) appearing in the top-10 of all of USNWR, QS, and THE rankings. However, university was not asked in the 2018 survey ]
    <!-- feat_income_employ -- -->
-   "Pledges/commitments:" Whether ever taken a 'Giving What We Can
    Pledge', whether 'Earning to Give' <!-- feat_gwwc_etg -->
-   "Controls" for age, time-in-EA, and survey-year (used in all
    models)^[We refer to the latter as "controls" because they aid our
    interpretation of other features of interest, as noted above.
    However, these are *also* of independent interest.]

<!--we will check whether this has an impact on the coefficient for year 2018.-->

**We focus on three key outcomes:**

1.  Amount donated (converted to US dollars)^[Here we focus on the average of last-year's and next year's donation for each individual, where both are present, and otherwise we use whichever one is present. In the presence of recall and reporting error, we expect this will improve the reliability of our estimates. The [plots and figures above](#plan-actual) also suggest that planned and actual donations are closely related. We report a set of related results for the simpler donation outcome in the robustness appendix.]

2.  Donation as a share of income^[Where income is missing or where income was reported as 0 we impute it based on student status and country.]

3.  Whether donated more than 1000 USD

```{r feature_sets_don, echo=FALSE, warning=FALSE}

#moved to build side cross-year...
# eas_all %<>% mutate(
#   l_don_c = log(donation_c+1),
#   l_don_av_2yr = log(don_av2_yr+1)
# )

  #THIS is adapted from the engagement modeling
  #TODO: adapt to donations further, considering important explanatory variables
  #TODO: subset of these that appear across all relevant years (or )
  #Todo (maybe?) put these back into the eas_all construction

#targets:
bin_out <- c("d_don_1k", "d_don_10pct")

num_out <- c('donation_c', 'don_av2_yr', 'l_don_c', "l_don_av_2yr", "don_share_inc_imp_bc5k", "donation_plan_c")
targets <- c(bin_out, num_out)
targets_short <- c("don_av2_yr", "don_share_inc_imp_bc5k", "d_don_1k") #Note -- don_av2_yr is the right one for qpoisson as it already expresses things in exponents. l_don_av2_yr was the one to use in the loglinear model, which we are not emphasizing

  # BUT it gets complicated -- maybe we need to code a matrix/spreadsheet here for the model choices
  #TODO: HIGH
  # -- `l_don_av_2yr` -- only relevant for linear (not QPoisson) models, in Qpoisson we should use `donation_c`
  # -- `d_don_1k` -- really deserves a logit model, optimally
  # -- Once we get the multi-year going we need year dummies
  # -- For the 'impact of first heard' probably we can focus only on donation/income?

targets_short_names <- c("Log (Avg don +1)", "Don/Income", "Donated 1k+")

#features and controls
geog <- c("where_live_cat", "city_cat")
key_demog <- c("ln_age", "not_male_cat", "student_cat", "race_cat", geog)
key_demog_n <- c("age_d2sd", "not_male_cat", "student_cat", "race_cat", geog)

#note: I added d_top6_uni even though it is entirely missing in 2018. (Todo) Let's check whether it's inclusion makes a difference for the estimation of the 'year=2018' coefficient. If so, recoding dummies with 'contrast codings' may fiz this problem.

feat_income_employ <- c("ln_income_c_imp_bc5k", "d_pt_employment", "d_not_employed", "d_top6_uni")

#Note -income_c_imp_diqr has been adjusted to  with 5k minimum

feat_income_employ_n <- c("income_c_imp_diqr", "d_pt_employment", "d_not_employed", "d_top6_uni")

#feat_income_iqr_employ <- c("income_c_imp_diqr", "d_pt_employment", "d_not_employed", "d_top6_uni")

#note The IQR is about 32000 or 55084 if we look at those who reported

feat_gwwc_etg <- c("d_gwwc_ever_0", "d_career_etg")

#feat_fh <- c("first_hear_ea_lump")

#feat_2020 <- c("uni_higher_rank_d2sd") #features (thus far) available for 2020 only
#NOTE: we can include this one as it's normalized, even though it's missisng for  years other than 2020 for now. Just don't read too much into it nor into the year dummies when this is included, perhaps.

controls <- c("ln_years_involved", "year_f") #note this assumes those 2009 or earlier started in 2009

controls_n <- c("years_involved_d2sd", "year_f") #note this assumes those 2009 or earlier started in 2009

robust_controls <- c("ln_years_involved_post_med",  "ln_age_if_older", "ln_income_c_imp_if_richer")

robust_controls_n <- c("years_inv_d2sd_post_med",  "age_d2sd_post_med", "income_c_imp_diqr_if_richer")

#note -- I swapped "age_ranges" for "gender_d2sd" because this will allow us a decent measure of 'is the age effect nonlinear' (see datacolada on the superiority of this over a quadratic)

#contrasts(normtimeBP02$Nasality) = contr.treatment(4)
#DR; I was never able to get the contrasts thing to work so I went with 'base group is most common group'

```

```{r}
# Define vector for renaming terms in regression output
#"new_names'" moved to `build/labeling_eas.R'

```

```{r impute_norm_features, warning=FALSE}
#TODO - medium -- try to move all this to build side

#We impute variables where missing and normalizing all variables to be mean-zero and to be on the same scale.

diqr <- function(x) {
  (x - mean(x, na.rm=TRUE))/IQR(x, na.rm=TRUE)
}

gtmed <- function(x) {
  x*(x>med(x))
}

#TODO -- HIGH importance -- why 43 missing values for donation_c?

eas_all_s <- eas_all %>%
  filter(!is.na(don_av2_yr) & year_f %in% last_3_years) %>%
  mutate(
    #(re) code scaling and 2-part splits for the modeling sample (2018-20, reporting donations)
    age_d2sd = arm::rescale(age), #Todo (automate this with `mutate(across)` thing)
    age_if_older = gtmed(age),

    ln_age_if_older = gtmed(ln_age),
    ln_years_involved_post_med = gtmed(ln_years_involved),
    years_involved_d2sd = arm::rescale(year - as.numeric(year_involved)),
    years_inv_d2sd_post_med = gtmed(years_involved_d2sd),
    income_c_imp_diqr = diqr(income_c_imp_bc5k),
    age_d2sd_post_med = arm::rescale(age_if_older),
    income_c_imp_diqr_if_richer = gtmed(income_c_imp_diqr),
    ln_income_c_imp_if_richer= gtmed(ln_income_c_imp_bc5k)
  ) %>%
  rowwise() %>%
  mutate(don_share_inc_imp_bc5k = min(don_share_inc_imp_bc5k, 1)) %>%  #recode about 84 values so the range is between 0-1 for frac. logit to work
  ungroup() %>%
  dplyr::select(all_of(c(num_out, bin_out, controls, key_demog, feat_income_employ, feat_gwwc_etg, robust_controls)),
                income_c, income_c_imp, income_c_imp_bc5k, income_c_imp_diqr, income_c_imp_diqr_if_richer, first_hear_ea_lump, years_involved, age,
                contains("d2sd"), contains("iqr")) %>% #I have added first_hear_ea_lump back even though we don't use it here because we want to use it in ML; I hope it doesn't mess anything up
  # years_involved included to put in sumstats
  labelled::set_variable_labels(.labels = as.list(key_eas_all_labels), .strict=FALSE)


  #Recode missing as 0 for all dummies, as a 'NA category' for categoricals
  #also for normalized variables; i.e., set missings to the mean
eas_all_s_rl <- eas_all_s %>%
    mutate(across(matches("d_|not_just_white"), missing_to_zero))

eas_all_s_rl_imp <- eas_all_s_rl %>%
      mutate(across(matches("d2sd|diqr"), missing_to_zero)) %>%
    labelled::set_variable_labels(.labels = as.list(key_eas_all_labels), .strict=FALSE)
#TODO: (future) -- check for sensitivity to this imputation vs dropping these obs

# Write imp dataset to csv and RDS for ML/predictive modeling in donation_pred.R etc.

saveRDS(eas_all_s_rl_imp, file = here("data","edited_data","eas_all_s_rl_imp.Rdata"))

eas_all_s_rl_imp %>% write_csv(here("data", "edited_data/eas_all_s_rl_imp.csv"))

```

```{r, eval=FALSE}

count_uniq <- eas_all_s_rl %>% #counts of unique values for each feature in each year
  grp_uniq(year_f)

recover_cols <- count_uniq %>%  #any columns *without* unique features in each year?
  select_if(~ any(.==1))%>%
  names()

```

<!--
In the more extensive 'bookdown' version we report the summary
statistics for the data used in the models. See
[HERE](https://rethinkpriorities.github.io/ea_data_public/eas-donations.html#descriptive).
-->

<!-- bookdown_start -->

We report summary statistics on a selection of these features and target outcomes below, limited to the subset who report a zero or positive previous or current-year donation (as in our modeling).

```{=html}
<!--
Todo: medium priority...
- mostly OK, still needs some labeling tweaks etc
-->
```
```{r}

#don_inc_career_tab

(
  don_inc_career_tab <- eas_all_s_rl_imp %>%
            filter(!is.na(don_av2_yr)) %>%
      mutate(`Earn-to-give` = as.factor(d_career_etg),
           `GwwC` = as.factor(d_gwwc_ever_0)) %>%
  ungroup() %>%
  filter(year_f %in% last_3_years) %>%
 dplyr::select(starts_with("don"), starts_with("d_don"), starts_with("inc"), -income_c_imp_diqr,
               d_pt_employment, d_not_employed,
               `Earn-to-give`, `GwwC`) %>%
  dplyr::select(-starts_with("l_"), -d_don_10pct, -ends_with("d2sd"),
                -matches("_if_|_post_")) %>%
    .summ(title = "Donations, income, career, most recent years,  (subset: reporting 0+ donation)",
          digits = c(0,0,1,1,2,1),
          labels=TRUE,
          logical.labels = c("No", "Yes"),
          factor.counts = FALSE,
           out="kable") %>%
  kable_styling()
)

```

Above, we report donations, income, career, and GWWC pledge rates. ... <!-- ... XXX Note
that the 'Don. avg.', the average of the current year and planned
donation, is close, but slightly above the 'Donation' variable, and has
somewhat more positive values. Note also that income imputation (where
missing or stated as below 5000 USD) recodes about 10% of these values,
and leads to a fairly similar average income figure (compare 'Income
(not imp.)' and 'Income (imp. bc.)', where the latter refers to the
income imputation and bottom-coding at 5000 USD). Note that, perhaps
unsurprisingly, the rates of self-reported 'earning-to-give' careers and
'having ever made a Giving What We Can pledge' is higher among those in
this sample (those who report a donation) than in the overall EAS
sample. -->

```{r }

#TODO for future posts/time permitting: split by 'whether reported donation', test for differences

(
  demog_etc_tab <- eas_all_s_rl_imp %>% ungroup() %>%
    filter(year_f %in% last_3_years) %>%
    droplevels() %>%
        filter(!is.na(don_av2_yr)) %>%
 dplyr::select(-contains("don"), -starts_with("d_don"), -starts_with("inc"), -starts_with("ln_"), -d_don_10pct, -ln_income_c_imp_bc5k,  -matches("d2sd|_if_|_post_")) %>%
    select(-d_pt_employment, -d_not_employed, -d_career_etg, -d_gwwc_ever_0, -first_hear_ea_lump, -year_f, -years_involved, -age) %>%
  select(everything()) %>%
     labelled::set_variable_labels(.labels = as.list(key_eas_all_labels), .strict=FALSE) %>%
    sumtable(
    labels = TRUE, #uses assigned in Hmisc or sjlabelled
                simple.kable = TRUE, title = "Demography etc., most recent years (subset: reporting 0+ donation)",
    digits = 1,
    factor.counts = FALSE,
    out="kable"
) %>%
  kable_styling()
)

(
  year_etc_tab <- eas_all_s_rl_imp %>% ungroup() %>%
    filter(year_f %in% last_3_years) %>%
    droplevels() %>%
        filter(!is.na(don_av2_yr)) %>%
 dplyr::select(year_f, years_involved, age) %>%
     labelled::set_variable_labels(.labels = as.list(key_eas_all_labels), .strict=FALSE) %>%
    sumtable(
    labels = TRUE, #uses assigned in Hmisc or sjlabelled
                simple.kable = TRUE, title = "Year, years-involved, age; most recent years (subset: reporting 0+ donation)",
    digits = 1,
    factor.counts = FALSE,
    out="kable") %>%
  kable_styling()
)

```

As the table above suggests, we are modeling the most recent EA survey years ...
only, with roughly ... <!-- equal --> shares of each year <!-- (although somewhat fewer in
each subsequent year). --> ...  The largest (or plurality) demographic groups (as
in the sample overall, see other posts) are <!-- male, non-students, of
'white' ethnicity, and from the US. At least 43% come from one of cities
named in the EA survey (over 50% of those responding), and 13% (where we
have data) have some education at a 'top-6' global university.--> ...

<!-- bookdown_end -->

#### Constructing models {.unnumbered}

**We focus on the following modeling specifications:** ^[We report log-linear models in the robustness appendix.
]

1.  Proportional-effects 'Quasi-Poisson' model for 'amount donated' outcomes (allowing the expected donation to be an exponential function of the features). ^[We focus on the 'average of planned and actual, where present, otherwise the non-missing value'. Although this outcome variable is continuous (and this cannot strictly follow a Poisson distribution, the quasi-poisson model is still able to obtain consistent parameter estimates, provided that the conditional mean is correctly specified [@silvaLOGGRAVITY2006].
]

2.  Fractional logit (@papkePanelDataMethods2008) 'donation as a share
    of income'

3.  Logit regression for the binary 'donated over 1000 USD' outcome

```{r lin-model-inputs}

# Define models (For LINEAR models ... used only in appendix, but some of these are reused in other models)
#-----------------------------------------------------

feat_list = list(
  #better to make this a 'named list'? (TODO -- @oska would that improve the code?)
  c(key_demog, feat_income_employ, controls),
  c(key_demog, feat_income_employ, controls, robust_controls),
 # c(key_demog, feat_income_employ, feat_fh, controls, robust_controls), #robust controls here because 'first heard' is likely entangled with tenure and age
  c(key_demog, feat_income_employ, feat_gwwc_etg, controls) )

feat_list_n = list(
  #better to make this a 'named list'? (TODO -- @oska would that improve the code?)
  c(key_demog_n, feat_income_employ_n, controls_n),
  c(key_demog_n, feat_income_employ_n, controls_n, robust_controls_n),
  c(key_demog_n, feat_income_employ_n, feat_gwwc_etg, controls_n) )

feat_names = c("Baseline", "Robust controls",  "Base + EtG & GWWC")

rhs_vars_list <- rep(feat_list, length(targets_short))
#rhs_vars_list_iqr <- rep(feat_list_iqr, length(targets_short))

outcome_vars_list <- rep(as.list(targets_short), each=length(feat_list))

dfs <- rep(list(eas_all_s_rl_imp), length(outcome_vars_list))

```

```{r model-prep-linear, warning=FALSE}

## Create dataframe for modeling
linear_models <- make_model_df(rhs_vars_list, outcome_vars_list, dfs)

# Fit linear models
linear_models <- linear_models %>%
  mutate(
    lm_fit = fit_models(
      linear_models, "formulas", "dfs", fun = fit_lm)
    )
#warning `using type = "numeric" with a factor response will be ignored‘-’ not meaningful for factor`
# @DR: Why are these models being fit on binary outcomes? DR, @OM: It is fit on all the outcomes including the binary ones, no? However, we haven't reported it yet. Anyways, I think there is still a norm of considering 'linear probability models' in Economics, and arguments on its behalf, at least as a robustness check.

# Extract coefficients, fitted and residuals
model_feat_names <- rep(c(feat_names), times= length(targets_short))
model_oc_names <- rep(c(targets_short_names), each= length(feat_names))
model_names <- paste(model_oc_names, model_feat_names, sep = ": ")

```

```{r model-fit-linear, eval=FALSE, warning=FALSE}

linear_models <- linear_models %>%
mutate(lm_coefficients = map(lm_fit,
                             extract_coefficients,
                             replacement_names = new_names,
                             robust_SE = TRUE),
#TODO -fix -- Medium importance (as linear is just for robustness checks...) error/warning: `'^’ not meaningful for factors`

       lm_resids = map(lm_fit, residuals),
       lm_fitted = map(lm_fit, fitted))

#note: in modeling_df, lm_fit and qp_fit are the 'model output' objects
# `lm_resids` are a list of vectors of residuals from each linear model
# `lm_fitted` are a list of vectors of predicted outcomes from each linear model

# Error: Problem with `mutate()` column `lm_coefficients`.
## ℹ `lm_coefficients = map(...)`.
## x only 0's may be mixed with negative subscripts


<!-- Todo: get Oska's help to run this -->
```

```{r, eval=FALSE}

#trying out some simple models just as a place to test what is going on

test_qp <- eas_all_s_rl_imp %>%
  glm(don_av2_yr ~ ln_income_c_imp_bc5k  + ln_age + not_male_cat + student_cat + race_cat + where_live_cat + city_cat + d_pt_employment +    d_not_employed + d_top6_uni + ln_years_involved + year_f, family=quasipoisson, data =.)



test_fl <- eas_all_s_rl_imp %>%  glm(don_share_inc_imp ~ ln_income_c_imp_bc5k + ln_age + not_male_cat + student_cat + race_cat +
    where_live_cat + city_cat  + d_pt_employment +
    d_not_employed + d_top6_uni + ln_years_involved + year_f, family = quasibinomial('logit'), data = .)


test_logit <- eas_all_s_rl_imp %>%  glm(d_don_1k ~ age_d2sd + not_male_cat + student_cat + race_cat +
    where_live_cat + city_cat + income_c_imp_diqr + d_pt_employment +
    d_not_employed + d_top6_uni + years_involved_d2sd + year_f,
    family = binomial, data = .)

```

```{r qp-models}

qp_targets_short <- c("don_av2_yr")
qp_outcome_vars <- rep(as.list(qp_targets_short), each=length(feat_list)) # List of outcome variables for quasi-poisson
qp_rhs_vars <- rep(feat_list, length(qp_targets_short)) # List of independent variables
qp_dfs <- rep(list(eas_all_s_rl_imp), length(qp_outcome_vars)) # List of dataframes to fit models to

qp_models <- make_model_df(qp_rhs_vars, qp_outcome_vars, qp_dfs) # Create dataframe for models

# Add model names
feat_group_names <- c("1. Baseline", "2. Robust controls",
                    "3. Base + ETG + GWWC")

qp_model_names <- feat_group_names

qp_models <- qp_models %>%
  mutate(model_name = rep(qp_model_names, length(qp_targets_short)))

# Fit quasi-poisson models
qp_models <- qp_models %>%
  mutate(
    qp_fit = fit_models(
      qp_models, "formulas", "dfs", fun = fit_glm)
    )

# Extract coefficients
## Takes a little while, consider parallels package?
qp_models_noexp <- qp_models %>%
  mutate(qp_coefficients = map(qp_fit,
                               extract_coefficients,
                               replacement_names = new_names,
                               exponentiate = FALSE,
                               robust_SE = TRUE),
         qp_resids = map(qp_fit, residuals),
         qp_fitted = map(qp_fit, fitted))

qp_models <- qp_models %>%
  mutate(qp_coefficients = map(qp_fit,
                               extract_coefficients,
                               replacement_names = new_names,
                               exponentiate = TRUE,
                               robust_SE = TRUE),
         qp_resids = map(qp_fit, residuals),
         qp_fitted = map(qp_fit, fitted))

```

```{r frac-reponse-work}

#Note: redone/redoing  - fractional logit instead of Quasi-poisson with offset
#Discussion:

fl_targets_short <- c("don_share_inc_imp_bc5k")
fl_outcome_vars <- rep(as.list(fl_targets_short), each=length(feat_list)) # List of outcome variables for quasi-poisson
fl_rhs_vars <- rep(feat_list, length(fl_targets_short)) # List of independent variables
fl_dfs <- rep(list(eas_all_s_rl_imp), length(fl_outcome_vars)) # List of dataframes to fit models to

#----------

# Function to remove a particular string from a list
#remove_str_list (moved to rstuff functions)

# Create dataframe for models
fl_models <- make_model_df(fl_rhs_vars, fl_outcome_vars, fl_dfs)

fl_models <- fl_models %>% mutate(model_name = rep(feat_group_names, length(fl_targets_short)))

```

```{r fit-extract-fl}

# Fit fractional logit models

fl_models <- fl_models %>%
  mutate(
    fl_fit = fit_models(
      fl_models, "formulas", "dfs", fun = fit_glm,
          family = quasibinomial('logit'))
    )

# Extract coefficients
## Takes a little while, consider parallels package?
fl_models_noexp <- fl_models %>%
  mutate(fl_coefficients = map(fl_fit,
                               extract_coefficients,
                               replacement_names = new_names,
                               exponentiate = FALSE,
                               robust_SE = TRUE),
         fl_resids = map(fl_fit, residuals),
         fl_fitted = map(fl_fit, fitted))

fl_models_noexp_nonrobust <- fl_models %>%
  mutate(fl_coefficients = map(fl_fit,
                               extract_coefficients,
                               replacement_names = new_names,
                               exponentiate = FALSE,
                               robust_SE = FALSE),
         fl_resids = map(fl_fit, residuals),
         fl_fitted = map(fl_fit, fitted))

fl_models <- fl_models %>%
  mutate(fl_coefficients = map(fl_fit,
                               extract_coefficients,
                               replacement_names = new_names,
                               exponentiate = TRUE,
                               robust_SE = TRUE),
         fl_resids = map(fl_fit, residuals),
         fl_fitted = map(fl_fit, fitted))
```

```{r Logit-models, echo=FALSE}

logit_targets_short <- c("d_don_1k")
logit_outcome_vars <- rep(as.list(logit_targets_short), each = length(feat_list))
logit_rhs_vars <- rep(feat_list_n, length(logit_targets_short))
logit_dfs <- rep(list(eas_all_s_rl_imp), length(logit_outcome_vars)) # List of dataframes to fit models to

logit_models <- make_model_df(logit_rhs_vars, logit_outcome_vars, logit_dfs) # Create dataframe for models

# Add model names
logit_model_names <- feat_group_names

logit_models <- logit_models %>% mutate(model_name = rep(logit_model_names, length(logit_targets_short)))

logit_models <- logit_models %>%
  mutate(logit_fit = fit_models(
    logit_models, "formulas", "dfs", fun = fit_glm,
    family = binomial
  ))

logit_models <- logit_models %>%
  mutate(logit_coefficients = map(logit_fit,
                               extract_coefficients,
                               replacement_names = new_names,
                               exponentiate = TRUE,
                               robust_SE = TRUE))
```

```{=html}
<!--
# TODO --------------------------------------------------------------------
# Consider imputation (especially for income variable) #DR: the ad-hoc imputation we've used so far is probably good enough for now
## Should this be done before normalizing? #DR: it shouldn't matter, but yeah, it seems like better practice to impute before normalizing
-->
```

#### Models (tables and plots of results; latest years combined) {.unnumbered}

We put together forest plots of (normalized) coefficients from the distinct set of models outlined above, where these can be compared on the same scales. Specifically, we consider,

-   for each of the three key outcomes ('amount donated (averaged)',
    'donation as a share of income', 'donated over 1000 USD'),
-   models with three specific sets of features, yielding nine models in total (plus robustness checks in the appendix).

The feature sets, which we will refer to in the forest plots below, are:

**1. "Base"** (baseline model)

Demographics:^["log" notes
that the natural log was taken (allowing a proportional 'elasticity'
relationship).]
`r eas_all_s %>% select(all_of(key_demog)) %>% lab_list_to_text`

Career-related:
`r eas_all_s %>% select(all_of(feat_income_employ)) %>% lab_list_to_text`

Controls:
`r eas_all_s %>% select(all_of(controls)) %>% lab_list_to_text`

<!-- "imp" denotes that income is imputed where missing"-->

*Logit model: standardizations:*

::: {.foldable}
For the Logit models we use standardizations instead of logged continuous variables. We divide age and tenure by two sample standard deviations for each, following @gelmanScalingRegressionInputs2008. This allows the coefficients of continuous features like income to be compared to those for binary features like "whether employed". We divide income by the sample inter-quartile range (`r op(IQR(eas_all_s$income_c_imp))` USD). As the distribution appears highly skewed, normalizing income by 2sd would yield extremely large and hard-to-interpret coefficients.
:::


**2. "Robust controls":** Including all of the features in *Base* as well as a second term for each of "`r eas_all_s %>% select(all_of(robust_controls)) %>% lab_list_to_text`" that takes a positive value only where these exceed their respective sample medians, and is otherwise set to zero. These represent 'adjustment terms' allowing us to see whether and how time-in-EA, age, and income may have a different relationship with donations at higher values of each of these.^[This simple 'two part' specification is partially related to the DataColada discussion [here](http://datacolada.org/62).
]

```{=html}
<!--
**3. "Base + Robust + First heard":** Including all of the features in *Robust controls* as well as binary variables for "where first heard of EA".^[Here we use the robust controls to 'robustly control' for nonlinear background effects that might be entangled with 'where first heard of EA'.
]
-->
```

**3. "Base + ETG + GWWC":** Including all of the features in *Base* as
well as the binary variables
"`r eas_all %>% select(all_of(feat_gwwc_etg)) %>% lab_list_to_text`",
i.e., whether reported ever having taken the Giving What We Can Pledge,
and whether they report their career as 'earning-to-give'.

\

Note that we report models with *each of the three feature sets* in each of the forest plots below. However, each forest plot reports on a single outcome and a single 'theme', e.g., focusing on reporting just the coefficients on demographics from across *each of the above three model feature sets* (with some repeated coefficients across plots).

**These themes are**

-   Demographics (including age and time-in-EA)\*
-   Employment/career, GWWC, EtG, Income\*\*
-   "Non-response" to particular questions (in the appendix)\*\*\*

However, it is important to remember that the reported estimates in each forest plot come from models that 'control for' other features (as reported).

Note that we exclude the 'two-part' coefficients (in the 'Robust controls' models) from the forest plots.^[In the 'Robust controls' model there are two coefficients for each of age, income and 'time in EA' for below-median values, and a second one representing the adjustment to this coefficient for above-median values of each of these. We leave these coefficients out of the forest plots to avoid confusion, reporting them in tables under "[Age, time-in-EA, Income, year; possible nonlinearity](#nonlin)" instead.
]

We present the coefficients on 'Age, Time in EA, Income, and nonlinear
adjustments for each of these' [in a separate set of tables](#nonlin)
([web
link](https://forum.effectivealtruism.org/posts/nb6tQ5MRRpXydJQFq/ea-survey-2020-series-donation-data#Age__time_in_EA__Income__Year__possible_nonlinearity)).^[We use the natural log of income in our models of donation amounts and 'donation/income' outcome. Here the (untransformed) coefficients have an elasticity interpretation --- percent increase in donations (or donation share) for a given percentage increase in income. As this scale is not comparable to our other coefficients, we do not report it in these forest plots. We report this in tables under "[Age, time-in-EA, Income; possible nonlinearity](#nonlin)" instead
.], ^[To avoid clutter we present the results for nonresponse feature set in an
[appendix tableonly](https://rethinkpriorities.github.io/ea_data_public/eas-donations.html#nonresponse).]

```{r group-vars}
# Create variable groupings for displaying coefficients
## Can't think of a tidier way to do this...
### Check that this extracts all necessary

demog_coefs <- c("Age", "Where live:", "Student", "city", "Gender:", "white", "race")
emp_gw_etg_coefs <- c("top-6 uni", "employ", "etg", "gwwc")
#fh_coefs <- c("hear")
nr_coefs <- c("response", "NA")

nonlin_coefs <- c("income", "age", "year") #coefficients of interest not (always) reported elsewhere, allowing us to consider nonlinearity

# Filter the coefficients returned from using broom::tidy
## Keep only those specified in character vector keep
extract_coefs <- function(df, keep, term_col = term,
                          ignore.case = TRUE,
                          exclude = NULL){

  # Add assertion statements in/doc string

  if (ignore.case == TRUE){
    keep <- tolower(keep)
  }

  keep <- paste(keep, collapse="|")

  coef_df <- df %>% filter(str_detect(tolower({{term_col}}), keep))

  if (!is.null(exclude)){
  exclude <- paste(exclude, collapse="|")
    coef_df <- coef_df %>% filter(!str_detect(tolower({{term_col}}), exclude))
  }

  return(coef_df)
}
```

```{r collect-feature-set--coefs}
# Extract coefficients for each feature set, for making forest plots (and tables)

# forms <- list("qp", "fl", "logit")
#--> pmap (todo @oska)
qp_coefs <- qp_models %>%
  select(qp_coefficients, model_name, outcome) %>%
  tidyr::unnest(., cols = c(qp_coefficients))

qp_coefs_noexp <- qp_models_noexp %>%
  select(qp_coefficients, model_name, outcome) %>%
  tidyr::unnest(., cols = c(qp_coefficients))

fl_coefs <- fl_models %>%
  select(fl_coefficients, model_name, outcome) %>%
  tidyr::unnest(., cols = c(fl_coefficients))

fl_coefs_noexp <- fl_models_noexp %>%
  select(fl_coefficients, model_name, outcome) %>%
  tidyr::unnest(., cols = c(fl_coefficients))

fl_coefs_noexp_nonrobust <- fl_models_noexp_nonrobust %>%   select(fl_coefficients, model_name, outcome) %>%
  tidyr::unnest(., cols = c(fl_coefficients))

logit_coefs <- logit_models %>%
  select(logit_coefficients, model_name, outcome) %>%
  tidyr::unnest(., cols = c(logit_coefficients))

# feature_sets <- list("demog", "inc_emp_gw_etg", "fh", "nr")
#--> pmap2 feature_sets, forms (todo @oska) .. or maybe map across forms but not feature sets here, as we need bespoke exclusions

exclude_demog_coefs <- c("response","older", "post_med", "ln_" )

exclude_inc_coefs <- c("response", "if above", "older", "na")

#exclude_fh_coefs <- c("response", "if above", "older") #will prob need to add coefs for all the small fh categories if we put these all in

demog_coefs_qp <- extract_coefs(qp_coefs, demog_coefs, exclude = c(exclude_demog_coefs))
emp_gw_etg_coefs_qp <- extract_coefs(qp_coefs, emp_gw_etg_coefs, exclude = exclude_inc_coefs)
nr_coefs_qp <- extract_coefs(qp_coefs, nr_coefs)
nonlin_coefs_qp <- extract_coefs(qp_coefs, nonlin_coefs)
nonlin_coefs_qp_noexp <- extract_coefs(qp_coefs_noexp, nonlin_coefs)

nonlin_coefs_qp_combo  <- bind_rows(nonlin_coefs_qp_noexp %>% filter(str_detect(term, "Year of")==FALSE), nonlin_coefs_qp %>% filter(str_detect(term, "Year of")==TRUE))

demog_coefs_fl  <- extract_coefs(fl_coefs, demog_coefs,  exclude = exclude_demog_coefs)

emp_gw_etg_coefs_fl <- extract_coefs(fl_coefs, emp_gw_etg_coefs, exclude = exclude_inc_coefs)

#TODO: doublecheck the income coefficients:

nr_coefs_fl <- extract_coefs(fl_coefs, nr_coefs)
nonlin_coefs_fl <- extract_coefs(fl_coefs, nonlin_coefs)
nonlin_coefs_fl_noexp <- extract_coefs(fl_coefs_noexp, nonlin_coefs)
nonlin_coefs_fl_noexp_nonrobust <- extract_coefs(fl_coefs_noexp_nonrobust, nonlin_coefs)

nonlin_coefs_fl_combo  <- bind_rows(nonlin_coefs_fl_noexp %>% filter(str_detect(term, "Year of")==FALSE), nonlin_coefs_fl %>% filter(str_detect(term, "Year of")==TRUE))


demog_coefs_logit <- extract_coefs(logit_coefs, demog_coefs, exclude = exclude_demog_coefs)
emp_gw_etg_coefs_logit <- extract_coefs(logit_coefs, c("income", emp_gw_etg_coefs), exclude = exclude_inc_coefs)
nr_coefs_logit <- extract_coefs(logit_coefs, nr_coefs)
nonlin_coefs_logit <- extract_coefs(logit_coefs, nonlin_coefs)

```

```{r forest-plots-all}

group_fp_do <- function(df, groups=model_name, xlims=c(NA,NA), vl=1){
  df %>%
    grouped_forest_plot(., groups = {{groups}}, vline = {{vl}}) +
    coord_cartesian(xlim = {{xlims}}) +
    scale_colour_discrete(name = "",
                        labels = function(x) str_wrap(x, width = 15)) +
  theme_bw() +
  theme(legend.position = "bottom") +
  guides(fill=guide_legend(nrow=2,byrow=TRUE))
}
#TODO: Add to caption at bottom, automate, specify the exact models (or at least explain the control variables)
# Legend on top or bottom?
# Todo -- gray line in background
# add more grid lines


fp_qp_subtitle <- "Quasi-poisson: relative rates, 95% CIs (Robust SEs); colors = models"

fp_logit_subtitle <- "Logit model, proportional effects, 95% CIs; colors = models"

fp_fl_subtitle <- "Frac. Logit model, prop. effects, 95% CIs; colors = models"

fp_caption = str_wrap("Results come from three distinct models, each with different sets of features. These models and their features are fully described in the main text. All confidence intervals are heteroskedasticity-robust (White, 1980).", 120)

#todo (@oska) purr::map these too
fp_demog_coefs_qp <- demog_coefs_qp %>%
  group_fp_do(vl=1) +
  labs(title= "'Donation amount', demog. coefficients",
       subtitle = fp_logit_subtitle,
       caption = fp_caption ) +
  theme(plot.caption = element_text(size = 8, hjust = 0))

fp_demog_coefs_fl <- demog_coefs_fl %>%
  group_fp_do(vl=1) +
  labs(title= "'Donation/income', demog. coefficients, 2018-20",  subtitle = fp_fl_subtitle,   caption = fp_caption) #TODO -- get me my ticks for 0.25, 0.5 etc

fp_demog_coefs_logit <- demog_coefs_logit  %>%
  group_fp_do(vl=1) +
  labs(title= "'Donated $1000+', demog. coef., 2018-20",  subtitle = fp_logit_subtitle,   caption = fp_caption)


#todo (@oska) purr::map these too
fp_emp_gw_etg_coefs_qp <- emp_gw_etg_coefs_qp %>%
  filter(!(str_detect(model_name, "Robust") & str_detect(term, "Income"))) %>%  #Don't show income in models with nonlinear income terms
  group_fp_do(vl=1) +
  labs(title= "'Donation amount', career-related coefs, 2018-20",  subtitle = fp_qp_subtitle,
         caption = fp_caption)

#TODO -- NA STILL HERE!


fp_emp_gw_etg_coefs_fl <- emp_gw_etg_coefs_fl %>%
  filter(!(str_detect(model_name, "Robust") & str_detect(term, "Income"))) %>%
  group_fp_do(vl=1) +
  labs(title= "'Donation as share of income', proportional model, career-related coefs",  subtitle = fp_qp_subtitle,   caption = fp_caption)

fp_emp_gw_etg_coefs_logit <- emp_gw_etg_coefs_logit %>%
  filter(!(str_detect(model_name, "Robust") & str_detect(term, "Income"))) %>%
  filter(!(str_detect(term, "NA"))) %>%
  group_fp_do(vl=1, xlims=c(0,NA)) +
  labs(title= "'Donated $1000+', Logit model, career-related coefs",  subtitle = fp_logit_subtitle,
         caption = c(fp_caption)
       )

###
##TODO -- get NA out of here!

```

#### Model theme: Demographics {#demog_mod .unnumbered}

Below, we plot the estimates and
([heteroskedasticity-robust](https://en.wikipedia.org/wiki/Heteroscedasticity-consistent_standard_errors))
95% confidence intervals for key demographic coefficients for each of
the three models.^[Again, we want to emphasize that each these are from
models which also 'control' for a wide set of characteristics (e.g.,
income, age, and time-in-EA), as catalogued above.]

We first present the results from our Quasi-Poisson model of donation amount (expressed as the average of current and planned donation in a year, or whichever is noted). This model allows effects to be "proportional", as described and interpreted below.^[The exponentiated coefficients presented here can be interpreted as 'in considering average donations for each group, by what amount should we multiply this if we look at an individual with the variable 'switched on' versus switched off', all else equal'? For continuous variables, it expresses 'how much do we multiply this if we increase our the variable by one (normalized) measure?]

```{r fp_demog_coefs_qp}
fp_demog_coefs_qp
```


\

*Again with ggplotly: is it better easier to read this way?* 
```{r fp_demog_coefs_qp}
fp_demog_coefs_qp %>% ggplotly
```


In the figure above, the vertical bar at "1" represents the coefficient of "no difference in donation between these groups, all else equal". ... (possible brief discussion of key results here, or only in Forum post). <!--  ... As all of the 95% confidence intervals cross this line (for City, just barely), we cannot rule out 'no difference' by conventional frequentist null-hypothesis testing.

Still, the evidence suggests that (all else equal), individuals from big cities named in the EA survey donate substantially more.^[We did not run a Bayesian analysis here. However, in our experience such approaches, under a flat prior belief, put most of the posterior probability mass within similar ranges as do the frequentist confidence intervals. This would suggest we could be fairly confident that (e.g.) those from big EA cities tend to donate more, all else equal. However, further, explicit Bayesian analysis might be merited.
]

Point estimates imply they donate about... more/less <!-- XXX 18-19% more on average (considering our baseline model and our model with robust controls), or about 25% more on average, when we also adjust for earning-to-give and GWWC status.^[However, GWWC and EtG can be seen as very closely related to the outcome we are looking for; thus, even in a descriptive sense, this coefficient is hard to interpret in model 3.
] -->

 <!-- seems to have very little relationship to donation here, or at least we have little evidence to suggest a relationship. The coefficients on "Not just white" are close to zero, with wide 95% confidence intervals. -->

 <!--  weakly suggest that women and nonbinary people donate somewhat less, all else equal. Students seem to donate substantially less, perhaps only 70-80% as much as nonstudents. Still, in a conventional sense (95% confidence interval), we cannot rule out that any of these differences observed in our sample are due to chance, and that in fact, the differences in the relevant *population* are zero, or in the opposite direction. (Also, see caveats in other post about potential non-representativeness, recall bias and measurement error, etc.) -->

```{r fp_demog_coefs_fl}
fp_demog_coefs_fl
```

Above we plot the estimates, for the same features, for our (Fractional Logit) models of the 'share of income donated'.^[Where income is imputed where 0 or below 5000, as explained above. Here, we topcode the (relatively few) responses that report donations in excess of income to have a value of 1 (donated 100% of income.) As in all of these plots, these are from models containing the three broad sets of features outlined above, including (the log of) reported income; thus we do not impose an exact proportional relationship between income and donations.
]

... <!-- We find generally similar results as in the models of donation amounts,
with perhaps a slightly more important role for gender (non-males
donating a lower share all else equal), and a slightly less important
role for student status. However, none of these coefficients are
statistically significant in a conventional sense. -->

```{r}
rpct <- function(x){ (round(x, 3) -1)*100}
```

<!-- XXX ... refocus key results, or only in forum --> ...

EAs living in named cities donate ... <!-- a substantially higher share of their income, all else equal --> a roughly `r op(rpct(demog_coefs_fl$estimate[demog_coefs_fl$term=="City: Named big city" & demog_coefs_fl$model_name=="1. Baseline" ]))`% greater share in our baseline model.^[Note -- this *doesn't* mean they donate 20 more percentage points of their income; the share of the income they donate, on average, is 20% larger. E.g., if a group of non-big-EA-city-people donated 10% of their income on average, we might expect a comparable big-EA-city group of people to donate 12% of their income on average.
]

<!-- Again, we have little evidence that ethnicity is related to this
outcome.  --> Non-males seem to be donating a ... <!-- somewhat lower share of income, -->
`r op(demog_coefs_fl$estimate[demog_coefs_fl$term=="Gender: Not-male" &  demog_coefs_fl$model_name=="1. Baseline" ]*100)`%
as high a share in our baseline model. Students also seem to donate ...
<!-- lower shares, -->
`r op(demog_coefs_fl$estimate[demog_coefs_fl$term=="Student" &  demog_coefs_fl$model_name=="1. Baseline" ]*100)`%
as high in our baseline model.

```{r fp_demog_coefs_logit}
fp_demog_coefs_logit
```

The final outcome variable is 'whether an individual donated \$1000 or more'. ^[Here we also present the normalized age coefficient. Recall, for the models of *other outcomes* we present age in a later table, in elasticity terms)
]

Age has a ... <!-- strong relationship with this outcome:--> a two standard
deviation increase in age (about `r round(sdev(eas_all_s$age)*2,1)`
years older) is associated with a near-doubling of the probability of
donating 1000 USD or more in our model.
(`r  round((demog_coefs_logit$estimate[demog_coefs_logit$term=="Age (2sd norm.)" &  demog_coefs_logit$model_name=="1. Baseline" ]), 2)`
times as high in our baseline model.)

*Again, in our baseline model...*

-   EAs living in named big cities are ... roughly
    `r rpct(demog_coefs_logit$estimate[demog_coefs_logit$term=="City: Named big city" &  demog_coefs_logit$model_name=="1. Baseline" ])`%
    more likely,

-   Non-males are about
    `r round((demog_coefs_logit$estimate[demog_coefs_logit$term=="Gender: Not-male" &  demog_coefs_logit$model_name=="1. Baseline" ])*100,1)`%
    as likely as males,

-   'Not-just-white' people about
    `r round((demog_coefs_logit$estimate[demog_coefs_logit$term=="Not just white" &  demog_coefs_logit$model_name=="1. Baseline" ])*100,1)`%
    as likely as white people,

-   and students
    `r round((demog_coefs_logit$estimate[demog_coefs_logit$term=="Student" &  demog_coefs_logit$model_name=="1. Baseline" ])*100,1)`%
    as likely as nonstudents...

...to donate at least 1000 dollars, all else equal.

The graph suggests that each of these results are ... <!--  similar across the
three models. Most of these coefficients are statistically significant
in a conventional sense. -->

#### Model theme: Employment/career, GwwC, EtG, (income) {#mod_emp .unnumbered}

Next we consider employment and career-related features.^[We consider the relationship between income and donations and donations as a share of income [in the next](#nonlin) section. For these models we express income in natural logs, so the interpretation of the 'raw coefficients' are approximately *income elasticities*; these are not on the same scale as the standardized coefficients below.
]

```{r}
fp_emp_gw_etg_coefs_qp
```

... GWWC pledge

<!--
Unsurprisingly, those who report ever having taken the GWWC pledge, and
those who report being in an Earning-to-Give career also report donating
substantially more, all else equal. However, the magnitudes are
strikingly large: they donate nearly twice as much, and over 50% more,
respectively.
-->


Those who attended global top-6 universities (defined above)  <!-- donate
substantially more,-->  donate about
`r rpct((emp_gw_etg_coefs_qp$estimate[emp_gw_etg_coefs_qp$term=="Top-6 Uni: Yes" &  emp_gw_etg_coefs_qp$model_name=="1. Baseline" ]))`%
more in our baseline model point estimate.

... <!--Those part-time-employed seem to donate somewhat more, all else equal,
although confidence intervals are very wide. Those who are not employed
tend to donate less.-->

```{r}
fp_emp_gw_etg_coefs_fl
```

... Discussions: patterns for shares of income, key relationships...

<!--
Similarly to above, those who report ever having taken the GWWC pledge,
and those who report being in an Earning-to-Give career tend to donate a
much larger *share of income* than the rest, all else equal.
Specifically, over *twice as large a share* for GWWC, and over 50% more
for EtG.

We see similar patterns as above for top-6 university and for those who
are not employed. However the results for part-time employer contrast to
the previous plot. While the part-time employed tend to donate somewhat
more overall, they tend to donate a lower *share of their income* than
the base group (the full-time employed), all else equal. (Again, recall
that this already controls for income and other factors.)
-->

```{r}
fp_emp_gw_etg_coefs_logit
```

<!-- We see similar --> ... patterns for our logit models of "whether donated at
least 1000 USD" ... (compare across models)

<!-- with some notable exceptions. Similarly, GWWC pledgers
are *much more likely* to donate at least this amount. EtG people show a
less dramatic difference, only being about
`r rpct((emp_gw_etg_coefs_logit$estimate[emp_gw_etg_coefs_logit$term=="EtG career" &  emp_gw_etg_coefs_logit$model_name=="3. Base + ETG + GWWC" ]))`%
more likely than the base group, all else equal.
-->

Here (for models without the two-part controls) we also present the
relationship to income. <!-- ...Unsurprisingly, income has a strong positive
association with the probability of donating 1k or more: --> An income that
is `r eas_all_s$income_c_imp %>% IQR(na.rm=TRUE) %>% round(0)` USD greater (the
25-75 'interquartile range') is associated with a
`r op((emp_gw_etg_coefs_logit$estimate[emp_gw_etg_coefs_logit$term=="Income (imp., IQR norm)" & emp_gw_etg_coefs_logit$model_name=="1. Baseline"]))`
times greater probability donating 1k or more.

... (other standouts)

<!--
Those from top-6 universities and the part-time employed appear slightly
(but not 'significantly') more likely to donate 1k or more, all else
equal. The not-employed are substantially *less* likely to donate 1k or
more.
-->

#### Age, time-in-EA, Income, Year; possible nonlinearity {#nonlin .unnumbered}

<!-- Todo: make table here including age, time-in-ea, income and 'nonlinear' coefficients where present, for all three models. -->

```{r nonlin-tables}

#Making table of *raw coefficients* for logged income variables here, as these have elasticity interpretations

nonlin_tables <- bind_rows(nonlin_coefs_qp_combo, nonlin_coefs_fl_combo, nonlin_coefs_logit) %>%
    select(outcome, term, model_name, estimate, std.error, p.value, conf.low, conf.high) %>%
  mutate(
  term = str_replace_all(term, c("ln_" = "log ", "EApost_med"  = "EA (post median)")),
  term =  str_replace_all(term, key_eas_all_labels),
  term =  str_replace_all(term, c("Involved" = "in")),
  term =  str_replace_all(term, c("_bc5k" = "")),
  term =  str_replace_all(term, c("imp." = "imp., bc")),
  term =  str_replace_all(term, c("years_inv_d2spost_med" = "Years in EA (2sd norm) post-median"))
  ) %>%
    filter(!str_detect(model_name, "GWWC")) %>%
  mutate(
    outcome = str_replace_all(outcome,            key_eas_all_labels
    )
    ) %>%
    arrange(term) %>%
    group_by(outcome) %>%
    group_split


nonlin_don_avg_tab <- nonlin_tables[[1]] %>% select(-outcome) %>%
  .kable(caption = nonlin_tables[[1]][[1,"outcome"]],
          col.names = NA, digits=3) %>%
    .kable_styling("striped")


nonlin_don_gt1k_tab <- nonlin_tables[[2]] %>% select(-outcome) %>%
  .kable(caption = nonlin_tables[[2]][[1,"outcome"]],
          col.names = NA, digits=3) %>%
    .kable_styling("striped")

nonlin_donshare_tab <- nonlin_tables[[3]] %>% select(-outcome) %>%
  .kable(caption = nonlin_tables[[3]][[1,"outcome"]],
          col.names = NA, digits=3) %>%
    .kable_styling("striped")

```

*Discussion of modeling continuously-valued features:*

::: {.foldable}
We next present the estimates, from six of the nine models (excluding
models involving GWWC and earning-to-give), for the continuous-valued
features (age, income, and years-in-EA) as well for the survey-year
categorical feature. We present these for both the baseline and the
'Robust control' models; the latter allow us to consider distinct
patterns where values are above-median. We present these in tables
rather than forest plots, as the interpretation is subtle.
:::


```{r nonlin_don_avg_tab}

nonlin_don_avg_tab

```

In the Baseline model...  we see that, unsurprisingly, donations clearly and strongly increase in income, all else equal; <!--...  XXX I don't expect this to change though -->in the parlance of Economics, donation seems to be a 'Normal good.' The estimated income elasticity is `r op(nonlin_coefs_qp_combo$estimate[nonlin_coefs_qp_combo$term=="Log income (imp.)_bc5k" & nonlin_coefs_qp_combo$model_name =="1. Baseline"])`, suggesting that, all else equal, on average, as income increases in our data by some share 'X', donations increase by slightly ... <!-- less-->  than this share. <!-- I.e. donations increase slightly less than proportionally with income, suggesting that donation are not a 'luxury good' in the strict Econ-1 sense. However, the upper confidence interval is still somewhat above 1 (and the usual caveats about sample selection, unobservable factors, and causality apply).-->

In model 2 we include an adjustment coefficient to allow nonlinearity, allowing the elasticity of donations in income to be distinct for income levels above the median. ... <!-- XXX ... This coefficient is positive (very loosely suggesting that, for above-average levels of income, greater income leads to proportionally greater donations) but it is very small, and we cannot rule out zero difference. Arguably, the fact that even the upper confidence interval for this adjustment is rather small speaks against a very strong nonlinearity, and in favor of a 'proportional donations' model as a starting point. -->

Age: <!-- XXX ...  has a ...positive relationship to donation; --> the baseline coefficient suggests that as age doubles, contributions increase by `r op(nonlin_coefs_qp_combo$estimate[nonlin_coefs_qp_combo$term=="ln_age" & nonlin_coefs_qp_combo$model_name =="1. Baseline"]*100)`% on average, all else equal.
...<!-- Model 2 suggests that this 'age relationship' is approximately proportional in this way, with only a small (and statistically insignificant) positive adjustment for ages above the median (age `r med(eas_all_s$age)`). -->

Time-in-EA is strongly related to donations, <!-- XXX doublecheck...--> even 'controlling for age, etc.' (and vice versa). A doubling of years in EA is associated with a `r op(nonlin_coefs_qp_combo$estimate[nonlin_coefs_qp_combo$term=="ln_Years in EA" & nonlin_coefs_qp_combo$model_name =="1. Baseline"]*100)`% greater donation. The nonlinear adjustment term ... <!-- is again fairly small and statistically insignificant.-->

Each of the 'survey year difference' estimates (for `r year_n-1` and `r year_n`, with 2018 as the base year) are (... update/doublecheck) fairly close to 1 (representing 'no difference'). However, 95% confidence intervals are rather  ... wide, suggesting a lack of statistical power to discern a difference. ... <!-- XXX ^[This suggests that we may have little power to discern whether (all else equal) there are strong differences between donations among respondents to each survey, all else equal. Still, our best guess might be something close to a zero difference, and we might reasonably put substantial probability that any differences are not extremely large. (Strictly speaking, we would need to estimate Bayesian credible intervals to make statements like this; we use the standard frequentist confidence intervals somewhat imprecisely.)
] -->

We next present the corresponding coefficients for our fractional logit models of 'donations as a share of income'.

```{r}
nonlin_donshare_tab
```

The baseline models suggest that donation as a share of income is roughly (... constant, or slightly increasing in income. For every doubling of income, the *share of income* donated is seen to increase by `r op(nonlin_coefs_fl_combo$estimate[nonlin_coefs_qp_combo$term=="Log income (imp.)_bc5k" & nonlin_coefs_qp_combo$model_name =="1. Baseline"]*100)`%. ... <!-- XXX ... While the 95% confidence intervals include small decreases or increases in this share as income doubles (from about -5% to +13%), the evidence suggests approximate proportionality. Model 2 suggests this pattern continues at larger incomes, with little apparent nonlinearity above median income (and fairly tight bounds on this). -->

Discussion (or forum only): age vs donation share...
<!-- Age is very strongly positively associated with 'donation as a share of income', with a doubling of age approximately relating to a `r op(nonlin_coefs_fl_combo$estimate[nonlin_coefs_qp_combo$term=="ln_age" & nonlin_coefs_qp_combo$model_name =="1. Baseline"]*100)`% increase in share of income donated (i.e., more than a doubling of this share, on average, all other observables held constant).^[We want to emphasize each of these coefficients 'hold other observable things constant'. E.g., age has the mentioned very strong relationship to share of income donated---even holding time-in-EA constant and vice versa.
]
-->

<!-- ... report Years-in-EA also shows a very strong association to share of income donated, with a doubling of this 'tenure' approximately relating to a `r op(nonlin_coefs_fl_combo$estimate[nonlin_coefs_qp_combo$term=="ln_Years in EA" & nonlin_coefs_qp_combo$model_name =="1. Baseline"]*100)`% increase in share of income donated.

The adjustment coefficients for each of these are small and fairly-tightly bounded, suggesting perhaps only small differences in the above relationships for values of age and tenure above the medians.

In these models we *do* find some statistically significant associations between survey year and share-of-income donated, all else equal, with `r year_n-1` and 2020 generally having lower values than 2018. However, we do not want to read too much into this, given possible differences in survey response composition, as discussed in other posts.

-->

Finally, we consider the models of 'donated 1k or more':

```{r}
nonlin_don_gt1k_tab

```

Here we present exponentiated coefficients, representing relative proportional rates of this outcome for the distinct groups.

Age is ... <!-- strongly positively associated with this outcome (as for the other donation outcomes), with a 2 sd difference in age (about `r op(sdev(eas_all$age)*2)`) years associated with a near doubling of the probability of making a 1k donation. Unsurprisingly, income is also strongly associated with donating 1k or more. -->

Year coefficients <!-- The coefficients for the `r year_n-1` and (especially) `r year_n` EA survey year
dummies are substantially below 1, suggesting donating 1k or more is
becoming less prevalent among otherwise-similar individuals. --> ^[However, as
noted above, we are cautious about these 'year coefficients' because of
potential changes in EA survey promotion and response that may not
reflect actual changes in the EA population.]

The 'years in EA' coefficients are  (...) extremely strong. These suggest
that a 2 sd increase in tenure in EA
(`r op(sdev(eas_all_s$years_involved)*2)` years) is associated with a
`r op(nonlin_coefs_logit$estimate[nonlin_coefs_logit$term=="Years in EA (2 sd norm.)" & nonlin_coefs_logit$model_name =="1. Baseline"])`
times greater relative chance of donating 1k or more relative to the
base group), all else equal.

### Predictive models {#predictive-models}

We use elastic-net and random-forest modeling approaches with validation (these are standard in the modern 'machine learning' tooklit), to derive a model that 'predicts well'.\*

\

*Discussion: why build 'predictive models of donation'? :*

::: {.foldable}

**TODO: move (some?) of this to methods section**

We focus on predicting the individual's donation in a year, focusing on the same set of outcomes used in the previous section. For this model to be useful for an *actual* prediction problem going forward, it would need to rely on 'ex-ante' characteristic that were already observable at the time of a career/EtG/pledge decision.^[An alternate project might try to predict future total EA donations in total in subsequent years and decades. This could embody both a prediction problem for individuals and uncertainties at the aggregate level. This is even further from what we are doing here, but seems worthwhile for future work, combining the EA survey with other measures and assessments.]  These might include immutable demographics, career plans, and pledges previously taken, and consider year and trend effects.

Although we have these models in mind, this is not what we are doing here. We are not posing a specific 'prediction problem' per se. Instead we are using machine learning tools built for prediction problems to generate 'data-driven insights' about factors related to EA donation behavior. Here, we do not than directly specifying all of the included components of the model (features, interaction terms, etc.). Instead we provide a large set of possible 'inputs' and use ML techniques to train models that should predict well *outside of the data they are trained on*. These models should do a good job of accomplishing the task: 'if you gave me a set of features of an EA, I should have a fairly accurate guess at what they will donate.'

The insights from these models should also be treated with caution. Again, they may not be deriving causal relationships. Furthermore, the parameters derived from model-fitting ML procedures are not in general unbiased or consistent, and it is difficult to derive proper confidence intervals for these parameters.

Still, the benefit of this exercise may be considered 'the derivation of robust and predictive relationships in the data that are mainly driven by the data itself, rather than our preconcieved ideas.' These models may also be useful building blocks towards future predictive work.

:::

*Discussion: 'elastic net' models:*

::: {.foldable}
**TODO: move (some?) of this to methods section**

In brief, the *elastic net* models involve linear models (log-linear in
our case), i.e., '*regressions*', that carefully 'penalize' the
(squared) magnitude of coefficients, in effect shrinking these towards
zero. The penalties are specifically 'tuned' and 'validated' to maximize
the predictive power of the model. As these are essentially regression
approaches, we can report the sign and magnitude of the coefficients
used in the 'optimally tuned' predictive model. (However, we should be
careful about interpreting these parameters, and statistical inference
is challenging. See e.g., @Mullainathan2017 for a detailed discussion.)

:::

*Discussion: 'Decision tree' models:*

::: {.foldable}
**TODO: move (some?) of this to methods section**

*Decision tree* models (which we do not report on here) take a different
approach, attempting to discern optimal ways to split the data into
conditional groups (e.g., 'income over 20k') and subgroups (e.g.,
'students versus nonstudents with income below 20k'), and finally making
a prediction for each subgroup at the 'bottom of the tree'. The *random
forest* approach extends the above to allow a sort of averaging across
an ensemble of trees that are derived independently, each selecting a
random subset of the features.
:::


We fit these models/approaches, starting with a wide set of potential features, to explain each of the three main outcomes considered above.^[The features include (nearly) all of those considered above, as well as 'where first heard of EA' responses. Giving What we Can' pledge was not included here, because this seems to be too close to the outcome of interest (i.e., it seems to be a very strong collider). We do not include the decision-tree model in this plot: while such models are useful for interpretation, they are relatively unstable.] Below, we plot the seven most 'important' features (aka variables) for predicting the log of donation amount (average of planned and actual, where available) according to the random forest and elastic net ('regression') models.

```{=html}
<!-- DR, @Oska -- surprisingly, I don't see d_gwwc_ever_0. Was this dropped? I actually think it *should* be dropped (as well as  perhaps d_career_etg), as these seem way too leaky/collidery. But I'm wondering what actually went on and how and why it might have been dropped.
Also, should/could we allow interactions and quadratic terms at all?
-->
```
```{=html}
<!-- Work is in `donation_pred.R`

Consider sensitivity to outliers -- possibly do prediction for those below 500k income -->
```
```{r read-pred-model-results}

# Need to see each of these variables for all models
tuning_folder <- here("analysis", "intermed_results", "donation_prediction", "tuning_results")
final_models <- here("analysis", "intermed_results", "donation_prediction", "final_models")

l_don_av_2yr_best_params <- readRDS(here(final_models, "l_don_av_2yr.Rdata")) %>%
  filter(!grepl("decision", model, ignore.case = TRUE))

don_share_inc_imp_best_params <- readRDS(here(final_models, "don_share_inc_imp.Rdata")) %>%
  filter(!grepl("decision", model, ignore.case = TRUE))

d_don_1k_best_params <- readRDS(here(final_models, "d_don_1k.Rdata")) %>%
  filter(!grepl("decision", model, ignore.case = TRUE))

l_don_av_2yr_best_params_filter <- readRDS(here(final_models, "l_don_av_2yr_filter.Rdata"))  %>%
  filter(!grepl("decision", model, ignore.case = TRUE))

# don_share_inc_imp_best_params_filter <- readRDS(here(final_models, "don_share_inc_imp_filter.Rdata"))
# d_don_1k_best_params_filter <- readRDS(here(final_models, "d_don_1k_filter.Rdata"))

l_don_av_2yr_best_params <- l_don_av_2yr_best_params %>%
  bind_rows(l_don_av_2yr_best_params_filter)

#don_share_inc_imp_best_params <- don_share_inc_imp_best_params %>% bind_rows(don_share_inc_imp_best_params_filter)

#d_don_1k_best_params <- d_don_1k_best_params %>%  bind_rows(d_don_1k_best_params_filter)
```

```{r}
recode_params <- function(df){
    # Shortcut function to tidy up variable names in parameter df
    df <- df %>% dplyr::select(model, vi) %>%
    tidyr::unnest(vi) %>%
    mutate(model = str_replace_all(model,
                                   c("preprocess_" = "", "_"  = " ")),
           Variable =  str_replace_all(Variable, key_eas_all_labels),
           Variable =  str_replace_all(Variable,
                                       c("_"  = " ", "_Student" ="", "ln" = "log")),
           Sign = if_else(is.na(Sign), "NA", Sign))
}


norm_vi <- function(df, slice_top = 7){
  # Shortcut function for calculating normalized variable importance
  # Not reproducible...
  df %>% group_by(model) %>%
    mutate(Norm = scale_var(Importance)) %>%
    group_by(Variable) %>%
    mutate(Total_Norm = sum(Norm)) %>%
    group_by(model) %>%
    slice_max(Total_Norm, n = slice_top) %>%
    mutate(Variable = fct_reorder(Variable, Norm))
}

plot_vi <- function(df, shapes = shape_colours){
  # Shortcut function for plotting normalized variable importance (output of norm_vi)
  df %>% ggplot(aes(y = Variable, x = Norm, colour = model, shape = Sign)) +
    scale_shape_manual(values = shapes) +
    geom_point(size = 4, stroke = 5) +
    xlab("Normalised feature importance") + ylab("")
}


#specific changing of variable  and signs for the below.
mutate_labels_sign_snip <- function(df) {
  df %>%
     mutate(
  Variable = str_replace_all(Variable,
    c("First-heard EA"="Heard EA:",
      "response" = "resp.",
      "Gender Gender" = "Gender",
      "unknown" = "No resp.",
      "Student Student" = "Student",
      "X80000" = "80000")),
  Sign = if_else(is.na(Sign), "NA", Sign)
    )
}

# Set colors for shapes as a named vector
shape_colours <- c("NA" = 120, "NEG" = 95, "POS" = 43)
```

```{r}
# Tidy up parameters
l_don_av_2yr_best_params_recode <- l_don_av_2yr_best_params %>% filter(is.na(filter_name)) %>% recode_params
l_don_av_2yr_best_params_recode_filter <- l_don_av_2yr_best_params %>% filter(!is.na(filter_name)) %>% recode_params

don_share_inc_imp_best_params_recode <- don_share_inc_imp_best_params %>% recode_params

d_don_1k_best_params_recode <- d_don_1k_best_params %>% recode_params
```

```{r plot-dec-trees, fig.show='hide', warning = FALSE, message=FALSE, eval=FALSE}
#NOT RUN
# Starting to plot decision trees (early stages)

l_don_av_2yr_tree <- l_don_av_2yr_best_params %>%
  mutate(
    model = str_replace_all(
      model, c("preprocess_" = "", "_"  = " "))
  ) %>%
  filter(!grepl("decision", model, ignore.case = TRUE))
) %$%
  workflowsets::extract_fit_parsnip(fit[[1]])

# l_don_av_2yr_treeX <- l_don_av_2yr_best_params %>%
#    mutate(
#     model = str_replace_all(
#       model, c("preprocess_" = "", "_"  = " "))
#   ) %>%
#   filter(str_det(model, "decision tree")) %$%
#   workflowsets::extract_fit_parsnip(fit[[1]])

```

```{r l_don_av_2yr-plot-importance}

#, fig.dim = c(10, 10)

(
iplot_l_don_av_2yr_best_params <-
  l_don_av_2yr_best_params_recode %>%
  filter(!grepl("tree", model, ignore.case = TRUE)) %>%
    norm_vi(slice_top = 10) %>%
    mutate_labels_sign_snip %>%
  slice_max(Total_Norm, n = 10) %>%
  mutate(Variable = fct_reorder(Variable, Norm)) %>%
  ggplot(aes(y = Variable, x = Norm, colour = model, shape = Sign)) +
    scale_shape_manual(values = c(120, 95, 43)) +
  geom_point(
    position = position_jitter(seed = 42,  width = 0.1, height = 0.1),
    size = 4, stroke = 5) +
  xlab("Normalised feature importance") +
  ylab(element_blank()) +
  ggtitle("Normalized importance scores: predicting log(don.)")
)

```

Above, we report the 'importance scores' for the ten most important features ('variables') for two distinct approaches to predicting log (average) donation.^[Note that (as is common in machine learning modeling) all features have been normalized to be on the same scale; for continuous features such as age and income we take the natural log of each, and divide each by two standard deviations of the logged feature, following @gelmanScalingRegressionInputs2008.
]

These importance scores are technically defined [here](https://topepo.github.io/caret/variable-importance.html#model-specific-metrics). For the elastic net ("linear reg") approach, we depict the coefficients' signs with a "+" or "-"; for tree/forest-based modeling this is less straightforward.

Most important predictors... <!-- XXX brief discussion here? further discussion in Forum?-->

<!-- ...  XXX Income (normalized, bottom-coded at 5000 USD, and logged) is the most important predictor in for each model, by a wide margin. After this, the relative importances vary. E.g., the random forest model deems age and student status to be particularly important, while linear model does not; in turn the linear model puts substantial importance on non-response to the student status question, but the random forest modeling does not. Both put substantial importance on years involved and 'not employed' statuses. Considering 'where one first-heard of EA', the linear model finds nonresponse and (to a lesser extent) GiveWell to be positively related and important, and 80000 hours to be negatively related to the predicted donation. It also finds Earning to Give to be somewhat important (and positively related), but perhaps to a lesser extent than we might have expected.
-->

*The 'non-response' features:*

::: {.foldable}
As they are difficult to interpret, and probably not useful for future predictions, we will basically *not discuss the non-response features* further.^[We might expect non-response to be associated with lower engagement, and perhaps lower donation rates. However, we are considering non-responses to particular questions *among* those who did report a donation amount. We could speculate about why non-response to the 'where heard of EA' question seems to be associated with donating more. E.g., perhaps people  who donate more tend to come from a group that don’t feel as connected to any of the EA organizations/sources listed, but didn’t want to put “other/don’t remember” so skip the question. Or perhaps this nonresponse is picking up something about the value of time or income that was not fully captured in the income question (which not everyone answers) We hope to investigate this in future work. However, we don't expect that this nonresponse will be useful in models aimed at predicting *future* donations, as the nonresponse here is basically occurring contemporaneously with donations. Note also that statisticians and economists caution against drawing inferences from machine-learning predictive models  (@mullainathan2017).
]
:::

As our data exhibited some large donation-amount outliers, (which is naturally tied to high income), we re-trained models on a subset of the data with only respondents who earned less than \$500,000, with importance scores reported below.

```{r l_don_av_2yr-plot-importance-filter}
(
iplot_l_don_av_2yr_best_params_filter <-
  l_don_av_2yr_best_params_recode_filter %>%
      filter(!grepl("tree", model, ignore.case = TRUE)) %>%
    norm_vi(slice_top = 10) %>%
    mutate_labels_sign_snip %>%
  slice_max(Total_Norm, n = 10) %>%
  mutate(Variable = fct_reorder(Variable, Norm)) %>%
  ggplot(aes(y = Variable, x = Norm, colour = model, shape = Sign)) +
    scale_shape_manual(values = c(120, 95, 43)) +
  geom_point(
    position = position_jitter(seed = 42,  width = 0.1, height = 0.1),
    size = 4, stroke = 5) +
  xlab("Normalised feature importance") +
  ggtitle("Importance: predict log(don.) (filter: income < 500k)")
)
```

The results are ... <!-- very similar. Again income is still the most important predictor for both models. The ranking of variables, and the importance scores are generally similar to the 'unfiltered' model above.^[Earning to give is no longer one of the 'top 10 most important' features, while having heard of EA through LessWrong shows up as fairly important and negative in the linear model. However, this may tell us more about the variance and sensitivity of the importance scores in these prediction model approaches than about any real differences in the correlates of donations.
]
-->


We next focus specifically on the elastic-net regression-based model.

```{r enet-coefs-plot}


# TODO -- important -- what are the base groups here, especially for 'first-heard'? I thought in previous work we made "don't remember" the base group! This is important for interpreting the coefficient signs!

# Plot all coefficients for elastic net model
#TODO (DR @ OF -- plot the coefficients rather than the importance weights? (the latter are absolute value t-values anyways))

#TODO/check DR @ OF -- why is 'year of survey 2015' (and 2017) in here? those years should have been removed from the dataset; I think they have been, but still it somehow reports an importance score? -- OK I am removing '0' importance scores for now

(
enet_coefs_ldon <- l_don_av_2yr_best_params_recode %>%
      filter(
        grepl("regression", model, ignore.case = TRUE)  & Importance!=0) %>%
  mutate_labels_sign_snip %>%
  #mutate(Norm = scale_var(Importance)) %>%
    mutate(Variable = fct_reorder(Variable, Importance)) %>%

  ggplot(aes(y = Variable, x = Importance, shape = Sign)) +
      scale_shape_manual(values = c(95, 43)) +
  geom_point(size = 2, stroke = 4) +
  xlab("Feature importance") +
  ggtitle("Importance scores: predicting log(don.)")
)

```

The graph above presents the overall ranking of importance scores within
the elastic-net linear regression model, with symbols depicting whether
these features take on a positive or negative sign. Results... <!-- ... XXX In addition to those
mentioned above, substantial importance is assigned to other 'first
heard' sources, e.g., GWWC and several related sources, as well as Ted
Talks positively predict log donation, while 80000 Hours, Facebook, and
Educational course negatively predict log donation. -->

#### Predictive model: Shares of income donated {.unnumbered}

Next we consider the shares-of-income donated^[...With income imputed and
bottom-coded as mentioned in previous sections.]

```{r don_share_inc_imp-plot-importance, fig.dim = c(8, 6)}

(
  iplot_don_share_inc_imp_best_params <- don_share_inc_imp_best_params_recode %>%
      filter(!grepl("tree", model, ignore.case = TRUE)) %>%
    mutate_labels_sign_snip %>%
    norm_vi(slice_top = 10) %>%
  plot_vi() +
  ggtitle("Importance scores: predicting share of income donated ")
)

```

Results...
<!-- ... XXX
(Log) Income is deemed highly important as a predictor of *share* of income donated, in the random forest models but not in the regression models.^[We speculate that this might be due to mechanical differences in the models: while a linear model allows predicted donations shares to 'continuously scale with income', the random forest modeling needs to use discrete 'branches'. Also note that we explored the relationship between donations and income and income shares in more detail above, particularly in the final section of our [descriptive modeling](#nonlin).
]
-->

Both types of models (discuss here or in forum only...) <!-- XXX  assign some importance to age and years involved; but this is much stronger in the random forest (in the linear models these have positive signs but only middling importance). On the other hand, while both also assign importance to reporting that one 'could not remember or determine where they first heard of EA'; this is much stronger in the linear model (where it is deemed the most important feature, and it has a positive sign). Overall, the importance scores are rather divergent. The linear models assign some importance (and positive sign) to people indicating that they first heard of EA through 'Raising for Effective Giving, EA Funds, the Foundational Research Institute or the "Swiss group"', through GWWC and related 'pledge/charity orgs', and, to a much lesser extent, through GiveWell or through a Ted Talk.^[We recognize that these 'first-heard' groupings are somewhat ad-hoc and only moderately homogeneous. This may merit future work. Note also that the random forest models also put some importance on the nonlinear 'years involved term'.
]
-->

<!-- bookdown_start -->

We present the remaining signed non-zero importance scores from the linear model in the figure below.

```{r plot_enet_coefs_don_share}

(
plot_enet_coefs_don_share <- don_share_inc_imp_best_params_recode %>%
      filter(
        grepl("regression", model, ignore.case = TRUE) & Importance!=0) %>%
    mutate_labels_sign_snip %>%
  #mutate(Norm = scale_var(Importance)) %>%
    mutate(Variable = fct_reorder(Variable, Importance)) %>%

  ggplot(aes(y = Variable, x = Importance, shape = Sign)) +
      scale_shape_manual(values = c(95, 43)) +
  geom_point(size = 2, stroke = 4) +
  xlab("Feature importance") +
  ggtitle("Importance scores: predicting log(don.)")
)

```

<!-- bookdown_end -->

#### Predictive model: Donated 1k USD or more {.unnumbered}

```{r}

(
  iplot_don_1k_best_params <- d_don_1k_best_params_recode %>%
      filter(!grepl("tree", model, ignore.case = TRUE)) %>%
    mutate_labels_sign_snip %>%
    norm_vi(slice_top = 10) %>%
  plot_vi() +
  ggtitle("Importance scores: predicting donation > 1k USD ")
)
```

Results... <!-- XXX ... Both approaches deem (logged, imputed, bottom-coded) income to be the
most important predictor of donating 1k USD or more. Both also consider
(log) years involved and age to be substantially important. The logistic
regression elastic-net model assigns importance to several sources of
'learning about EA', with 'Ted Talks', 'GWWC and related', and (less so)
Givewell (as well as nonresponse) deemed particularly important, with
positive signs . The random forest (but not the linear model) also deems
student status to be a somewhat important associated feature.
-->


Next, we plot the (nonzero) importance scores for all of the
coefficients in the elastic-net logistic model of donating 1k or more.

```{r plot_enet_coefs_don_1k}

(
plot_enet_coefs_don_1k <- d_don_1k_best_params_recode %>%
    filter(
      grepl("logistic", model, ignore.case = TRUE) & Importance!=0) %>%
    mutate_labels_sign_snip %>%
  #mutate(Norm = scale_var(Importance)) %>%
    mutate(Variable = fct_reorder(Variable, Importance)) %>%

  ggplot(aes(y = Variable, x = Importance, shape = Sign)) +
      scale_shape_manual(values = c(95, 43)) +
  geom_point(size = 2, stroke = 4) +
  xlab("Feature importance") +
  ggtitle("Importance scores: predicting donation > 1000k USD")
)

```


#### Model Performance {-#model-perf}

We consider 'how well these models predict'. Overall, these models offer (...) some predictive power. E.g., note that about  `r op(mean(as.numeric(eas_all_s_rl_imp$d_don_1k)-1, na.rm=TRUE)*100)`% of the relevant sample reports over 1k in donations. Our logistic regression model can correctly predict ... <!--- 75% of these outcomes with a false-positive rate of around 25%.^[In comparison, a predictor that did not condition on observables could only correctly 'catch' 75% of these outcomes if it (randomly) predicted this outcome over 75% of the time, leading to a false-positive rate over 75%. -->
]


*Discussion: measuring prediction success :*

::: {.foldable}
We may want to consider 'how successful' our predictive models are at
making practically useful predictions. In other words, 'how far off' are
the predictions and classifications on average, from the actual
outcomes. This procedure considers the fit on randomly-drawn *set-aside*
'testing data', data that has not been used in 'training' (or 'fitting')
the model. Below, we consider some commonly-used metrics.
:::

##### Regression Model Performance {.unnumbered}

In order to assess the usefulness of each predictive regression model we consider both root-mean-square-error (RMSE) and mean-absolute-error (MAE).

*Computing RMSE:*

::: {.foldable}

NOTE: probably move this to the methods book

RMSE (aka [RMSD](https://en.wikipedia.org/wiki/Root-mean-square_deviation)) can be interpreted as the average 'Euclidean distance' between the actual values and the model's prediction. For each observation (in the set-aside 'testing sample'), to construct RMSE we:

1. Measure the differences between the actual and predicted outcome (e.g., donation),  2.  Square these differences
3.  Take the average of these squared differences across all observations,  4.  Take the square root of this

:::

*Computing MAE :*

::: {.foldable}
NOTE: probably move this to the methods book

To construct mean-absolute-error (MAE) we simply 1.  Measure the *absolute-value* differences between the actual and    predicted outcome (e.g., donation) for each observation. 2. Take the average of these across all observations

MAE has a much more straightforward interpretation: it simply asks 'how far off are we, on average?'
:::

*Use of RMSE vs MAE :*

::: {.foldable}
NOTE: probably move this to the methods book

While the RMSE is used in the model *fitting* for various reasons, it is arguably less-interpretable and less-relevant than MAE in *judging* the model's fit in cases like this one. RMSE error negatively assesses the model fit based on *squared* deviations, and is thus very sensitive to 'large mistakes'. This may be relevant where 'large errors are much much worse than small ones' -- here, this is not so clearly the case. In the presence of data with large outlying observations, prediction will tend to be poor by this measure.

To address this we:  1.  Present both RMSE and MAE 2.  Re-run the models of (log average) *donations* for the subset of individuals with incomes at or below 500,000 USD. For this subset, there are fewer influential donation outliers.

The latter also allows us to consider how sensitive the *model fit* is to outliers.
:::

*Dealing with transformed outcomes :*

::: {.foldable}
NOTE: probably move this to the methods book

Note that when considering models where the outcome is transformed (e.g., log(donations)) we construct the RMSE and MAE by exponentiating to generate predictions for the *level* outcomes, and then measure the deviations on the level scale.

When considering predicted outcomes on the *logarithmic* scale, both RMSE and MAE indicate roughly 'how many exponential orders of magnitude our predictions for the *non-logged outcomes* are off. E.g., a MSE of 1.5 for 'log donation' suggests an we are off by about $exp(1.5) =$ `r op(exp(1.5))` times in terms of donations, getting them off by a factor of about 5. This conversion avoids such complications.
:::

<!-- Talk about filter for income, didn't make much different for non donation_amount models -->

```{r regression-model-performance}

l_don_av_2yr_best_params <- l_don_av_2yr_best_params %>% mutate(dv = "Donation amount*")

don_share_inc_imp_best_params <- don_share_inc_imp_best_params %>%
  mutate(dv = "Donation as a share of income")


(
  reg_model_performance <- purrr::map(list(l_don_av_2yr_best_params, don_share_inc_imp_best_params), ~.x %>%
      dplyr::select(dv, rmse, mae, model, matches("filter_name"))
      ) %>%
  bind_rows() %>%
    mutate(filter_name = if_else(is.na(filter_name), "None", filter_name)) %>%
  rename("Dependent variable" = dv,
         "RMSE" = rmse,
         "MAE" = mae,
         "Model" = model,
         "Filter" = filter_name) %>%
  kable(caption = "Regression model performance",
        digits = 2) %>%
  kable_styling() %>%
  add_footnote("Note: While the model was trained using logs of the dependent variable, RMSE and MAE were calculated in levels", notation = "symbol")
)

p_load(ie2misc)

mad_naive <- ie2misc::madstat(eas_all_s_rl_imp$donation_c, na.rm=TRUE)

sd_naive <- round((sd(eas_all_s_rl_imp$donation_c, na.rm=TRUE)), 0)

```

How does this compare to a 'naive model' in which we predict the average donation for everyone? Note that for the comparable unfiltered data, the mean absolute deviation is `r op(mad_naive)` and the standard deviation is `r op(sd_naive)`. The predictive model reduces this uncertainty substantially (...).

##### Classification Model Performance {-}

1.  ROC curve to consider the differences in the predictive power of the various models.
2. Comparison to  uninformed classifier, which would simply predict a positive outcome with some random probability $p$ (this maps out the 45 degree line).

<!-- DR @Oska -- maybe you can explain and label the ROC curve a bit more? Also, they look almost identical -- are you sure these are from the 3 distinct models? -->

```{r roc-curves}
# Add column for ROC curve

roc_curve <- yardstick::roc_curve
unnest <- tidyr::unnest
pr_curve <- yardstick::pr_curve

# Calculate ROC curve
d_don_1k_best_params$roc_curve <- d_don_1k_best_params %>% select(true_y, preds, pred_prob, model) %>%
  unnest(cols = everything()) %>%
  group_by(model) %>%
  group_map(~ roc_curve(., true_y, .pred_FALSE))

# Calculate AUC
d_don_1k_best_params$auc <- d_don_1k_best_params %>% select(true_y, preds, pred_prob, model) %>%
  unnest(cols = everything()) %>%
  group_by(model) %>%
  group_map(~ yardstick::roc_auc(., truth = true_y, estimate = .pred_FALSE))

# Extract AUC
d_don_1k_best_params <- d_don_1k_best_params %>%
  unnest_wider(., col = auc) %>%
  select(-c(.metric, .estimator)) %>%
  rename(auc = .estimate)

# Plot ROC curve
(roc_curve_d_don_1k <- d_don_1k_best_params %>% select(roc_curve, model) %>%
  unnest(cols = everything()) %>%
  rename_with(snakecase::to_title_case) %>%
  ggplot(aes(x = 1-Specificity, y = Sensitivity, colour = Model)) +
  geom_line() +
  geom_abline(slope=1, intercept = 0, linetype = "dotted") +
  theme_bw()
)
```

*Discussion: ROC curve, AUC:*

::: {.foldable}

NOTE: probably move this to the methods book

An ROC curve plots the true positive rate (sensitivity) as a function of the false positive rate (1-specificity). Here the true positive rate gives the rate at which our model correctly predicts a respondent to donate over \$1000, with the false positive rate giving the rate at which these predictions are incorrect.

Better classifiers will have an ROC curve that is further North-West, with the perfect classifier being an L-shaped curve passing through $(0,0) \rightarrow(0,1) \rightarrow(1,1)$. Where classifiers ROC curves do not cross, it is clear that one will be performing better than another. That is not the case here. Both models seem to be performing relatively similarly, with the ROC curves overlapping somewhat. It is difficult to discern which model is performing the best, and this will depend on our criterion.

However, both models yield curves substantially above the 45 degree line, thus substantially outperforming an uninformed classifier. For example, if we are willing to accept about a 25% rate of false positives (falsely predicting a 1k+ donation), the logistic regression model correctly predicts about 75% of true positives (and the random forest model about 73%).

We can use the area under the curve (AUC) measure to compare classifiers
for all costs of misclassification. This measure quantifies how close
the ROC curve is to the optimal L-shaped curve.
:::

```{r auc-values}
calculate_metrics <- function(df, metrics, preds = preds, true_y = true_y){
  df %>% mutate(purrr::map2_dfr({{true_y}}, {{preds}}, ~ purrr::map_dfc(metrics,
                                                                        do.call,
                                                                        list(.x, .y))))
}

class_metrics <- list(accuracy = yardstick::accuracy_vec,
                      recall = yardstick::recall_vec,
                      precision = yardstick::precision_vec,
                      f1_score = yardstick::f_meas_vec)

# Adding a no skill classifier to d_don_1k
## Messy code
truth <- d_don_1k_best_params$true_y[[1]]
majority <- tail(names(sort(table(truth))), 1)

pred_majority <- as.logical(rep(majority, length(truth)))

.pred_FALSE <- 1 - pred_majority
.pred_TRUE <- 1 - .pred_FALSE

pred_prob <- tibble(.pred_FALSE, .pred_TRUE, truth)

no_skill <- tibble(model = "No Skill",
                   true_y = list(truth),
                   pred_prob = list(pred_prob),
                   preds = list(factor(pred_majority, levels = levels(truth)))) %>%
  calculate_metrics(class_metrics)

no_skill$auc <- yardstick::roc_auc_vec(truth, .pred_TRUE)

purrr::map_df(list(no_skill, d_don_1k_best_params), ~.x %>%
             select(model, auc, accuracy)) %>%
  rename_with(snakecase::to_title_case) %>%
  rename(AUC = Auc) %>%
  kable(digits = 3) %>%
  kable_styling()
```

Here we see that the ... <!-- XXX ... random forest performs best in terms of AUC. All
models perform much better than a no-skill classifier which simply
predicts the majority class. This would suggest that there is predictive
power in the explanatory variables included in these models. -->

<!-- Classes aren't perfectly balanced, but I believe that ROC random classifier curve is correct as the baseline should be fixed in ROC (see https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/) -->

```{r}
# Calculate precision recall curve
# d_don_1k_best_params$pr_curve <- d_don_1k_best_params %>% select(true_y, preds, pred_prob, model) %>%
#   unnest(cols = everything()) %>%
#   group_by(model) %>%
#   group_map(~ pr_curve(., true_y, .pred_FALSE))
#
# d_don_1k_best_params %>% select(pr_curve, model) %>%
#   unnest(cols = everything()) %>%
#   ggplot(aes(x = recall, y = precision, colour = model)) +
#   geom_path() +
#   coord_equal() +
#   theme_bw() +
#   geom_hline(aes(yintercept = 0.46))
```

```{r conf-mat}

best_d_don_1k_preds <- d_don_1k_best_params %>%
  filter(f1_score == max(f1_score)) %$% preds[[1]]

d_don_1k_true <- d_don_1k_best_params %>%
  filter(f1_score == max(f1_score)) %$% true_y[[1]]
#DR: @Oska, I'm just guessing this is what you wanted

d_don_1k_preds_df <- tibble(preds = best_d_don_1k_preds,
                            truth = d_don_1k_true)

cm <- yardstick::conf_mat(d_don_1k_preds_df, truth, preds)

autoplot(cm, type = "heatmap") +
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1") +
  ggtitle("Predicting Donations over $1k")
```

<!-- Todo: add a discussion and explanation of this. I will explain or link an explanation of the concepts ... such as the one in the [scratch notes below](#classification_model_disc). -->


### Summary of modeling results -- see [introduction](#sum-results)

<!-- At the end of the introduction ['summary' section](#sum-results) we give
an overall characterization of our modeling results in bullet points; we
do not repeat it here. -->

## Appendix: Extra analysis and robustness checks {#robust-appendix}

EA Forum Post: "See [bookdown
appendix](https://rethinkpriorities.github.io/ea_data_public/eas_donations.html#robust-appendix)
for further analysis and robustness checks."

<!-- bookdown_start -->

This appendix contains further and analysis and robustness checks, as
mentioned or alluded to in the above 'main text'.

::: {.rmdnote}
This section is part of the 'Bookdown' version only and not in the EA
forum post.
:::

### 'Saved to donate later' {#save-to-don .unnumbered}

**Added 30 Oct 2021**

::: {.rmdnote}
A [comment from
TylerMaule](https://forum.effectivealtruism.org/posts/nb6tQ5MRRpXydJQFq/ea-survey-2020-series-donation-data?commentId=gx95kaca5owGxsktf)
prompted us to revisit this question.
:::

In 2020, after the main donation questions, we also asked

  Are you presently saving money to donate later, rather than
  donate now? If so, roughly how much money are you planning
  to save in 2020 for this purpose?

We report on this only briefly here. We did not emphasize it for several
reasons.

1.  It is unclear how much this responses reflects a strong intention or
    commitment (e.g., a DAF contribution) that will be manifested in
    actual later donations. The amount someone reports they are
    "presently saving ... to donate later" could express only a a vague
    intention to donate late and may be subject to them changing their
    mind, changes in financial circumstance, or value drift.

2.  It is difficult to aggregate this savings with *actual* donations,
    as this could lead to double-counting. However, reporting all
    results for both measures (saving and actual donations) could make
    this report bulky.

However, we hope to look more closely at saving-to-donate behavior in
future. This may be particularly relevant for [patient
longtermists](https://80000hours.org/2020/08/the-emerging-school-of-patient-longtermism/).

**Key figures**

```{r}

eas_new %<>%
  rowwise() %>%
  mutate(
    donate_later_amount_na0 =
      if_else(
      is.na(donate_later_amount_c), 0, donate_later_amount_c),
    donate_later_amount_min50 = max(50, donate_later_amount_c),
    donate_later_amount_min50_na0 = if_else(
      is.na(donate_later_amount_min50), 0, donate_later_amount_min50)
    ) %>%
ungroup()

#TODO: maybe move above to build side

num_savdon <- sum(str_det(eas_new$donate_later, "yes")==TRUE, na.rm=TRUE)
num_na_savdon <- sum(is.na(eas_new$donate_later)==TRUE, na.rm=TRUE)

count_eas_new <- NROW(eas_new)

med_savdon <- med(eas_new$donate_later_amount_c)
mean_savdon <- mean(eas_new$donate_later_amount_c, na.rm=TRUE)

mean_savdon0 <- mean(eas_new$donate_later_amount_na0, na.rm=TRUE)
med_savdon0 <- med(eas_new$donate_later_amount_na0)

mean_savdon0_lt100k <- mean(eas_new$donate_later_amount_na0[eas_new$donate_later_amount_na0<=100000], na.rm=TRUE)

```

-   `r op((num_savdon/count_eas_new)*100)`% of responses report saving
    to donate later. (This represents
    `r op((num_savdon/(count_eas_new-num_na_savdon)*100))`% of those who
    answered this question).

-   Among those who report saving to donate, median donations are
    `r med_savdon` USD and mean donations are `r op(mean_savdon)` USD

-   Setting nonresponses to zero the mean 'saved to donate' amount
    (including zeroes) is `r op(mean_savdon0)` (and the median is  `r op(med_savdon0)` of course)

We provide a histogram below, with the horizontal axis on a logarithmic
scale.

```{r}
#ddate -- adding 'saving to donate' amount here


#eas_new %>% tabyl(donate_later) %>% .kable() %>% .kable_styling()

(
  sav_don_ty <- eas_new %>%
    hist_plot_lscale(eas_new$donate_later_amount_min50, breaks = breaks) +
    labs(title="Histogram of 'saved to to donate later, 2020, bottom-coded at 50'", x = "Amount in USD", y = "Number of respondents"))


```

### Donations by engagement level {#don_eng .unnumbered}

```{r}

don_share_income_by_engage_sp

```

### Donations and income by student/employment status {#don_inc_by_student .unnumbered}

Below, we tabulate income and donations, split by student and employment
status.

```{r}

don_inc_by_student

# don_inc_by_student %>%    as_image(width = 8)

```

### Donation as share of income by tenure, 'faceted' by referrer to survey\* {#don_share_by_tenure_facet_referrer .unnumbered}

```{r don_share_by_tenure_facet_referrer}

(
don_share_by_tenure_facet_referrer <-
  eas_new %>%
  filter(!is.na(age_approx_ranges)) %>%
  ggplot() +
  aes(x = tenure, y = don_share_inc_imp) +
 geom_point(size = 0.15, colour = "#0c4c8a", position = position_jitter(seed = 42,  width = 0.1, height = 0.001)) +
  geom_smooth(span = 0.75) +
  scatter_theme +
  facet_grid(vars(), vars(referrer_cat2), scales = "free") +
labs(title = "2019 donation as share of (imputed) income by time in EA faceted by referrer category") +
    labs(x = get_label(eas_new$referrer_cat2)) +
  ylim(0, 0.3)
) 

#%>% ggplotly

```

For several major groups of referrers, we (again) see a strong positive
association between time-in-EA and donations as a share of income.

### Additional: Donations and income by whether a Longtermist cause is top priority {.unnumbered}

We were asked to compare the donations of those who prioritize
longtermist causes to the remainder of EAs. Below, we tabulate this by
whether a respondent gives *some* longtermist cause as high a priority
rating as any other cause.

```{r}

#eas_new %>% tabyl(lt_above_mn_priority)

  #lt_4plus_priority = case_when(

don_income_by_priority <-
    eas_new %>%
        dplyr::select(income_k_c, donation_c, donation_plan_c, don_share_inc_imp,  lt_top_priority) %>%
        tbl_summary( by = lt_top_priority,
                      type = c(all_continuous()) ~ "continuous2",
      statistic = list(all_continuous() ~ sumstatvec),
        label = list(income_k_c ~ "Income in $1000 USD",
                   donation_c ~ "Last year's donation (in USD)",
                   donation_plan_c ~ "Latest planned donation",
                  don_share_inc_imp ~ "Last year's donation as share of (imputed) income"  ),
            missing = c("no")
        ) %>%
    bold_labels() %>%
    add_n() %>%
    add_overall()

don_income_by_priority

 # don_income_by_priority %>% as_image(width = 8)





# Todo (medium): fix labeling and titles above
# Todo (High): add standard error and perhaps tests, perhaps  Bayes factors

# Todo (Medium-high): visualization of donations by top-priority category (poverty, animals, LT, meta/other)


```

```{=html}
<!--
  Considering this across major 'highest priority' categories (need to first create a categorical variable for this ... "Poverty highest", "Animals highest", "LT or AI highest", "Meta-highest", "Tie for highest")
-->
```
```{r, eval=FALSE}

don_inc_priority_plot <-  eas_new %>%
  grp_sum(income_c_imp_bc5k, donation_c, XXX) %>%
  plot_grp(country_big) +
  xlab("Mean income in USD (imputed if <5k/missing") +
  ylab("Mean donations, CIs")

```

The above presentation is meant to be broadly descriptive. Overall, it
appears that those who prioritize long-term causes tend to donate a bit
less at median but more at mean, and there is a lot of overlap between
the two groups.

### Planned donation by year, comparison tests {#plan-don-by-year}

We report the amount people say they expect to donate in the current
(survey) year below. Note the high share of missing values for 2017, and
the substantially-larger sample in 2018. As noted above, this suggests
that comparing the planned donations for 2018 (in EA survey 2018) to
actual donations for 2018 (reported in `r year_n-1` EA survey) may not be
informative.

```{r plan_don_per_year}

(
plan_don_per_year <- eas_all %>%
 filter(year>=2017) %>%
 group_by(year) %>%
 summarise("Number reported" = sum(!is.na(donation_plan_c)), N = n(),
            "Number missing" = sum(is.na(donation_plan_c)),
           "Proportion missing (%)" = sum(is.na(donation_plan_c))/n()*100,
            "Mean planned donation" = mean(donation_plan_c, na.rm=TRUE),
           "Median planned donation" = median(donation_plan_c, na.rm=TRUE)) %>%
 kable(caption = "Planned donation per year") %>%
 kable_styling()
)

```

Below, the results of the signed rank test for plan versus actual
donation, excluding 0 donations.

```{r report-w_signed_test_planned_actual_no0s}

w_signed_test_planned_actual_no0s
```

### Donations by income 'earning-to-give' career status

```{r}

(
  don_income_etg <- eas_all %>%
  filter(year_n>=2018) %>%
  ggplot(aes(x = income_c_imp_bc_k, y = donation_2019, color = d_career_etg)) +
  geom_point(size = 1, alpha = 0.7) + # draw the points
  geom_smooth(aes(method = 'loess',
                  fill = d_career_etg)) + # @Oska -- note I am using  local smoothing here.
  scale_x_log10(name = "Income in $1K USD (imputed if <5k/missing)", n.breaks = 5, limits = c(5, 5000)) +
  scale_y_log10(
    name = "Donation amount",
    # labels = scales::dollar,
    labels = scales::label_number_si(prefix = "$"),
    n.breaks = 10
  ) +
  scale_color_discrete(name = "Earning to give") +
  scale_fill_discrete(guide = "none") +
  theme(axis.text.x = element_text( angle = 90, vjust = 0.5, hjust = 1 ),
        legend.position = c(.87,.15),
        legend.background = element_rect(fill=alpha('blue', 0.01)))
)


#don_income_etg + facet_wrap(~d_gwwc_ever)
```

### Donation shares by income (by US-residence, `r year_n-1`) {#usa-don}

By request, we consider median and mean shares of income donated,
separately tallying for US residents and others. This allows us to
compare donation rates to those reported for the US overall. In
particular, @meer2020 reports a roughly 1.6-2.1% percent share of income
donated throughout all of the US income quantiles (except or the lowest
5%). The statistics below suggest that EAs who fill out the donation
questions in the survey to donate a greater share of their income,
perhaps at least twice as much.

Recall that the *mean* share of *total* (imputed) income donated (for
2019) was `r op(tot_don/tot_inc_imp_bc*100)`% (imputing income where
below 5k or missing).

If we focus on US-resident nonstudents across all years, the mean share
is `r op(tot_share_don_us_nonstudent*100)`%.

```{r}

mean_don_sharenonstud_usa_w20 <-  eas_all %>%
  filter(d_live_usa==1 & d_student==0) %>%
  rowwise() %>%
  mutate(don_share_inc_imp_bc5k = min(don_share_inc_imp_bc5k, 0.2)) %>%
  ungroup() %>%
  summarize(mean_share_nonstud_usa_w20 = mean(don_share_inc_imp_bc5k, na.rm=TRUE) )

```

We could also 'average the individual shares donated'; as this is highly
sensitive to outliers (e.g., people with the lowest incomes reporting
donating many times this share), we 'Winsorise', capping each
individual's 'share of income donate' at some percent. Following this
procedure and capping at 20%, the mean share of income donated, across
all years, for US-resident non-students in the EA survey, is
`r op(mean_don_sharenonstud_usa_w20$mean_share_nonstud_usa_w20*100)`%.

We plot donation shares on a log scale, for US and non-US residents
separately (across all years). Below, the blue dash gives us the median
for USA (vs non-USA), and the smoothed curve tries to best fit the mean.
The left graph is 'Winsorised' at 20%, and right one at 40%

```{r}

don_share_income_by_usa  <- eas_all %>%
  filter(!is.na(d_live_usa)) %>%
  rowwise() %>%
  mutate(don_share_inc_imp_bc5k = min(don_share_inc_imp_bc5k, 0.4)) %>%
  ungroup() %>%
   group_by(d_live_usa) %>%
  mutate(med_usa = median(don_share_inc_imp_bc5k, na.rm=TRUE)) %>%
  ungroup() %>%
  ggplot(aes(x = income_c_imp_bc_k, y = don_share_inc_imp_bc5k)) +
  ggpointdensity::geom_pointdensity(adjust = 0.25) +
  geom_smooth(method = "loess") +
geom_hline(aes(yintercept=med_usa), linetype="dashed", size=0.5, color = "blue") +
  geom_hline(yintercept=0.1, linetype="dashed", size=0.5, color = "red") +
  scale_y_continuous(labels = scales::label_percent(accuracy = 1L)) +
  scale_x_log10(breaks = scales::log_breaks(n=7)) +
  scale_color_viridis_c("Neighbours") +
  xlab("Income in $1K USD (imputed if missing, bottom-code at 5k)") +
  theme(axis.title.x = element_text(size = 10)) +
   ylim(NA,.21) +
  ylab("Donations/Income (top-code at 40%)") +
  facet_wrap(~d_live_usa)  +
  labs(title="By US residence: 2019 'Don. share of income' by income (w/ imputing)")


don_share_income_by_usa_w20  <- eas_all %>%
  filter(!is.na(d_live_usa)) %>%
  rowwise() %>%
  mutate(don_share_inc_imp_bc5k = min(don_share_inc_imp_bc5k, 0.2)) %>%
  ungroup() %>%
   group_by(d_live_usa) %>%
  mutate(med_usa = median(don_share_inc_imp_bc5k, na.rm=TRUE)) %>%
  ungroup() %>%
  ggplot(aes(x = income_c_imp_bc_k, y = don_share_inc_imp_bc5k)) +
  ggpointdensity::geom_pointdensity(adjust = 0.25) +
  geom_smooth(method = "loess") +
geom_hline(aes(yintercept=med_usa), linetype="dashed", size=0.5, color = "blue") +
  geom_hline(yintercept=0.1, linetype="dashed", size=0.5, color = "red") +
  scale_y_continuous(labels = scales::label_percent(accuracy = 1L)) +
  scale_x_log10(breaks = scales::log_breaks(n=7)) +
  scale_color_viridis_c("Neighbours") +
  xlab("Income in $1K USD (imputed if missing, bottom-code at 5k)") +
  theme(axis.title.x = element_text(size = 10)) +
  ylab("Donations/Income ('Windsorised' (top-coded) at 40%)") +
  ylim(NA,.21) +
  facet_wrap(~d_live_usa)  +
  labs(title="By US residence: 2019 'Don. share of income' by income (w/ imputing)")

mean_don_share_income_usa_nonstudent_w20  <- eas_all %>%
  filter(d_live_usa==1 & d_student==0) %>%
  rowwise() %>%
  mutate(don_share_inc_imp_bc5k = min(don_share_inc_imp_bc5k, 0.2)) %>%
  ungroup() %>%
  dplyr::summarise(mean_don_share= mean(don_share_inc_imp_bc5k, na.rm=TRUE)
  )




#don_income_etg + facet_wrap(~d_gwwc_ever)
```

### Models: nonresponse coefficients {#nonresponse}

Below we report the 'no response' coefficients for all of the models
presented above (recall that these are 'controls' included in each
specification). These are exponentiated, as in the presentations above.
We report these only here because we suspect they are of less direct
interest.

```{r}
# make nice table (and/or forest plot?) of nonreseponses

(
  nr_coefs_table <-  bind_rows(nr_coefs_qp, nr_coefs_fl,nr_coefs_logit) %>%
    select(outcome, term, model_name, estimate, std.error, p.value, conf.low, conf.high) %>%
  filter(!str_detect(model_name, "GWWC"))  %>%
  filter(str_detect(term, "response|na|NA"))  %>%
     mutate(
    outcome = str_replace_all(outcome,  key_eas_all_labels)
    ) %>%
    arrange(term) %>%
  .kable(digits=3) %>%
    .kable_styling()
)

```

<!-- bookdown_end -->


