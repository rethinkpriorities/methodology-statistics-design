{
  "hash": "be816d521118eaabed4ab1aaf1b05eaa",
  "result": {
    "markdown": "# Other suggested sections\n\n## Multilevel modeling\n\n[Slack thread on 'what are random effects'?](https://rethinkpriorities.slack.com/archives/G01BDCD2QPR/p1660060733533869)\n\n[A beginner's guide to LMER](https://rstudio-pubs-static.s3.amazonaws.com/63556_e35cc7e2dfb54a5bb551f3fa4b3ec4ae.html) ... the `LMER` package uses a maximum likelihood (not Bayesian) approach\n\n\n### Discussing 'partial pooling'\n\nFrom [Tristan Mahr's vignette](https://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/) -- this vignette explains a lot (but it doesn't go into the formal maths)\n\n\n<!-- \nI’m still trying to get my head around (quickly) what we are talking about when we say ‘mixed models’ and ‘random effects’, at least when using lmer.\n\nSuppose I run the following model\n\nm_video_aud <-  all_surveys all_surveys %>%  filter(wave==\"video\") %>% \nlmer(interest80k_mn ~ 1 + condition + audience + condition:audience + (1|reader),\ndat=.)\n\nThis allows a “random intercept” for `reader` ...  But what does that mean? \nHere is my interpretation — how wrong am I?\n The intercept (“average interest all else comparable”) for each of the two readers is equal to the overall average intercept plus some ‘random deviation’ drawn from a [normal?] distribution   \n \nIn effect, this means that the ‘reader coefficient adjustment’ coefficient will be smaller than  the adjustment we would get if we just looked at the difference in (the residual of the) outcome across readers. I.e., we ‘adjust less’ for reader than we would do if we made reader a fixed effect (i.e., a ‘dummy in a standard regression’). \n\nComputationally, I think this happens because  we are assuming something like a normal distribution of the random deviation, where smaller deviations are more likely than large ones, and 0 is the most likely. Thus the larger is the ‘observed reader difference’ we see in the data, the more likely this is o be a ‘freak random draw’ … thus we shrink this difference term a bit towards 0.  \n-->\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4)\n#> Loading required package: Matrix\nlibrary(dplyr)\nlibrary(tibble)\n\n# Convert to tibble for better printing. Convert factors to strings\nsleepstudy <- sleepstudy %>% \n  as_tibble() %>% \n  mutate(Subject = as.character(Subject))\n\n# Add two fake participants\ndf_sleep <- bind_rows(\n  sleepstudy,\n  tibble(Reaction = c(286, 288), Days = 0:1, Subject = \"374\"),\n  tibble(Reaction = 245, Days = 0, Subject = \"373\"))\n\ndf_sleep\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 183 × 3\n   Reaction  Days Subject\n      <dbl> <dbl> <chr>  \n 1     250.     0 308    \n 2     259.     1 308    \n 3     251.     2 308    \n 4     321.     3 308    \n 5     357.     4 308    \n 6     415.     5 308    \n 7     382.     6 308    \n 8     290.     7 308    \n 9     431.     8 308    \n10     466.     9 308    \n# … with 173 more rows\n```\n:::\n:::\n\n\n\n## (Open and robust science: RP attitudes, discussions, resources) {#opensci}\n\nIntegrate from:\n\n[Reinstein discussions here](https://daaronr.github.io/metrics_discussion/robust-diag.html)\n\nCode to do [Robustness checks as a 'specification chart'](https://github.com/ArielOrtizBobea/spec_chart), and sensitivity analysis\n\nSource: https://pbs.twimg.com/media/ESyDHGjUYAYHfva?format=jpg&name=medium\n\n## (Meta-analysis) {#meta}\n\nIncorporate and consolidate from [Reinsteins meta notes](https://daaronr.github.io/metrics_discussion/metaanalysis.html) and more\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}