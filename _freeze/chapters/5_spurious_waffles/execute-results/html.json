{
  "hash": "09c36ba55e6d025518faee1eb4354710",
  "result": {
    "markdown": "# Ch 5. The Many Variables & The Spurious Waffles {#mcelreath_ch5}\n\n\nLoad the [Waffle House](https://www.snopes.com/fact-check/fema-waffle-house-index/) data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rethinking)\nlibrary(pacman)\ndata(WaffleDivorce)\nd <- WaffleDivorce\n\np_load(daggity, install=FALSE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(WaffleDivorce)\n\n#detach(package:rethinking, unload = T)\nlibrary(brms)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in completeSubclasses(classDef2, class1, obj, where) : \n  trying to get slot \"subclasses\" from an object of a basic class (\"NULL\") with no slots\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError: package or namespace load failed for 'tidyverse':\n .onLoad failed in loadNamespace() for 'dbplyr', details:\n  call: setClass(cl, contains = c(prevClass, \"VIRTUAL\"), where = where)\n  error: error in contained classes (\"character\") for class \"ident\"; class definition removed from 'dbplyr'\n```\n:::\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n:::\n\n\n\n> Reasons given for multiple regression models include: (1) Statistical -control” for confounds\n\nDR: but endogenous controls can make this problem worse\n\n## 5.1. Spurious association {-}\n\n> But there’s no reason high marriage rate must cause more divorce\n\nDR: what would this even mean? ... the 'causal' part is ambiguous in this example, particularly when we are talking about aggregates like 'rates for the whole population'. I'm familar with the framework of the Rubin Causal model, where we consider the counterfactual state of some real-world outcome, when another real-world outcome would be set at one level or another. \n\n> easy to imagine high marriage rate indicating high cultural valuation of marriage and therefore being associated with low divorce rate\n\nDR: but the latter is not causal\n\n> Since the outcome and the predictor are both standardized, the intercept - should end up very close to zero \n\n$\\alpha ∼ Normal(0, 0.2)$\n\nDR: why not exactly 0? Is this a 'sample drawn from the population' issue?\n\n\n> So when $\\beta_A = 1$, a change of 1.2 years in median age at marriage is associated with a full standard deviation change in the outcome variable  [which he thinks is absurdly large]\n\n\nDR: But why are these 'absurdly large', maybe the sd in the outcome is small too?\n\n> [a model] that includes both age at marriage and marriage rate will help us. \n\nDR: maybe but these are aggregates. Casual thinking will be difficult.  Primitives are more like 'whether an individual of a.certain age gets married or divorced'\n\n> causal impact\n\nDR: redundant language. All impacts are causal :)\n\n> DAG will tell you the consequences of intervening to change a variable. But only if the DAG is correct\n\n> Perhaps a direct effect would arise because younger people change faster than older people and are therefore more likely to grow incompatible with a partner. Second, it has an indirect effect by influencing the marriage rate, which then influences divorce, A $\\rightarrow$  M $\\rightarrow$  D\n\n> these different arrows, we need more than one statistical model.\n\n> Model m5.1, the regression of D on A, tells us only that the total influence of age at marriage is strongly negative with divorce rate\n\nDR: May not be *influence*... because unobserved factor could drive both\n\n> The 'total' here means we have to account for every path from A to D. There are two such paths in this graph: $A \\rightarrow  D$, a direct path, and $A \\rightarrow  M \\rightarrow D$, an indirect path. In general, it is possible that a variable like A has no direct effect at all on an outcome like D. It could still be associated with D entirely through the indirect path. That type of relationship is known as mediation, and we'll have another example later.\n\n\n> This DAG is also consistent with the posterior distributions of models m5.1 and m5.2. Why? Because both M and D 'listen' to A\n\n> conditional independencies  ... which variables become dis-associated when we condition on some other set of variables. D\n\n... that every pair of variables is correlated. This is because there is a causal arrow between every pair. These arrows create correlations.\n\n> DR: but effects could add up to zero by coincidence. (Actually, he has a case like this further down, he just fails to mention it here)\n\n> They share a cause, and this leads them to be correlated with one another through that cause. But suppose we condition on A. All of the information in M that is relevant to predicting D is in A. So once we've conditioned on A, M tells us nothing more about D. So in the second DAG, a testable implication is that D is independent of M, conditional on A. In other words, $D \\perp  M|A$.\n\n> Here’s the code to define the second DAG and display the implied conditional independencies.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dagitty)\n\nDMA_dag2 <- dagitty::dagitty('dag{ D <- A -> M }')\nimpliedConditionalIndependencies( DMA_dag2 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nD _||_ M | A\n```\n:::\n:::\n\n\n> So for example once you fit a multiple regression to predict divorce using both marriage rate and age at marriage, the model addresses the questions: (1) After I already know marriage rate, what additional value is there in also knowing age at marriage? (2) After I already know age at marriage, what additional value is there in also knowing marriage rate?\n\nDR: can we differentiate 1 from 2?\n\n### Coding data vis and univariate models  (Kurz) {-}\n\nKurz on themes and plots:^[ Going forward, each chapter will have its own plot theme. In this chapter, we’ll characterize the plots with `theme_bw() + theme(panel.grid = element_rect())` and coloring based off of \"firebrick\".]\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"ggrepel\", dependencies = T)\nlibrary(ggrepel)\n\nd %>%\n  ggplot(aes(x = WaffleHouses/Population, y = Divorce)) + #note we can put transformations *within* the `aes`\n  stat_smooth(method = \"lm\", fullrange = T, size = 1/2,\n              color = \"firebrick4\", fill = \"firebrick\", alpha = 1/5) + #the linear plot and the shaded 'some sort of bounds' thing\n  geom_point(size = 1.5, color = \"firebrick4\", alpha = 1/2) +\n  geom_text_repel(data = d %>% filter(Loc %in% c(\"ME\", \"OK\", \"AR\", \"AL\", \"GA\", \"SC\", \"NJ\")),  \n                  aes(label = Loc), \n                  size = 3, seed = 1042) +  # this makes it reproducible\n  scale_x_continuous(\"Waffle Houses per million\", limits = c(0, 55)) +\n  ylab(\"Divorce rate\") +\n  coord_cartesian(xlim = c(0, 50), ylim = c(5, 15)) +\n  theme_bw() +\n  theme(panel.grid = element_blank())  #removes gridlines\n```\n\n::: {.cell-output-display}\n![](5_spurious_waffles_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n*Note: I'm skipping the map plotting, for now, even though it's a cool vis.*\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Standardize median age marriage\"}\nd <-\n  d %>%\n  mutate(MedianAgeMarriage_s = (MedianAgeMarriage - mean(MedianAgeMarriage)) /\n           sd(MedianAgeMarriage))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"fit the first univariable model.\"}\nb5.1 <- \n  brm(data = d, \n      family = gaussian,\n      Divorce ~ 1 + MedianAgeMarriage_s,\n      prior = c(prior(normal(10, 10), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(uniform(0, 10), class = sigma)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5,\n      file = \"fits/b05.01\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(b5.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Divorce ~ 1 + MedianAgeMarriage_s \n   Data: d (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept               9.69      0.22     9.25    10.12 1.00     5173     4232\nMedianAgeMarriage_s    -1.04      0.21    -1.46    -0.63 1.00     4436     4274\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.51      0.16     1.24     1.86 1.00     5403     4424\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nBelow...\n\n- Create: `nd` -- tibble of standardized predictor values to plot over\n- apply `fitted` to `nd` with the `newdata` argument to fit values over this range (rather than over the  data the model was fit on)\n\n\"to return model-implied expected values for `Divorce`\"\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define the range of `MedianAgeMarriage_s` values we'd like to feed into `fitted()`\nnd <- tibble(MedianAgeMarriage_s = seq(from = -3, to = 3.5, length.out = 30))\n\n# now use `fitted()` to get the model-implied trajectories\nf <- \n  fitted(b5.1, newdata = nd) %>%\n  as_tibble() %>%\n  # tack the `nd` data onto the `fitted()` results\n  bind_cols(nd)\n```\n:::\n\n\n\nAnd plot this... \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot\nggplot(data = f, \n       aes(x = MedianAgeMarriage_s, y = Estimate)) +\n  geom_smooth(aes(ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  geom_point(data = d, #overlaying the original data\n             aes(y = Divorce), \n             size = 2, color = \"firebrick4\") +\n  ylab(\"Divorce\") +\n  coord_cartesian(xlim = range(d$MedianAgeMarriage_s), \n                  ylim = range(d$Divorce)) +\n  theme_bw() +\n  theme(panel.grid = element_blank())                   \n```\n\n::: {.cell-output-display}\n![](5_spurious_waffles_files/figure-html/unnamed-chunk-9-1.png){width=288}\n:::\n:::\n\n\nNext   ... they do the same thing but with marriage rate as the predictor\n\n::: {.cell}\n\n```{.r .cell-code}\nd <-\n  d %>%\n  mutate(Marriage_s = (Marriage - mean(Marriage)) / sd(Marriage))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nb5.2 <- \n  brm(data = d, \n      family = gaussian,\n      Divorce ~ 1 + Marriage_s,\n      prior = c(prior(normal(10, 10), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(uniform(0, 10), class = sigma)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5,\n      file = \"fits/b05.02\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(b5.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Divorce ~ 1 + Marriage_s \n   Data: d (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nPopulation-Level Effects: \n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      9.68      0.25     9.19    10.17 1.00     5003     4432\nMarriage_s     0.63      0.25     0.15     1.11 1.00     5641     4331\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.75      0.19     1.43     2.17 1.00     5148     3934\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\n\nI'll skip plotting this for now\n\n### Multiple regression notation {-}\n\nModel with\n\n`mu <- a + bM*M + bA*A`\n\n> Notice how bA doesn’t move, only grows a bit more uncertain, while bM is only associated with divorce when age at marriage is missing from the model. You can interpret these distributions as saying:\n\n> Once we know median age at marriage for a State, there is little or no additional predictive power in also knowing the rate of marriage in that State.\n> In that weird notation, $D \\perp M|A$. This tests the implication of the second DAG from earlier.\n\n> Since the first DAG did not imply this result, it is out\n\nDR: This seems like a very weak test and a poor strategy for making inferences about causality (more so *running* with those inferences to underly future modeling).  Even when age *is* present in the model, the compatibility intervals for the marriage rate coefficient include rather large coefficients in either direction.\n\n\n### Fitting the (multivariate) model {-}\n\n'Priors for each slope'^[\"Notice we’re using the same prior prior(normal(0, 1), class = b) for both predictors. Within the brms framework, they are both of class = b. But if we wanted their priors to differ, we’d make two prior() statements and differentiate them with the coef argument.\"]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb5.3 <- \n  brm(data = d, \n      family = gaussian,\n      Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s,\n      prior = c(prior(normal(10, 10), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(uniform(0, 10), class = sigma)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5,\n      file = \"fits/b05.03\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(b5.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s \n   Data: d (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept               9.69      0.22     9.27    10.13 1.00     5542     4118\nMarriage_s             -0.19      0.30    -0.78     0.40 1.00     4042     4202\nMedianAgeMarriage_s    -1.23      0.31    -1.82    -0.62 1.00     4021     4081\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.53      0.16     1.25     1.88 1.00     5026     3391\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\n `mcmc_plot(b5.3)` seems to produce a ggplot object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(\nxxx <- mcmc_plot(b5.3) + ggtitle(\"My fancy bayesplot-based coefficient plot\") +\n  theme_bw() +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank()\n  )\n)\n```\n\n::: {.cell-output-display}\n![](5_spurious_waffles_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\nWith the `bayesplot::mcmc_intervals` function you can specify what you want more precisely. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"bayesplot\", dependencies = T)\nlibrary(bayesplot)\n\npost <- posterior_samples(b5.3)\n\ncolor_scheme_set(\"red\")\nmcmc_intervals(post[, 1:4], \n               prob = .7,\n  prob_outer = 0.99,\n               point_est = \"mean\") + #Kurz and the default use 'median'\n  ggtitle(\"My fancy bayesplot-based coefficient plot\") +\n  theme_bw() +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![](5_spurious_waffles_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nNote; this approach it required you to work with the `posterior_samples()` instead of the brmsfit object. Just to be different, I set `point_est = \"mean\"` instead of median, and `prob = .7` for the 'inner interval' and `prob_outer = 0.99`.\n\n> The tidybaes::stat_pointinterval() function offers a third way, this time with a more ground-up ggplot2 workflow.\n\n... I will return to that later, perhaps\n\n\n### Plotting multivariate posteriors. {-}\n\n> (1)  Predictor residual plots. These plots show the outcome against residual predictor\nvalues. ...\n> (2) Posterior prediction plots. These show model-based predictions against raw data,\nor otherwise display the error in prediction. They are tools for checking fit and\nassessing predictions. ...\n> (3) Counterfactual plots. These show the impliTd predictions for imaginary experiments. These plots allow you to explore the causal implications of manipulating one or more variables.\n\n\n\n#### Predictor residual plots {-}\n\n> To get ready to make our residual plots, we'll predict `Marriage_s` with `MedianAgeMarriage_s`.\n\nDR: going for 'the surpriseing part of one explanatory variable once you already know another'. Somehow I find it strange that we're doing a Bayesian estimate to get this object too, rather than just doing a simple least-squares linear fit. In the context of the latter I recall some clean 'regression algebra' results such as the 'long and short regression' and 'omitted variable bias' formulae.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb5.4 <- \n  brm(data = d,\n      family = gaussian,\n      Marriage_s ~ 1 + MedianAgeMarriage_s,\n      prior = c(prior(normal(0, 10), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(uniform(0, 10), class = sigma)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5,\n      file = \"fits/b05.04\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(b5.4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Marriage_s ~ 1 + MedianAgeMarriage_s \n   Data: d (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              -0.00      0.10    -0.20     0.20 1.00     5731     4560\nMedianAgeMarriage_s    -0.71      0.10    -0.91    -0.51 1.00     5623     4288\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.72      0.08     0.59     0.89 1.00     5449     4270\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\n> With `fitted()`, we compute the expected values for each state (with the exception of Nevada). Since the `MedianAgeMarriage_s` values for each state are in the date we used to fit the model, we'll omit the `newdata` argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- \n  fitted(b5.4) %>%\n  as_tibble() %>%\n  bind_cols(d)\n\nhead(f)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 19\n  Estimate Est.E…¹     Q2.5  Q97.5 Locat…² Loc   Popul…³ Media…⁴ Marri…⁵ Marri…⁶\n     <dbl>   <dbl>    <dbl>  <dbl> <fct>   <fct>   <dbl>   <dbl>   <dbl>   <dbl>\n1    0.431   0.119  0.199    0.667 Alabama AL       4.78    25.3    20.2    1.27\n2    0.489   0.124  0.245    0.732 Alaska  AK       0.71    25.2    26      2.93\n3    0.144   0.104 -0.0560   0.350 Arizona AZ       6.33    25.8    20.3    0.98\n4    1.01    0.177  0.657    1.36  Arkans… AR       2.92    24.3    26.4    1.7 \n5   -0.430   0.117 -0.660   -0.197 Califo… CA      37.2     26.8    19.1    0.39\n6    0.202   0.106 -0.00519  0.409 Colora… CO       5.03    25.7    23.5    1.24\n# … with 9 more variables: Divorce <dbl>, Divorce.SE <dbl>, WaffleHouses <int>,\n#   South <int>, Slaves1860 <int>, Population1860 <int>, PropSlaves1860 <dbl>,\n#   MedianAgeMarriage_s <dbl>, Marriage_s <dbl>, and abbreviated variable names\n#   ¹​Est.Error, ²​Location, ³​Population, ⁴​MedianAgeMarriage, ⁵​Marriage,\n#   ⁶​Marriage.SE\n```\n:::\n:::\n\n\nAfter a little data processing, we can make Figure 5.3.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf %>% \n  ggplot(aes(x = MedianAgeMarriage_s, y = Marriage_s)) +\n  geom_point(size = 2, shape = 1, color = \"firebrick4\") +\n  geom_segment(aes(xend = MedianAgeMarriage_s, yend = Estimate), \n               size = 1/4) +\n  geom_line(aes(y = Estimate), \n            color = \"firebrick4\") +\n  coord_cartesian(ylim = range(d$Marriage_s)) +\n  theme_bw() +\n  theme(panel.grid = element_blank())     \n```\n\n::: {.cell-output-display}\n![](5_spurious_waffles_files/figure-html/unnamed-chunk-19-1.png){width=288}\n:::\n:::\n\n\n\n\n**Skipped a bunch here; come back to it**\n\n> The trick with simulating counterfactuals is to realize that when we manipulate some variable X, we break the causal influence of other variables on X.\n\n> Now we can use `sim`, which you met in the previous chapter, to simulate observations from model `m5.3_A`. But this time we’ll tell it to simulate both M and D, in that order. Why in that order? Because we have to simulate the influence of A on M before we simulate the joint influence of A and M on D. The vars argument to sim tells it both which observables to simulate and in which order.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# prep data 5.21\nsim_dat <- data.frame( A=A_seq )\n# simulate M and then D, using A_seq\ns <- sim( m5.3_A , data=sim_dat , vars=c(\"M\",\"D\") )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot( sim_dat$A , colMeans(s$D) , ylim=c(-2,2) , type=\"l\" ,\nxlab=\"manipulated A\" , ylab=\"counterfactual D\" )\nshade( apply(s$D,2,PI) , sim_dat$A )\nmtext( \"Total counterfactual effect of A on D\" )\n```\n:::\n",
    "supporting": [
      "5_spurious_waffles_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}