{
  "hash": "3de0098d6ccef76530dadabc7d5d1a5d",
  "result": {
    "markdown": "# Ch 2. Small/Large Worlds {#mcelreath_ch2}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pacman)\np_load(dplyr, magrittr, ggplot2, stringr, tidyr, install = FALSE)\n```\n:::\n\n\n\n\n## 2.1. Garden of Forking data\n\n> The small world is the self-contained logical world of the model.\n\n> The way that Bayesian models learn from evidence is arguably optimal in the small world. When their assumptions approximate reality, they also perform well in the large world. But large world performance has to be demonstrated rather than logically deduced.\n\n> This demonstrates that there are three (out of 64) ways for a bag containing \\[a combo of blue and white marbles\\] to produce the data. The inferential power comes from comparing this count to the numbers of ways each of the other conjectures of the bag's contents could produce the same data.\n\n> .... can be computed just by multiplying the new count by the old count. This updating approach amounts to nothing more than asserting that (1) when we have previous information suggesting there are $W_prior$ ways for a conjecture to produce a previous observation $D_{prior}$ and (2) we acquire new observations $D_{new}$ that the same conjecture can produce in $W_{new}$ ways, then (3) the number of ways the conjecture can account for both $D_{prior}$ as well as $D_{new}$ is just the product $W_{prior} \\times W_{new}$.\n\n> This is sometimes known as the principle of indifference: When there is no reason to say that one conjecture is more plausible than another, weigh all of the conjectures equally. This book does not use nor endorse \"ignorance\" priors. As we'll see in later chapters, the structure of the model and the scientific context always provide information that allows us to do better than ignorance.\n\n![](images/paste-748728A1.png){width=\"335\"}\n\n::: {.callout-note collapse=\"true\"}\n## Note: I'm skipping the construction of the 'forking paths' plot pasted above\n\nKurz laboriously calculates the values and constructs it using tibbles and ggplot. Not sure what the latter teaches us.\n:::\n\n### 2.1.2 Using prior information {.unnumbered}\n\nSome functions and data for tabulating 'how likely is the data we drew' (marbles we saw) given different bag compositions. We 'count the (equally likely) ways' below. This is the product of 'ways of drawing the first (blue) marble', the second 'white', and the third 'blue' marble.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Ways of producing data\"}\n# if we make two custom functions, here, it will simplify the code within `mutate()`, below\nn_blue <- function(x) {\n  rowSums(x == \"b\")\n}\n\nn_white <- function(x) {\n  rowSums(x == \"w\")\n}\n\nt <-\n  # for the first four columns, `p_` indexes position\n  tibble(p_1 = rep(c(\"w\", \"b\"), times = c(1, 4)),\n         p_2 = rep(c(\"w\", \"b\"), times = c(2, 3)),\n         p_3 = rep(c(\"w\", \"b\"), times = c(3, 2)),\n         p_4 = rep(c(\"w\", \"b\"), times = c(4, 1))) %>%\n  mutate(`draw 1: blue`  = n_blue(.),\n         `draw 2: white` = n_white(.),\n         `draw 3: blue`  = n_blue(.)) %>%\n  mutate(`ways to produce` = `draw 1: blue` * `draw 2: white` * `draw 3: blue`)\n\nt %>%  knitr::kable()\n```\n\n::: {.cell-output-display}\n|p_1 |p_2 |p_3 |p_4 | draw 1: blue| draw 2: white| draw 3: blue| ways to produce|\n|:---|:---|:---|:---|------------:|-------------:|------------:|---------------:|\n|w   |w   |w   |w   |            0|             4|            0|               0|\n|b   |w   |w   |w   |            1|             3|            1|               3|\n|b   |b   |w   |w   |            2|             2|            2|               8|\n|b   |b   |b   |w   |            3|             1|            3|               9|\n|b   |b   |b   |b   |            4|             0|            4|               0|\n:::\n:::\n\n\nNext we get more data: another blue marble (already shown above), and there is a 'shortcut':\n\n> You could start all over again, making a garden with four layers to trace out the paths compatible with the data sequence. Or you could take the previous counts---the prior counts---over conjectures (0, 3, 8, 9, 0) and just update them in light of the new observation. It turns out that these two methods are mathematically identical, as long as the new observation is logically independent of the previous observations\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Ways; add a marble\"}\nt <-\n  t %>%\n  rename(`previous counts` = `ways to produce`,\n         `ways to produce` = `draw 1: blue`) %>%\n  select(p_1:p_4, `ways to produce`, `previous counts`) %>%\n  mutate(`new count` = `ways to produce` * `previous counts`)\n```\n:::\n\n\nWe can also incorporate 'different data' -- e,g., certain amounts of 'prior counts of each bag in the factory' ... we simply multiply these in. 'You first draw a factory bag with a particular count ... which can occur XXX ways, and then you draw a blue marble from that bag, which can occur YYY ways, etc'.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"add factory counts\"}\nt <- t %>%\nselect(p_1:p_4, `new count`) %>%\nrename(`prior count` = `new count`) %>%\n  mutate(`factory count` = c(0, 3:0)) %>%\nmutate(`new count` = `prior count` * `factory count`)\n\nt %>%\nknitr::kable()\n```\n\n::: {.cell-output-display}\n|p_1 |p_2 |p_3 |p_4 | prior count| factory count| new count|\n|:---|:---|:---|:---|-----------:|-------------:|---------:|\n|w   |w   |w   |w   |           0|             0|         0|\n|b   |w   |w   |w   |           3|             3|         9|\n|b   |b   |w   |w   |          16|             2|        32|\n|b   |b   |b   |w   |          27|             1|        27|\n|b   |b   |b   |b   |           0|             0|         0|\n:::\n:::\n\n\n### 2.1.3. From counts to probability {.unnumbered}\n\n![](images/paste-D5765E47.png)\n\nPutting this all together to compute the plausibilities\n\n::: {.callout-note }\n\n*DR question:* is this with a flat prior ... starting with an equal probability of each bag type, or is 'plausibility' something different than the posterior?\n\n::: \n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"plausibilities\"}\nt %>%\n  select(p_1:p_4) %>%\n  mutate(p                      = seq(from = 0, to = 1, by = .25),\n         `ways to produce data` = c(0, 3, 8, 9, 0)) %>%\n  mutate(plausibility = `ways to produce data` / sum(`ways to produce data`))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 7\n  p_1   p_2   p_3   p_4       p `ways to produce data` plausibility\n  <chr> <chr> <chr> <chr> <dbl>                  <dbl>        <dbl>\n1 w     w     w     w      0                         0         0   \n2 b     w     w     w      0.25                      3         0.15\n3 b     b     w     w      0.5                       8         0.4 \n4 b     b     b     w      0.75                      9         0.45\n5 b     b     b     b      1                         0         0   \n```\n:::\n:::\n\n\n## 2.2. Building a model\n\n> Designing a simple Bayesian model benefits from a design loop with three steps. (1) Data story: Motivate the model by narrating how the data might arise. (2) Update: Educate your model by feeding it the data. (3) Evaluate: All statistical models require supervision, leading to model revision\n\n*DR question*: How can we 'revise the model' without overfitting or otherwise cheating in some way that overstates the confidence we should have in our results?\n\n> The maximum height of the curve increases with each sample, meaning that fewer values of $p$ amass more plausibility as the amount of evidence increases\n\n**Power of Bayesian inference in small-sample contexts**\n\n> Why? In non-Bayesian statistical inference, procedures are often justified by the method's behavior at very large sample sizes, so-called asymptotic behavior. As a result, performance at small samples sizes is questionable. In contrast, Bayesian estimates are valid for any sample size. This does not mean that more data isn't helpful---it certainly is. Rather, the estimates have a clear and valid interpretation, no matter the sample size. But the price for this power is dependency upon the initial plausibilities, the prior. If the prior is a bad one, then the resulting inference will be misleading.\n\n*DR note*: There are some frequentist/non-Bayesian procedures and tests that don't rely on large sample approximations; e.g., Fisher's exact test\n\n### 2.2.1 - the 'globe tossing' data story {.unnumbered}\n\n### 2.2.2 Bayesian updating {-}\n\nStart with a particular sequence of data, accumulate trials and successes\n\n::: {.cell}\n\n```{.r .cell-code}\n(d <-\n    tibble(\n      toss = c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\"),\n      n_trials = 1:9,\nn_success = cumsum(toss == \"w\")\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 × 3\n  toss  n_trials n_success\n  <chr>    <int>     <int>\n1 w            1         1\n2 l            2         1\n3 w            3         2\n4 w            4         3\n5 w            5         4\n6 l            6         4\n7 w            7         5\n8 l            8         5\n9 w            9         6\n```\n:::\n:::\n\n\nNext, we compute and plot the plausibility of every 'true share of water p' after observing the draws from the globe-tossing. We update this after each toss.\n\nOK I need to construct some data on this globe tossing first; I skipped this earlier\n\n\nNext we build the tibble plausibility/updating tibble.\n\nGoing through the coding steps for my own benefit... First we expand the tibble to consider each of 50 possible `p_water` values after each trial (toss).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsequence_length <- 50\n\n(\nplaus_globe_updates <-\nd %>%\n  expand(nesting(n_trials, toss, n_success),\n         p_water = seq(from = 0, to = 1, length.out = sequence_length))\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 450 × 4\n   n_trials toss  n_success p_water\n      <int> <chr>     <int>   <dbl>\n 1        1 w             1  0     \n 2        1 w             1  0.0204\n 3        1 w             1  0.0408\n 4        1 w             1  0.0612\n 5        1 w             1  0.0816\n 6        1 w             1  0.102 \n 7        1 w             1  0.122 \n 8        1 w             1  0.143 \n 9        1 w             1  0.163 \n10        1 w             1  0.184 \n# … with 440 more rows\n```\n:::\n:::\n\n\nNext we create the 'lagged' columns (for ease of plotting the updating):\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"updating globe tossing - lags\"}\nplaus_globe_updates %<>%\n  group_by(p_water) %>%\n # you can learn more about lagging here: https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/lag or here: https://dplyr.tidyverse.org/reference/lead-lag.html\n  mutate(lagged_n_trials  = lag(n_trials,  k = 1),\n         lagged_n_success = lag(n_success, k = 1)) %>%\n  ungroup()\n\nplaus_globe_updates[95:105,]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 11 × 6\n   n_trials toss  n_success p_water lagged_n_trials lagged_n_success\n      <int> <chr>     <int>   <dbl>           <int>            <int>\n 1        2 l             1  0.898                1                1\n 2        2 l             1  0.918                1                1\n 3        2 l             1  0.939                1                1\n 4        2 l             1  0.959                1                1\n 5        2 l             1  0.980                1                1\n 6        2 l             1  1                    1                1\n 7        3 w             2  0                    2                1\n 8        3 w             2  0.0204               2                1\n 9        3 w             2  0.0408               2                1\n10        3 w             2  0.0612               2                1\n11        3 w             2  0.0816               2                1\n```\n:::\n:::\n\n\nNext we start with a flat prior and, for each 'trial' ...\n\n- compute the likelihood of the data (sucesses and trials) given each probability of water, according to the *binomial probability function*.\n- both with the previous 'lagged' data and adding the new data point\n- normalize each of these by dividing by the likelihood of the data that has arisen,\n- this yields the 'plausbility' of each probability of water   . (I.e., the posterior?)\n\n::: {.cell}\n\n```{.r .cell-code}\nplaus_globe_updates %<>%\n  mutate(prior  = ifelse(n_trials == 1, 1, #DR: I adjusted this to =1 to avoid confusing it with a certainty that p=.5\n          dbinom(\n            x= lagged_n_success,\n            size = lagged_n_trials,\n            prob = p_water)),\n         likelihood = dbinom(x  = n_success,\n                             size = n_trials,\n                             prob = p_water),\n         strip      = str_c(\"n = \", n_trials)) %>%\n  # the next three lines allow us to normalize the prior and the likelihood,\n  # putting them both in a probability metric\n  group_by(n_trials) %>%\n  mutate(prior      = prior      / sum(prior),\n         likelihood = likelihood / sum(likelihood))\n```\n:::\n\n\n\nPlotting this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplaus_globe_updates %>% filter(n_trials==5) %>%\n  # plot!\n  ggplot(aes(x = p_water)) +\n  geom_line(aes(y = prior), linetype = 2) +\n  geom_line(aes(y = likelihood)) +\n  scale_x_continuous(\"proportion water\", breaks = c(0, .5, 1)) +\n  scale_y_continuous(\"plausibility\", breaks = NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~strip, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](2_small_large_worlds_files/figure-html/unnamed-chunk-10-1.png){width=576}\n:::\n:::\n\n\n^[DR note: although the plot looks smooth, I think this is only because we chose a small interval of probability ... 50 different probabilities. When I reduce it to 5 probabilities this gets choppy]\n\n\n## 2.3. Components of the model\n\n::: {.callout-note collapse=\"true\"}\n## Coding tip: `dbinom`, `pbinom`, `rbinom`, etc\n\n> \"d\" in `dbinom` stands for density. Functions named in this way almost always have corresponding partners that begin with \"r\" for random samples and that begin with \"-p\" for cumulative probabilities\n:::\n\n> The distributions we assign to the observed variables typically have their own variables.\n\n### What prior? {.unnumbered}\n\n> So where do priors come from? They are both engineering assumptions, chosen to help the machine learn, and scientific assumptions, chosen to reflect what we know about a phenomenon. The flat prior in Figure 2.5 is very common, but it is hardly ever the best prior.\n\n> There is a school of Bayesian inference that emphasizes choosing priors based upon the personal beliefs of the analyst. While this subjective Bayesian approach thrives in some statistics and philosophy and economics programs, it is rare in the sciences.\n\n> If your goal is to lie with statistics, you'd be a fool to do it with priors, because such a lie would be easily uncovered. Better to use the more opaque machinery of the likelihood. Or better yet-don't actually take this advice!---massage the data, drop some \"outliers,\" and otherwise engage in motivated data transformation\n\n> because non-Bayesian procedures need to make choices that Bayesian ones do not, such as choice of estimator or likelihood penalty.\n\n## 2.4 'Making the model go'\n\n### 2.4.1 Bayes theorem {.unnumbered}\n\nIn word form:\n\n> Posterior (probability of any given value of $p$) =\n\n(Probability of the data \\[given p\\] $\\times$ \\[Prior probability of p\\]) divided by the 'Average probability of the data'\n\nI.e., (in my own words) 'how likely is this data *and* the particular parameter p' divided by 'the probability of this data overall' (given any p, with the probability of each p following the prior)\n\n> 'average probability of the data'\n\n> Averaged over what? Averaged over the prior. It's job is just to standardize the posterior, to ensure it sums (integrates) to one. In mathematical form: Pr(W, L) = E\n\nProbability of one Water followed by one Land:\n\n$$Pr(W, L|p)$$ $$=E\\Big( Pr(W, L|p) Big)$$\n\n$$= \\int  Pr(W,L|p) Pr(p)dp $$\n\n> The key lesson is that the posterior is proportional to the product of the prior and the probability of the data \\[given the prior\\]. Why? Because for each specific value of p, the number of paths through the garden of forking data is the product of the prior number of paths and the new number of paths. A flat prior constructs a posterior that is simply proportional to the likelihood\n\n::: {.callout-note collapse=\"true\"}\n## \"Bayesian data analysis isn't about Bayes' theorem\"\n\nDissing the 'HIV test false positive' thing\n\n> Inference under any probability concept will eventually make use of Bayes' theorem. Common introductory examples of 'Bayesian' analysis using HIV and DNA testing are not uniquely Bayesian\n:::\n\n> Numerical techniques for computing posterior distributions: (1) Grid approximation (2) Quadratic approximation (3) Markov chain Monte Carlo (MCMC)\n\n> Grid approximation: Basically mechanical Bayesian updating of the probability of a parameter value being in a range, dividing up the space of possible parameters into different ranges. (And then smoothing?)\n\n> ... achieve an excellent approximation of the continuous posterior distribution by considering only a finite grid of parameter values\n\n> in most of your real modeling, grid approximation isn't practical. The reason is that it scales very poorly, as the number of parameters increases\n\n> Under quite general conditions, the region near the peak of the posterior distribution will be nearly Gaussian-or \"normal\"---in shape. This means the posterior distribution can be usefully approximated by a Gaussian distribution. A Gaussian distribution is convenient, because it can be completely described by only two numbers: the location of its center (mean) and its spread (variance) 】\n\n> logarithm of a Gaussian distribution forms a parabola. And a parabola is a quadratic function\n\n> For many of the most common procedures in applied statistics-linear regression, for example---the approximation works very well\n\n> (1) Find the posterior mode\n\nsome optimization algorithm, a procedure that virtually \"climbs\" the posterior distribution\n\n> (2) estimate the curvature near the peak. This curvature is sufficient to compute a quadratic approximation of the entire posterior distribution. In some cases, these calculations can be done analytically, but 】\n\n*DR question:* But how does it do this estimate of the curvature? Does it come out of many simulations, or the optimizing hill climbing thing, or??\n",
    "supporting": [
      "2_small_large_worlds_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}