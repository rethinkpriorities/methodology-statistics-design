{
  "hash": "f795a7f456aa66e073c4d2280e82e23b",
  "result": {
    "markdown": "# Monte-Carlo 'Fermi Estimation' Approaches {-#fermi}\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pacman)\np_load(rethinkpriorities)\nlibrary(rethinkpriorities)\nlibrary(dplyr)\nlibrary(tibble)\n```\n:::\n\n\n'BOTEC': Back of the envelope calculations are central to RP's work\n\n'Fermi estimation' is essentially a more formal approach to this, carefully defining and explaining each element of the 'model' equation.\n\nWhen we explicitly define (and justify) a probability distribution over each variable in the model, and compute (often through simulation) the overall uncertainty of the outputs (predictions, estimates, etc), we call this \"Monte Carlo Fermi Estimation\".^[At least David Reinstein thinks this is what it's called.]\n\n\n## 'How to measure anything', By Douglas Hubbard\n\nSee\n\n- [book website here](https://www.howtomeasureanything.com/3rd-edition/) \n- [RP book group notes and links](https://docs.google.com/document/d/1JMAuIiE52o0E0SxeItD9vH_lWRifWHEETIvjjTiB_Gk/edit#heading=h.16rulnin7e3i) (private access)\n- [Luke M's summary at Lesswrong](https://www.lesswrong.com/posts/ybYBCK9D7MZCcdArB/how-to-measure-anything)\n\n\nz\n\n\n#### Chapter 3: The Urn of Mystery Simulation\t{-}\n\n\n::: {.callout-note collapse=\"true\"}\n\n## Chapter 3: The Urn of Mystery Simulation\t\t\t\n\n\n**The point**\n\n> The Single Sample Majority Rule (i.e., The Urn of Mystery Rule): Given maximum uncertainty about a population proportion – such that you believe the proportion could be anything between 0% and 100% with all values being equally likely – there is a 75% chance that a single randomly selected sample [i.e., one ball drawn from the urn] is from the majority of the population\n\nDiscussion of exercise\n\n> This is a simulation that represents the Urn of Mystery example mentioned in Chapter 3, pages 44-46 from the 3rd edition of the book.  This is a more economical way of testing the Urn of Mystery example than having a warehouse full of thousands of urns filled with green and red marbles.  Consider that there are 1000 urns in the simulated warehouse, each with 0% to 100% green marbles.  The percentage of green marbles are generated separately for each urn using a uniform distribution (the maximum possible uncertainty in this case).  A marble is drawn at random from each urn.  The probability of drawing a green marble is, obviously, just the percentage of marbles that are green.  So, for each urn, the color of the randomly drawn marble is determined with a binary distribution using the percentage of green marbles as the chance of drawing green.  Otherwise, red is drawn.  In this simulation, we pretend the drawing person does not see the real percentage of green marbles in the urn.  The person only uses the drawn marble to determine whether to bet the majority is green or red.  We then determine whether that single draw turned out to be the majority color.  We can see that after 1000 urns the single draw is the same color as the majority about 75% of the time.\t\t\t\t\n\n::: \n\nThe above (folded) narrative is rather confusing, and the spreadsheet is rather bulky. We can explain it and simulate it much more simply by using code.  The point is... [what was the point again?]\n\n\n1. Randomly draw a 'share green' for each urn, for each of 1000 (or '$K=1000$') 'urns', and record the majority color, i.e.,  *is it more than half green?*\n\nWe set the value 'K=1000', which we could adjust later, indicating 'how many urns' we are using in our simulation. It makes it clearer to define things at the top and see how it drives the results. \n\n::: {.cell}\n\n```{.r .cell-code}\nK <- 1000\n\nurns <- runif(K, 0, 1)\n```\n:::\n\nThe code above yields the object `urns`, a vector of 1000 probabilities. It assigns 'urns' to be equal to the function `runif`, i.e., 'random uniform'.\n\n^[This uses arguments K draws, `0` lower bound, and `1` upper bound.]\n\n\n^[See the tutorial [HERE](https://bookdown.org/ndphillips/YaRrr/generating-random-data.html#uniform) for some tips on 'generating random data' in R.]\nWe could view the whole thing in several ways, such as by typing `view(urns)` or `View(urns)` for a peek. We can also have any part of this printed to the screen.  The point is that this object is there in the background (in our 'environment'). We don't need to see it in front of us, at all times, as with a spreadsheet. \n\nPrinting out a peek at this object:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(urns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n num [1:1000] 0.423 0.455 0.41 0.613 0.44 ...\n```\n:::\n:::\n\n\nThe object is a 'numeric vector' with 1000 elements, the first 5 or so are listed. Each of these represent\n\n2. For each of ($K= 1000$) urns, randomly draw a single marble, and record whether this draw is the same as the urn's majority color.^[Well, we don't actually draw a marble, we simply make another uniform draw, and if it's below the 'share green' for that urn, we call it green.] \n\nWe put this together into a 'tibble' data frame below^[I could reuse the above vector, but I'll just re-create it instead] \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nK <- 1000\n\nurns <- tibble(\n    share_green = runif(K, 0, 1), #recreating the vector of uniform draws as the first column of a tibble\n    majority = if_else(share_green > .5, \"green\", \"red\"), #classifying whether the tibble is majority green or red\n    draw = if_else(runif(K) > share_green, \"red\", \"green\") #if another random uniform draw exceeds the 'share green' in that urn, it's a draw of a red ball. Note this is doing this for the entire vector runif(K) atonce\n  ) \n```\n:::\n\n\n\n3. Display this 'matrix' of 1000 outcomes (or a peek at it)\n\n::: {.cell}\n\n```{.r .cell-code}\nurns\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,000 × 3\n   share_green majority draw \n         <dbl> <chr>    <chr>\n 1      0.335  red      red  \n 2      0.420  red      green\n 3      0.0116 red      red  \n 4      0.681  green    green\n 5      0.0729 red      red  \n 6      0.445  red      green\n 7      0.607  green    red  \n 8      0.398  red      red  \n 9      0.433  red      red  \n10      0.548  green    green\n# … with 990 more rows\n```\n:::\n:::\n\n\nThe snip above shows how these 'random draws' are generated, as noted above. \n\n\\\n\n4. Count 'which share of these agree with their urn's majority color'\n\n::: {.cell}\n\n```{.r .cell-code}\n(\n  share_agree <- summarize(urns, share_agree = sum(draw == majority) / K)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  share_agree\n        <dbl>\n1       0.763\n```\n:::\n\n```{.r .cell-code}\n#sum number of cases where the 'draw is the same color as the majority for the urn\n```\n:::\n\nI.e., 76.3% of the draws are the same color as the majority of their urn.^[Check out the raw code -- note how the number here is automatically generated with 'inline code'. I didn't literally type in that number.]\n\n\nThe above code is not as elegant as it should be, and we should clean it up. Still I think this is better than using the Excel spreadsheet. Why? It gives you more control, a better record of what you have done, the ability to do more powerful analysis, and you can do more with the results (such as embedding them into a dynamic document like this one).\n\nFor one example, suppose you wanted to test this with 1 *million* urns. That would be a huge pain to do in Excel... I dare you to try it.  In `R` we simply change the code to specify $K =1,000,000$ urns, and do the above again.^[Even better, rather than recopying the code, we would make it a function.]\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nK <- 1000*1000\n\n\nurns <- tibble(\n    share_green = runif(K, 0, 1),\n    majority = if_else(share_green > .5, \"green\", \"red\"), \n    draw = if_else(runif(K) > share_green, \"red\", \"green\") \n  ) \n\n(\n  share_agree <- sum(urns$draw == urns$majority)/K\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.75072\n```\n:::\n:::\n\nOn my computer, this ran almost immediately.\n\n\n### Tools for Monte-Carlo Fermi, other than Excel\n\nSome links to simple vignettes in other tools\n\n- [Causal, buy vs rent](https://my.causal.app/models/69949)\n\n\n- [Guesstimate: simplest trivial example](https://www.getguesstimate.com/models/3394)\n- [Guesstimate, slightly more involved: deep work before death of your mind](https://www.getguesstimate.com/models/2758)\n\n- [Squiggle notebook, GiveDirectly/Givewell mode](https://observablehq.com/@hazelfire/givewells-givedirectly-cost-effectiveness-analysis)\n\n- R code (working on it)\n\n\n## Monte-Carlo Fermi approaches to GiveWell-style Cost-Effectiveness Analysis\n\nEmbedded below, David Reinstein and Sam Nolan explain this approach, advocating its use in GiveWell models and beyond, laying out some building blocks,\n\n... and embed some tools and work-in-progress on this [HERE](https://effective-giving-marketing.gitbook.io/innovations-in-givewell-esque-ceas/),  also embedded below.\n\n\n**Overview**\n\n- The basic ideas\n- Causal and Guesstimate\n- Code-based tools\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::include_url(\"https://effective-giving-marketing.gitbook.io/innovations-in-givewell-esque-ceas/\")\n```\n\n<iframe src=\"https://effective-giving-marketing.gitbook.io/innovations-in-givewell-esque-ceas/\" width=\"672\" height=\"400px\" data-external=\"1\"></iframe>\n:::\n\n::: {.alert .alert-secondary}\n\nDR: We may want to look for ways to explicitly incorporate and integrate these approaches into our data analysis work in R, etc.\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}