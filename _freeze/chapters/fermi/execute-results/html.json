{
  "hash": "228ea1561e3388d711a4bdc1713c77e1",
  "result": {
    "markdown": "# Monte-Carlo 'Fermi Estimation' Approaches {-#fermi}\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pacman)\np_load(rethinkpriorities)\nlibrary(rethinkpriorities)\nlibrary(dplyr)\nlibrary(tibble)\n```\n:::\n\n\n'BOTEC': Back of the envelope calculations are central to RP's work\n\n'Fermi estimation' is essentially a more formal approach to this, carefully defining and explaining each element of the 'model' equation.\n\nWhen we explicitly define (and justify) a probability distribution over each variable in the model, and compute (often through simulation) the overall uncertainty of the outputs (predictions, estimates, etc), we call this \"Monte Carlo Fermi Estimation\".^[At least David Reinstein thinks this is what it's called.]\n\n\n## 'How to measure anything', By Douglas Hubbard\n\nSee\n\n- [book website here](https://www.howtomeasureanything.com/3rd-edition/) \n- [RP book group notes and links](https://docs.google.com/document/d/1JMAuIiE52o0E0SxeItD9vH_lWRifWHEETIvjjTiB_Gk/edit#heading=h.16rulnin7e3i) (private access)\n- [Luke M's summary at Lesswrong](https://www.lesswrong.com/posts/ybYBCK9D7MZCcdArB/how-to-measure-anything)\n\n\n### Accompanying tools and exercises {-}\n\n> DR: Hubbard provides a set of Excel spreadsheets to go through his examples and illustrations, but we have better tools than this. I hope to embed or link a set of these below.\n\n\n#### Chapter 3: The Urn of Mystery Simulation\t{-}\n\n\n::: {.callout-note collapse=\"true\"}\n\n## Chapter 3: The Urn of Mystery Simulation\t\t\t\n\n> This is a simulation that represents the Urn of Mystery example mentioned in Chapter 3, pages 44-46 from the 3rd edition of the book.  This is a more economical way of testing the Urn of Mystery example than having a warehouse full of thousands of urns filled with green and red marbles.  Consider that there are 1000 urns in the simulated warehouse, each with 0% to 100% green marbles.  The percentage of green marbles are generated separately for each urn using a uniform distribution (the maximum possible uncertainty in this case).  A marble is drawn at random from each urn.  The probability of drawing a green marble is, obviously, just the percentage of marbles that are green.  So, for each urn, the color of the randomly drawn marble is determined with a binary distribution using the percentage of green marbles as the chance of drawing green.  Otherwise, red is drawn.  In this simulation, we pretend the drawing person does not see the real percentage of green marbles in the urn.  The person only uses the drawn marble to determine whether to bet the majority is green or red.  We then determine whether that single draw turned out to be the majority color.  We can see that after 1000 urns the single draw is the same color as the majority about 75% of the time.\t\t\t\t\n\n::: \n\nThe above (folded) narrative is rather confusing, and the spreadsheet is rather bulky. We can explain it and simulate it much more simply by using code.  The point is... [what was the point again?]\n\n\n1. Randomly draw a 'share green' for each urn, for each of 1000 (or '$K=1000$') 'urns', and record the majority color, i.e.,  *is it more than half green?*\n\nWe set the value 'K=1000', which we could adjust later, indicating 'how many urns' we are using in our simulation. It makes it clearer to define things at the top and see how it drives the results. \n\n::: {.cell}\n\n```{.r .cell-code}\nK <- 1000\n\nurns <- runif(K, 0, 1)\n```\n:::\n\nThe code above yields the object `urns`, a vector of 1000 probabilities. It assigns 'urns' to be equal to the function `runif`, i.e., 'random uniform'.\n\n^[This uses arguments K draws, `0` lower bound, and `1` upper bound.]\n\n\n^[See the tutorial [HERE](https://bookdown.org/ndphillips/YaRrr/generating-random-data.html#uniform) for some tips on 'generating random data' in R.]\nWe could view the whole thing in several ways, such as by typing `view(urns)` or `View(urns)` for a peek. We can also have any part of this printed to the screen.  The point is that this object is there in the background (in our 'environment'). We don't need to see it in front of us, at all times, as with a spreadsheet. \n\nPrinting out a peek at this object:\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(urns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n num [1:1000] 0.978 0.82 0.982 0.188 0.706 ...\n```\n:::\n:::\n\n\nThe object is a 'numeric vector' with 1000 elements, the first 5 or so are listed. Each of these represent\n\n2. For each of ($K= 1000$) urns, randomly draw a single marble, and record whether this draw is the same as the urn's majority color.^[Well, we don't actually draw a marble, we simply make another uniform draw, and if it's below the 'share green' for that urn, we call it green.] \n\nWe put this together into a 'tibble' data frame below\n\n::: {.cell}\n\n```{.r .cell-code}\nurns <- urns %>%  #start with the vector of urns generated above\n  as.tibble() %>% #make it a tibble, a type of data frame\n  rename(share_green = value) %>% #by default the runif function  called this 'value'; we rename it \n  add_column(runif = runif(nrow(.))) %>%  #add a column of random uniform 0,1 draws, to use as our 'random draw' for each urn\n  mutate(\n    majority = if_else(share_green>.5, \"green\", \"red\"), #if most of an urn is green or red\n    draw = case_when(\n      runif > share_green ~ \"red\", #if the random draw exceeds the 'share green' in that urn, it's a draw of a red ball\n      TRUE ~ \"green\" #otherwise it's a draw of a green ball\n    )\n        )\n```\n:::\n\n3. Display this 'matrix' of 1000 outcomes (or a peek at it)\n\n::: {.cell}\n\n```{.r .cell-code}\nurns\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,000 × 4\n   share_green  runif majority draw \n         <dbl>  <dbl> <chr>    <chr>\n 1      0.978  0.314  green    green\n 2      0.820  0.0878 green    green\n 3      0.982  0.747  green    green\n 4      0.188  0.228  red      red  \n 5      0.706  0.848  green    red  \n 6      0.332  0.245  red      green\n 7      0.0976 0.302  red      red  \n 8      0.345  0.449  red      red  \n 9      0.464  0.0951 red      green\n10      0.879  0.212  green    green\n# … with 990 more rows\n```\n:::\n:::\n\n\nThe snip above shows how these 'random draws' are generated, as noted above. \n\n\\\n\n4. Count 'which share of these agree with their urn's majority color'\n\n::: {.cell}\n\n```{.r .cell-code}\n(\n  share_agree <- sum(urns$draw == urns$majority)/K\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.73\n```\n:::\n\n```{.r .cell-code}\n#sum number of cases where the 'draw is the same color as the majority for the urn\n```\n:::\n\nI.e., 73% of the draws are the same color as the majority of their urn.^[Check out the raw code -- note how the number here is automatically generated with 'inline code'. I didn't literally type in that number.]\n\n\nThe above code is not as elegant as it should be, and we should clean it up. Still I think this is better than using the Excel spreadsheet. Why? It gives you more control, a better record of what you have done, the ability to do more powerful analysis, and you can do more with the results (such as embedding them into a dynamic document like this one).\n\nFor one example, suppose you wanted to test this with 1 *million* urns. That would be a huge pain to do in Excel... I dare you to try it.  In `R` we simply change the code to specify $K =1,000,000$ urns, and do the above again.^[Even better, rather than recopying the code, we would make it a function.]\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nK <- 1000*1000\n\nurns <- runif(K, 0, 1)\n\nurns <- urns %>%  \n  as.tibble() %>%\n  rename(share_green = value) %>% \n  add_column(runif = runif(nrow(.))) %>% \n  mutate(\n    majority = if_else(share_green>.5, \"green\", \"red\"),\n    draw = case_when(\n      runif > share_green ~ \"red\",\n      TRUE ~ \"green\"\n    )\n        )\n\n(\n  share_agree <- sum(urns$draw == urns$majority)/K\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.74948\n```\n:::\n:::\n\nOn my computer, this ran almost immediately.\n\n\n## Monte-Carlo Fermi approaches to GiveWell-style Cost-Effectiveness Analysis\n\nEmbedded below, David Reinstein and Sam Nolan explain this approach, advocating its use in GiveWell models and beyond, laying out some building blocks,\n\n... and embed some tools and work-in-progress on this [HERE](https://effective-giving-marketing.gitbook.io/innovations-in-givewell-esque-ceas/),  also embedded below.\n\n\n**Overview**\n\n- The basic ideas\n- Causal and Guesstimate\n- Code-based tools\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::include_url(\"https://effective-giving-marketing.gitbook.io/innovations-in-givewell-esque-ceas/\")\n```\n\n<iframe src=\"https://effective-giving-marketing.gitbook.io/innovations-in-givewell-esque-ceas/\" width=\"672\" height=\"400px\" data-external=\"1\"></iframe>\n:::\n\n::: {.alert .alert-secondary}\n\nDR: We may want to look for ways to explicitly incorporate and integrate these approaches into our data analysis work in R, etc.\n\n:::\n",
    "supporting": [
      "fermi_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}