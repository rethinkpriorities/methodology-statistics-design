<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.113">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>RP: Methods - 23&nbsp; Chapter 4. Geocentric (linear) Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: 1;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/5_spurious_waffles.html" rel="next">
<link href="../chapters/3_sampling_the_imaginary.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><script async="" src="https://hypothes.is/embed.js"></script><script src="../site_libs/kePrint-0.0.1/kePrint.js"></script><link href="../site_libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"><div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">
<span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Chapter 4. Geocentric (linear) Models</span>
</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">RP: Methods</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/rethinkpriorities/methodology-statistics-design" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle sidebar-tool" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Intro., protocols, style</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction/overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/coding_data.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Coding, data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/presentation_method_discussion.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Presentation/visualisation</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Surveys/trial design</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/survey_designs_methods.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Survey design, item dev.</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/experiments_trials_design.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Experiments: Qualitative design, implementation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/qualitative-design-issues_plus.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Practical issues</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/exp_design_quant.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Quant. issues/VOI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/power_analysis_framework_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Power analysis &amp; workflow</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Statistics/modeling</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/basic_stats.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Frameworks, ‘models’, inference, tests</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/factor_analysis.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Factor analss: EFA/CFA, PCA</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/mixed_multilevel_random.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Random/mixed fx, mlm</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/ml_modeling.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Prediction, &amp; ML</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/time_series_application.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Time Series (applied)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/classification_model_notes.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Classification models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/causal_inf.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Causal Inference</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Other, worked examples</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/fermi.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">MonteCarlo ‘Fermi est.’</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../from_ea_market_testing/binary_trial_computations_redacted_ed.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Inference/equivalence tests, (binomial)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/other_sections.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Other suggested sections</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">Bayes/Rethinking</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/rethinking_bayes_recoding.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Overview (McE/Bayes)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/1_golem.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Ch 1. Golem of Prague</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/2_small_large_worlds.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Ch 2. Small/Large Worlds</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/3_sampling_the_imaginary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Ch 3. Sampling the Imaginary (posterior)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/4_geocentric_linear_models.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Chapter 4. Geocentric (linear) Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/5_spurious_waffles.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Ch 5. The Many Variables &amp; The Spurious Waffles</span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><!-- margin-sidebar --><div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#why-normal-distributions-are-normal" id="toc-why-normal-distributions-are-normal" class="nav-link active" data-scroll-target="#why-normal-distributions-are-normal"><span class="toc-section-number">23.1</span>  Why normal distributions are normal</a>
  <ul class="collapse">
<li><a href="#normal-by-addition." id="toc-normal-by-addition." class="nav-link" data-scroll-target="#normal-by-addition.">Normal by addition.</a></li>
  <li><a href="#normal-by-multiplication" id="toc-normal-by-multiplication" class="nav-link" data-scroll-target="#normal-by-multiplication">Normal by multiplication</a></li>
  <li><a href="#using-gaussian-distributions-but-why" id="toc-using-gaussian-distributions-but-why" class="nav-link" data-scroll-target="#using-gaussian-distributions-but-why">Using Gaussian distributions (“but why?”)</a></li>
  </ul>
</li>
  <li>
<a href="#a-language-for-describing-models-4.2" id="toc-a-language-for-describing-models-4.2" class="nav-link" data-scroll-target="#a-language-for-describing-models-4.2"><span class="toc-section-number">23.2</span>  A language for describing models (4.2)</a>
  <ul class="collapse">
<li><a href="#re-describing-the-globe-tossing-model-in-these-terms" id="toc-re-describing-the-globe-tossing-model-in-these-terms" class="nav-link" data-scroll-target="#re-describing-the-globe-tossing-model-in-these-terms">Re-describing the globe tossing model (in these terms)</a></li>
  </ul>
</li>
  <li>
<a href="#a-gaussian-model-of-height-4.3" id="toc-a-gaussian-model-of-height-4.3" class="nav-link" data-scroll-target="#a-gaussian-model-of-height-4.3"><span class="toc-section-number">23.3</span>  A Gaussian model of height (4.3)</a>
  <ul class="collapse">
<li><a href="#the-data-height" id="toc-the-data-height" class="nav-link" data-scroll-target="#the-data-height">The data (height)</a></li>
  <li><a href="#the-model" id="toc-the-model" class="nav-link" data-scroll-target="#the-model">The model</a></li>
  <li><a href="#grid-approximation-of-the-posterior-distribution" id="toc-grid-approximation-of-the-posterior-distribution" class="nav-link" data-scroll-target="#grid-approximation-of-the-posterior-distribution">Grid approximation of the posterior distribution</a></li>
  <li><a href="#sampling-from-the-posterior-3.4" id="toc-sampling-from-the-posterior-3.4" class="nav-link" data-scroll-target="#sampling-from-the-posterior-3.4">Sampling from the posterior (3.4)</a></li>
  </ul>
</li>
  <li><a href="#linear-prediction-4.4" id="toc-linear-prediction-4.4" class="nav-link" data-scroll-target="#linear-prediction-4.4">Linear prediction (4.4)</a></li>
  <li><a href="#curves-from-lines-4.5" id="toc-curves-from-lines-4.5" class="nav-link" data-scroll-target="#curves-from-lines-4.5"><span class="toc-section-number">23.4</span>  Curves from lines (4.5)</a></li>
  </ul><div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/rethinkpriorities/methodology-statistics-design/edit/main/chapters/4_geocentric_linear_models.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/rethinkpriorities/methodology-statistics-design/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title d-none d-lg-block">
<span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Chapter 4. Geocentric (linear) Models</span>
</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header><div class="cell">

</div>
<blockquote class="blockquote">
<p>geocentric model continues to make useful predictions</p>
</blockquote>
<blockquote class="blockquote">
<p>Linear regression is the geocentric model of applied statistics</p>
</blockquote>
<p>introduces linear regression as a Bayesian procedure.</p>
<blockquote class="blockquote">
<p>Other common and useful distributions will be used to build generalized linear models (GLMs).</p>
</blockquote>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
DR question: GLMs?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Admit I am still not completely sure what these are. In Econometrics we talked about ‘linear in parameters’ models. I’ve dug into the technical definition of GLMs and it is rather obtuse!</p>
</div>
</div>
</div>
<p><img src="images/paste-C2ABCFA6.png" class="img-fluid"></p>
<section id="why-normal-distributions-are-normal" class="level2" data-number="23.1"><h2 data-number="23.1" class="anchored" data-anchor-id="why-normal-distributions-are-normal">
<span class="header-section-number">23.1</span> Why normal distributions are normal</h2>
<blockquote class="blockquote">
<p>Many natural (and unnatural) processes have much heavier tails … A real and important example is financial time series</p>
</blockquote>
<p>After laying out his soccer field coin toss shuffle premise, McElreath wrote:</p>
<blockquote class="blockquote">
<p>It’s hard to say where any individual person will end up, but you can say with great confidence what the collection of positions will be. The distances will be distributed in approximately normal, or Gaussian, fashion. This is true even though the underlying distribution is binomial. It does this because there are so many more possible ways to realize a sequence of left-right steps that sums to zero. There are slightly fewer ways to realize a sequence that ends up one step left or right of zero, and so on, with the number of possible sequences declining in the characteristic bell curve of the normal distribution. (p.&nbsp;72)</p>
</blockquote>
<section id="normal-by-addition." class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="normal-by-addition.">Normal by addition.</h3>
<div class="cell">
<details><summary>coding football steps</summary><div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># we set the seed to make the results of `runif()` reproducible.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">4</span><span class="op">)</span></span>
<span></span>
<span><span class="va">pos</span> <span class="op">&lt;-</span></span>
<span>  <span class="co"># make data with 100 people, 16 steps each with a starting point of `step == 0` (i.e., 17 rows per person)</span></span>
<span>  <span class="fu">crossing</span><span class="op">(</span>person <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">100</span>,</span>
<span>           step   <span class="op">=</span> <span class="fl">0</span><span class="op">:</span><span class="fl">16</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="co">#DR: 'crossing is a great shortcut to 'create all combinations''</span></span>
<span></span>
<span>  <span class="co"># for all steps above `step == 0` simulate a `deviation`</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>deviation <span class="op">=</span> <span class="fu">map_dbl</span><span class="op">(</span> <span class="co">#DR: `map_dbl` to make it a 'vector of numbers' rather than a list</span></span>
<span>    <span class="va">step</span>, <span class="co">#for all 16 step entries</span></span>
<span></span>
<span>    <span class="op">~</span><span class="fu">if_else</span><span class="op">(</span><span class="va">.</span> <span class="op">==</span> <span class="fl">0</span>, <span class="fl">0</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1</span>, <span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="co">#defines a function with '~', 0 for step 0, otherwise a uniform distribution step length each time</span></span>
<span></span>
<span>  <span class="co"># after grouping by `person`, compute the cumulative sum of the deviations, then `ungroup()`</span></span>
<span>  <span class="fu">group_by</span><span class="op">(</span><span class="va">person</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>position <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/cumsum.html">cumsum</a></span><span class="op">(</span><span class="va">deviation</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="co">#cumsum is great</span></span>
<span>  <span class="fu">ungroup</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details><summary>Code</summary><div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">glimpse</span><span class="op">(</span><span class="va">pos</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output cell-output-stdout">
<pre><code>Rows: 1,700
Columns: 4
$ person    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…
$ step      &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 0, 1, 2, 3, 4, 5, 6, 7…
$ deviation &lt;dbl&gt; 0.00000000, -0.98210841, -0.41252078, -0.44525008, 0.62714843, -0.47914446, 0.44…
$ position  &lt;dbl&gt; 0.0000000, -0.9821084, -1.3946292, -1.8398793, -1.2127308, -1.6918753, -1.243063…</code></pre>
</div>
<details><summary>Code</summary><div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">precis</span><span class="op">(</span><span class="va">pos</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output cell-output-stdout">
<pre><code>                 mean        sd       5.5%      94.5%     histogram
person    50.50000000 28.874564  6.0000000 95.0000000    ▇▇▇▇▇▇▇▇▇▇
step       8.00000000  4.900421  0.0000000 16.0000000      ▇▅▅▅▅▅▅▅
deviation -0.02345358  0.560043 -0.8949205  0.8721156    ▅▅▅▃▇▅▅▃▃▃
position  -0.16368313  1.609085 -2.6457540  2.3513754 ▁▁▁▁▂▃▇▅▃▁▁▁▁</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p>Here’s the actual plot code.</p>
</blockquote>
<div class="cell">
<details><summary>Code</summary><div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="op">(</span></span>
<span>  <span class="va">p1</span> <span class="op">&lt;-</span> <span class="fu">ggplot</span><span class="op">(</span>data <span class="op">=</span> <span class="va">pos</span>,</span>
<span>       <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">step</span>, y <span class="op">=</span> <span class="va">position</span>, group <span class="op">=</span> <span class="va">person</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="co">#'where am I (vertical) at each step (horizontal) ... not sure what 'group' does here</span></span>
<span>  <span class="fu">geom_vline</span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span>, <span class="fl">8</span>, <span class="fl">16</span><span class="op">)</span>, linetype <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="co">#add vertical lines for x intercepts at steps 4 8 and 16</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>color <span class="op">=</span> <span class="va">person</span> <span class="op">&lt;</span> <span class="fl">4</span>, alpha  <span class="op">=</span> <span class="va">person</span> <span class="op">&lt;</span> <span class="fl">4</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="co">#the main lines of interest</span></span>
<span>    <span class="co">#focusing on 4 specific cases</span></span>
<span>  <span class="fu">scale_color_manual</span><span class="op">(</span>values <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"skyblue4"</span>, <span class="st">"black"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_alpha_manual</span><span class="op">(</span>values <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">/</span><span class="fl">7</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_x_continuous</span><span class="op">(</span><span class="st">"step number"</span>, breaks <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">4</span>, <span class="fl">8</span>, <span class="fl">12</span>, <span class="fl">16</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="co"># ticks and labels on the bottom, light gridlines</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>legend.position <span class="op">=</span> <span class="st">"none"</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<p><img src="4_geocentric_linear_models_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="614"></p>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
DR question about above code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>precisely what does ‘group’ do in the above?</li>
</ul>
</div>
</div>
</div>
<p>Code for plotting all random walks on soccer field, steps, and densities</p>
<p>Plots at 4 and 8 steps:</p>
<div class="cell">
<details><summary>Code</summary><div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Figure 4.2.a.</span></span>
<span><span class="co"># Figure 4.2.a.</span></span>
<span><span class="va">p1</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">pos</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">step</span> <span class="op">==</span> <span class="fl">4</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">position</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span>stat <span class="op">=</span> <span class="st">"density"</span>, color <span class="op">=</span> <span class="st">"dodgerblue1"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">coord_cartesian</span><span class="op">(</span>xlim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">6</span>, <span class="fl">6</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"4 steps"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Figure 4.2.b.</span></span>
<span><span class="va">p2</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">pos</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">step</span> <span class="op">==</span> <span class="fl">8</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">position</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_density</span><span class="op">(</span>color <span class="op">=</span> <span class="st">"dodgerblue2"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">coord_cartesian</span><span class="op">(</span>xlim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">6</span>, <span class="fl">6</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"8 steps"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Get the SD at 16 steps for plotting the functional normal distribution to compare it to.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
DR question: Isn’t there an analytical formula we could use here for the SD instead?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">

</div>
</div>
</div>
<div class="cell">
<details><summary>Code</summary><div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># this is an intermediary step to get an SD value</span></span>
<span><span class="va">pos_sd</span> <span class="op">&lt;-</span> <span class="va">pos</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">step</span> <span class="op">==</span> <span class="fl">16</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">summarise</span><span class="op">(</span>sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">position</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Plot at 16 steps, overlay normal distribution (DR – I put in the SD as an object from above)</p>
<div class="cell">
<details><summary>Code</summary><div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Figure 4.2.c.</span></span>
<span></span>
<span><span class="co"># Figure 4.2.c.</span></span>
<span><span class="va">p3</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">pos</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">step</span> <span class="op">==</span> <span class="fl">16</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">position</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">stat_function</span><span class="op">(</span>fun <span class="op">=</span> <span class="va">dnorm</span>,</span>
<span>                args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="va">pos_sd</span><span class="op">[[</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span>,</span>
<span>                linetype <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span>  <span class="co"># 2.180408 came from the previous code block</span></span>
<span>  <span class="fu">geom_density</span><span class="op">(</span>color <span class="op">=</span> <span class="st">"transparent"</span>, fill <span class="op">=</span> <span class="st">"dodgerblue3"</span>, alpha <span class="op">=</span> <span class="fl">1</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">coord_cartesian</span><span class="op">(</span>xlim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">6</span>, <span class="fl">6</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"16 steps"</span>,</span>
<span>       y <span class="op">=</span> <span class="st">"density"</span><span class="op">)</span></span>
<span></span>
<span></span>
<span></span>
<span><span class="co"># combine the ggplots</span></span>
<span><span class="va">p1</span> <span class="op">|</span> <span class="va">p2</span> <span class="op">|</span> <span class="va">p3</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<p><img src="4_geocentric_linear_models_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
DR question (code): Better way to code ‘extract the sd as a number’ above?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">

</div>
</div>
</div>
<blockquote class="blockquote">
<p>While we were at it, we explored a few ways to express densities. The main action was with the <code>geom_line()</code>, <code>geom_density()</code>, and <code>stat_function()</code> functions, respectively.</p>
</blockquote>
<p>DR: <code>geom_line(stat = "density"...</code> might be the same as <code>geom_density</code>, <code>stat_function</code> is mainly for analytical densities?</p>
<section id="but-why" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="but-why">But why?</h4>
<blockquote class="blockquote">
<p>Any process that ads together random values from the same distribution converges to a normal. But it’s not easy to grasp why addition should result in a bell curve of sums. Here’s a conceptual way to think of the process. Whatever the average value of the source distribution, each sample from it can be thought of as a fluctuation from the average value. When we begin to add these fluctuations together, they also begin to cancel one another out. A large positive fluctuation will cancel a large negative one. The more terms in the sum, the more chances for each fluctuation to be canceled by another, or by a series of smaller ones in the opposite direction. So eventually the most likely sum, in the sense that there are the most ways to realize it, will be a sum in which every fluctuation is canceled by another, a sum of zero (relative to the mean). (pp.&nbsp;73–74)</p>
</blockquote>
</section></section><section id="normal-by-multiplication" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="normal-by-multiplication">Normal by multiplication</h3>
<blockquote class="blockquote">
<p>small effects that multiply together are approximately additive, and so they also tend to stabilize on Gaussian distributions</p>
</blockquote>
<blockquote class="blockquote">
<p>Large deviates that are multiplied together do not produce Gaussian distributions, but they do tend to produce Gaussian distributions on the log scale</p>
</blockquote>
<p><strong>Skipped coding this for now</strong></p>
</section><section id="using-gaussian-distributions-but-why" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="using-gaussian-distributions-but-why">Using Gaussian distributions (“but why?”)</h3>
<section id="ontological-justification." class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="ontological-justification.">Ontological justification.</h4>
<p>The Gaussian is</p>
<blockquote class="blockquote">
<p>a widespread pattern, appearing again and again at different scales and in different domains. Measurement errors, variations in growth, and the velocities of molecules all tend towards Gaussian distributions. These processes do this because at their heart, these processes add together fluctuations. And repeatedly adding finite fluctuations results in a distribution of sums that have shed all information about the underlying process, aside from mean and spread. One consequence of this is that statistical models based on Gaussian distributions cannot reliably identify micro-process. (p.&nbsp;75)</p>
</blockquote>
<p>~I.e., we cannot ‘prove what we assumed’.</p>
<blockquote class="blockquote">
<p>Kurz: But they can still be useful.</p>
</blockquote>
</section><section id="epistemological-justification." class="level4" data-number="23.1.0.1"><h4 data-number="23.1.0.1" class="anchored" data-anchor-id="epistemological-justification.">
<span class="header-section-number">23.1.0.1</span> Epistemological justification.</h4>
<blockquote class="blockquote">
<p>Another route to justifying the Gaussian as our choice of skeleton, and a route that will help us appreciate later why it is often a poor choice, is that it represents a particular state of ignorance. When all we know or are willing to say about a distribution of measures (measures are continuous values on the real number line) is their mean and variance, then the Gaussian distribution arises as the most consistent with our assumptions.</p>
<p>That is to say that the Gaussian distribution is the most natural expression of our state of ignorance, because if all we are willing to assume is that a measure has finite variance, the Gaussian distribution is the shape that can be realized in the largest number of ways and does not introduce any new assumptions. It is the least surprising and least informative assumption to make. In this way, the Gaussian is the distribution most consistent with our assumptions… If you don’t think the distribution should be Gaussian, then that implies that you know something else that you should tell your golem about, something that would improve inference. (pp.&nbsp;75–76)</p>
</blockquote>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
DR question – can we justify this in our own words
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>E.g., to a skeptical audience or ‘client’?</p>
</div>
</div>
</div>
<p>From McElreath:</p>
<blockquote class="blockquote">
<p>By the ontological justification, the world is full of Gaussian distributions, approximately.</p>
</blockquote>
<blockquote class="blockquote">
<p>By the epistemological justification, the Gaussian represents a particular state of ignorance. When all we know or are willing to say about a distribution of measures (measures are continuous values on the real number line) is their mean and variance, then the Gaussian distribution arises as the most consistent with our assumptions maximum entropy?</p>
</blockquote>
<blockquote class="blockquote">
<p>if all we are willing to assume is that a measure has finite variance, the Gaussian distribution is the shape that can be realized in the largest number of ways</p>
</blockquote>
</section><section id="overthinking-gaussian-distribution." class="level4" data-number="23.1.0.2"><h4 data-number="23.1.0.2" class="anchored" data-anchor-id="overthinking-gaussian-distribution.">
<span class="header-section-number">23.1.0.2</span> Overthinking: Gaussian distribution.</h4>
<p>(Kurz quotes below)</p>
<p>Let <span class="math inline">\(y\)</span> be the criterion (DR: why ‘criterion’?), <span class="math inline">\(\mu\)</span> be the mean, and <span class="math inline">\(\sigma\)</span> be the standard deviation. Then the probability density of some Gaussian value <span class="math inline">\(y\)</span> is</p>
<p><span class="math display">\[p(y|\mu, \sigma) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \Bigg (- \frac{(y - \mu)^2}{2 \sigma^2} \Bigg).\]</span></p>
<p>Why not demystify that monster with a little R code? For simplicity, we’ll look at <span class="math inline">\(p(y)\)</span> over a series of <span class="math inline">\(y\)</span> values ranging from -4 to 4, holding <span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\sigma = 1\)</span>. Then we’ll plot.</p>
<div class="cell">
<details><summary>Code</summary><div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># define our input values</span></span>
<span></span>
<span><span class="fu">tibble</span><span class="op">(</span>y     <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from <span class="op">=</span> <span class="op">-</span><span class="fl">4</span>, to <span class="op">=</span> <span class="fl">4</span>, by <span class="op">=</span> <span class="fl">.1</span><span class="op">)</span>,</span>
<span>       mu    <span class="op">=</span> <span class="fl">0</span>,</span>
<span>       sigma <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="co"># compute p(y) using a hand-made gaussian likelihood</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span></span>
<span>    p_y <span class="op">=</span> <span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">pi</span> <span class="op">*</span> <span class="va">sigma</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="va">mu</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">sigma</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span></span>
<span>  <span class="co"># plot!</span></span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">y</span>, y <span class="op">=</span> <span class="va">p_y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ylab</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu">italic</span><span class="op">(</span><span class="va">p</span><span class="op">)</span><span class="op">(</span><span class="fu">italic</span><span class="op">(</span><span class="st">"y|"</span><span class="op">)</span><span class="op">*</span><span class="va">mu</span><span class="op">==</span><span class="fl">0</span><span class="op">*</span><span class="st">","</span><span class="op">~</span><span class="va">sigma</span><span class="op">==</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<p><img src="4_geocentric_linear_models_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid" width="384"></p>
</div>
</div>
<blockquote class="blockquote">
<p>You get the same results if you switch out that mutate line with <code>mutate(p_y = dnorm(y)) %&gt;%</code>. To learn more, execute <code><a href="https://rdrr.io/r/stats/Normal.html">?dnorm</a></code>.</p>
</blockquote>
<p><em>DR: how does executing it demystify it?</em></p>
</section></section></section><section id="a-language-for-describing-models-4.2" class="level2" data-number="23.2"><h2 data-number="23.2" class="anchored" data-anchor-id="a-language-for-describing-models-4.2">
<span class="header-section-number">23.2</span> A language for describing models (4.2)</h2>
<p>For example:</p>
<span class="math display">\[\begin{align*}
\text{criterion}_i &amp; \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i  &amp; = \beta \times \text{predictor}_i \\
\beta  &amp; \sim \text{Normal}(0, 10) \\
\sigma &amp; \sim \text{HalfCauchy}(0, 1).
\end{align*}\]</span>
<blockquote class="blockquote">
<p>The combination of variables and their probability distributions defines a joint generative model that can be used both to simulate hypothetical observations as well as analyze real ones.</p>
</blockquote>
<p>DR: No ‘error term’ as in standard econometrics statement</p>
<blockquote class="blockquote">
<p>We no longer have to remember seemingly arbitrary lists of bizarre conditions like homoscedasticity (constant variance), because we can just read these conditions from the model definitions. We specify distributions rather than error terms and conditions</p>
</blockquote>
<p>DR: Note these specific distributions are ‘stronger assumptions’, which people may argue are harder to justify</p>
<section id="re-describing-the-globe-tossing-model-in-these-terms" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="re-describing-the-globe-tossing-model-in-these-terms">Re-describing the globe tossing model (in these terms)</h3>
<p>DR: We previously just described it in words iirc</p>
<p>For the globe tossing model, the probability <span class="math inline">\(p\)</span> of a count of water <span class="math inline">\(w\)</span> based on <span class="math inline">\(n\)</span> trials was</p>
<span class="math display">\[\begin{align*}
w &amp; \sim \text{Binomial}(n, p) \\
p &amp; \sim \text{Uniform}(0, 1).
\end{align*}\]</span>
<p><img src="images/paste-F25C2CF0.png" class="img-fluid"></p>
<p>‘probability p and data drawn’ / ‘probability data drawn’</p>
<p>Well, it’s what we <em>compute</em> in the grid approximation, for each probability.</p>
<p>Remember the denominator is the same for all values of p.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
From model description to Bayes theorem
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell">
<details><summary>Code</summary><div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># how many `p_grid` points would you like?</span></span>
<span><span class="va">n_points</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span></span>
<span><span class="va">d</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu">tibble</span><span class="op">(</span>p_grid <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from <span class="op">=</span> <span class="fl">0</span>, to <span class="op">=</span> <span class="fl">1</span>, length.out <span class="op">=</span> <span class="va">n_points</span><span class="op">)</span>,</span>
<span>         w      <span class="op">=</span> <span class="fl">6</span>,</span>
<span>         n      <span class="op">=</span> <span class="fl">9</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>prior      <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">dunif</a></span><span class="op">(</span><span class="va">p_grid</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>         likelihood <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">w</span>, <span class="va">n</span>, <span class="va">p_grid</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>posterior <span class="op">=</span> <span class="va">likelihood</span> <span class="op">*</span> <span class="va">prior</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">likelihood</span> <span class="op">*</span> <span class="va">prior</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">d</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output cell-output-stdout">
<pre><code># A tibble: 6 × 6
  p_grid     w     n prior likelihood posterior
   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;
1 0          6     9     1   0         0       
2 0.0101     6     9     1   8.65e-11  8.74e-12
3 0.0202     6     9     1   5.37e- 9  5.43e-10
4 0.0303     6     9     1   5.93e- 8  5.99e- 9
5 0.0404     6     9     1   3.23e- 7  3.26e- 8
6 0.0505     6     9     1   1.19e- 6  1.21e- 7</code></pre>
</div>
</div>
<p>In case you were curious, here’s what they look like.</p>
<div class="cell">
<details><summary>Code</summary><div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">d</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="op">-</span><span class="va">w</span>, <span class="op">-</span><span class="va">n</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">gather</span><span class="op">(</span><span class="va">key</span>, <span class="va">value</span>, <span class="op">-</span><span class="va">p_grid</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="co"># this line allows us to dictate the order the panels will appear in</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>key <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">key</span>, levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"prior"</span>, <span class="st">"likelihood"</span>, <span class="st">"posterior"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span></span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">p_grid</span>, ymin <span class="op">=</span> <span class="fl">0</span>, ymax <span class="op">=</span> <span class="va">value</span>, fill <span class="op">=</span> <span class="va">key</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_ribbon</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_fill_manual</span><span class="op">(</span>values <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"purple"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_y_continuous</span><span class="op">(</span><span class="cn">NULL</span>, breaks <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>legend.position <span class="op">=</span> <span class="st">"none"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">facet_wrap</span><span class="op">(</span><span class="op">~</span><span class="va">key</span>, scales <span class="op">=</span> <span class="st">"free"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<p><img src="4_geocentric_linear_models_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<blockquote class="blockquote">
<p>The posterior is a combination of the prior and the likelihood. When the prior is flat across the parameter space, the posterior is just the likelihood re-expressed as a probability. As we go along, you’ll see that we almost never use flat priors in practice.</p>
</blockquote>
</div>
</div>
</div>
</section></section><section id="a-gaussian-model-of-height-4.3" class="level2" data-number="23.3"><h2 data-number="23.3" class="anchored" data-anchor-id="a-gaussian-model-of-height-4.3">
<span class="header-section-number">23.3</span> A Gaussian model of height (4.3)</h2>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Willem Sleeger’s <a href="https://willemsleegers.com/content/posts/5-my-bayesian-workflow-and-tutorial/my-bayesian-workflow-and-tutorial.html">“Figuring out Bayesian statistics”</a> covers much of this same ground.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>His blog entry is better formatted and talks through some parts of the intuition more. You may find it more useful than the present notes. I incorporate some of it below (with acknowledgement)</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>single measurement variable to model as a Gaussian distribution. There will be two parameters describing the distribution’s shape, the mean - and the standard deviation</p>
</blockquote>
<blockquote class="blockquote">
<p>the ‘estimate’ here will be the entire posterior distribution, not any point within it</p>
</blockquote>
<blockquote class="blockquote">
<p>And as a result, the posterior distribution will be a distribution of Gaussian distributions. or of the parameters of these</p>
</blockquote>
<blockquote class="blockquote">
<p>There are an infinite number of possible Gaussian distributions. Some have small means. Others have large means. Some are wide, with a large <span class="math inline">\(\sigma\)</span>. Others are narrow. We want our Bayesian machine to consider every possible distribution, each defined by a combination of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, and rank them by posterior plausibility. (p.&nbsp;79)</p>
</blockquote>
<section id="the-data-height" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="the-data-height">The data (height)</h3>
<blockquote class="blockquote">
<p>Let’s get the Howell (<a href="https://www.routledge.com/Demography-of-the-Dobe-Kung/Howell/p/book/9780202306490">2000</a>, <a href="https://www.ucpress.edu/book/9780520262348/life-histories-of-the-dobe-kung">2010</a>) data from McElreath’s <a href="https://xcelab.net/rm/statistical-rethinking/">rethinking package</a>.</p>
</blockquote>
<div class="cell">
<details><summary>Code</summary><div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">rethinking</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">Howell1</span><span class="op">)</span></span>
<span><span class="va">d</span> <span class="op">&lt;-</span> <span class="va">Howell1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Code notes on brms
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>(Kurz) Here we open our main statistical package, Bürkner’s <a href="https://github.com/paul-buerkner/brms">brms</a>. But before we do, we’ll want to detach the rethinking package. R will not allow users to use a function from one package that shares the same name as a different function from another package if both packages are open at the same time. The rethinking and <code>brms</code> packages are designed for similar purposes and, unsurprisingly, overlap in the names of their functions. To prevent problems, it is a good idea to make sure rethinking is detached before using brms. To learn more on the topic, see <a href="https://www.r-bloggers.com/r-and-package-masking-a-real-life-example/">this R-bloggers post</a>.</p>
</blockquote>
</div>
</div>
</div>
<div class="cell">
<details><summary>Code</summary><div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/rm.html">rm</a></span><span class="op">(</span><span class="va">Howell1</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/detach.html">detach</a></span><span class="op">(</span><span class="va">package</span><span class="op">:</span><span class="va">rethinking</span>, unload <span class="op">=</span> <span class="cn">T</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/paul-buerkner/brms">brms</a></span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Go ahead and investigate the data with <code><a href="https://rdrr.io/r/utils/str.html">str()</a></code>, the tidyverse analogue for which is <code>glimpse()</code>.</p>
<div class="cell">
<details><summary>Code</summary><div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">d</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output cell-output-stdout">
<pre><code>'data.frame':   544 obs. of  4 variables:
 $ height: num  152 140 137 157 145 ...
 $ weight: num  47.8 36.5 31.9 53 41.3 ...
 $ age   : num  63 63 65 41 51 35 32 27 19 54 ...
 $ male  : int  1 0 0 1 0 1 0 1 0 1 ...</code></pre>
</div>
</div>
<p>Here are the <code>height</code> values</p>
<div class="cell">
<details><summary>Code</summary><div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">d</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="va">height</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output cell-output-stdout">
<pre><code>   height
1 151.765
2 139.700
3 136.525
4 156.845
5 145.415
6 163.830</code></pre>
</div>
</div>
<p>We can use <code><a href="https://rdrr.io/r/stats/filter.html">filter()</a></code> to make an adults-only data frame.</p>
<div class="cell">
<details><summary>Code</summary><div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">d2</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">d</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">age</span> <span class="op">&gt;=</span> <span class="fl">18</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>There are a lot of ways we can make sure our <code>d2</code> has 352 rows. Here’s one.</p>
<div class="cell">
<details><summary>Code</summary><div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">d2</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">count</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output cell-output-stdout">
<pre><code>    n
1 352</code></pre>
</div>
</div>
</section><section id="the-model" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="the-model">The model</h3>
<blockquote class="blockquote">
<p>as mentioned earlier in this chapter, the empirical distribution needn’t be actually Gaussian in order to justify using a Gaussian probability distribution why not?</p>
</blockquote>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
DR question: Why? is this because we are considering <em>means</em>?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">

</div>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
On iid as an ‘epistemological assumption’
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The i.i.d. assumption is about how the golem represents its uncertainty. It is an epistemological assumption. It is not a physical assumption about the world, an ontological one. E. T. Jaynes (1922-1998) called this the mind projection fallacy, the mistake of confusing epistemological claims with ontological claims.71 The point isnt that epistemology trumps reality, but that in ignorance of such correlations the best distribution may be i.i.d.72</p>
</div>
</div>
</div>
<p>The likelihood for our model is</p>
<p><span class="math display">\[h_i \sim \operatorname{Normal}(\mu, \sigma),\]</span></p>
<p>our <span class="math inline">\(\mu\)</span> prior will be</p>
<p><span class="math display">\[\mu \sim \operatorname{Normal}(178, 20),\]</span></p>
<p>and our prior for <span class="math inline">\(\sigma\)</span> will be</p>
<p><span class="math display">\[\sigma \sim \operatorname{Uniform}(0, 50).\]</span></p>
<p>Here’s the shape of the prior for <span class="math inline">\(\mu\)</span> in <span class="math inline">\(N(178, 20)\)</span>.</p>
<div class="cell">
<details><summary>Code</summary><div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">ggplot</span><span class="op">(</span>data <span class="op">=</span> <span class="fu">tibble</span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from <span class="op">=</span> <span class="fl">100</span>, to <span class="op">=</span> <span class="fl">250</span>, by <span class="op">=</span> <span class="fl">.1</span><span class="op">)</span><span class="op">)</span>,</span>
<span>       <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">x</span>, mean <span class="op">=</span> <span class="fl">178</span>, sd <span class="op">=</span> <span class="fl">20</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ylab</span><span class="op">(</span><span class="st">"density"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<p><img src="4_geocentric_linear_models_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid" width="288"></p>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Why these particular values?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>The prior for <span class="math inline">\(\mu\)</span> is a broad Gaussian prior, centered on 178 cm, with 95% of probability between 178 - 40 cm. Why 178 cm? Your author is 178 cm tall. And the range from 138 cm to 218 cm encompasses a huge range of plausible mean heights for human populations. So domain-specific information has gone into this prior.</p>
</blockquote>
</div>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
DR question: why is 0 in the distribution for the std. error?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>I guess this is fixed later?</p>
</div>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
brms’s <code>get_prior</code>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>At this point Sleegers’ notes consider the <code>brms</code> function <code>get_prior</code> for the mean only model <code>height ~ 1</code>. <code>get_prior</code> peeks at the data to consider an (?appropriate) prior.</p>
<div class="cell">
<details><summary>Code</summary><div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">brms</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/brms/man/get_prior.html">get_prior</a></span><span class="op">(</span><span class="va">height</span> <span class="op">~</span> <span class="fl">1</span>, data <span class="op">=</span> <span class="va">d2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output cell-output-stdout">
<pre><code>                    prior     class coef group resp dpar nlpar bound  source
 student_t(3, 154.3, 8.5) Intercept                                  default
     student_t(3, 0, 8.5)     sigma                                  default</code></pre>
</div>
</div>
<p>It’s not clear to me what <code>get_prior</code> is doing here, or what its logic is. It would seem to be using the data to suggest priors, which McElreath seems to be against (but the ‘empirical bayes’ people seem to like). What exactly is the justification for doing this? the people who designed this package must have had something in mind.</p>
<p>Anyways, <code>get, suggesting_prior</code> suggests specific student=t distributions for the intercept (mean) and for sigma. These t-distributions have three parameters, one of which (the ‘degrees of freedom’) affects the skewness/fatness of tails relative to the normal distribution.</p>
</div>
</div>
</div>
<p><strong>Why simulate the prior probability distribution?</strong></p>
<blockquote class="blockquote">
<p>Once you’ve chosen priors for <span class="math inline">\(h\)</span>, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, these imply a joint prior distribution of individual heights. By simulating from this distribution, you can see what your choices imply about observable height. This helps you diagnose bad choices.</p>
</blockquote>
<blockquote class="blockquote">
<p>… it can be quite hard to anticipate how priors influence the observable variables</p>
</blockquote>
<p>The prior doesn’t affect the results much if you have a reasonably diffuse prior and lots of data. However: &gt; There are plenty of inference problems for which the data alone are not sufficient, no matter how numerous. Bayes lets us proceed in these cases. But only if we use our scientific knowledge to construct sensible priors. Using scientific knowledge to build priors is not cheating. The important thing is that your prior not be based on the values in the data, but only on what you know about the data before you see it.</p>
<p>…</p>
<blockquote class="blockquote">
<p>so to get the joint likelihood across all the data, we have to compute the probability for each <span class="math inline">\(h_i\)</span> [observed height] and then multiply all these likelihoods together</p>
</blockquote>
<p><br></p>
<p>Below: ggplot of the prior for <span class="math inline">\(\sigma\)</span>, a uniform distribution with a minimum value of 0 and a maximum value of 50.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="cell">
<details><summary>Code</summary><div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">tibble</span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from <span class="op">=</span> <span class="op">-</span><span class="fl">10</span>, to <span class="op">=</span> <span class="fl">60</span>, by <span class="op">=</span> <span class="fl">.1</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="co">#just the grid of sigma  space to plot over</span></span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">dunif</a></span><span class="op">(</span><span class="va">x</span>, min <span class="op">=</span> <span class="fl">0</span>, max <span class="op">=</span> <span class="fl">50</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="co">#I can 'create the y variable' within the ggplot</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_y_continuous</span><span class="op">(</span><span class="cn">NULL</span>, breaks <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>panel.grid <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<p><img src="4_geocentric_linear_models_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid" width="288"></p>
</div>
</div>
<p>We can simulate from both priors at once to get a prior probability distribution of <code>heights</code>.</p>
<div class="cell">
<details><summary>Code</summary><div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">1e4</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">4</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">tibble</span><span class="op">(</span>sample_mu    <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">178</span>,       sd  <span class="op">=</span> <span class="fl">20</span><span class="op">)</span>, <span class="co">#10k draws from normal for mean height</span></span>
<span>       sample_sigma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, min  <span class="op">=</span> <span class="fl">0</span>,         max <span class="op">=</span> <span class="fl">50</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>  <span class="co">#10k draws from uniform for sd of height</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="va">sample_mu</span>, sd  <span class="op">=</span> <span class="va">sample_sigma</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="co">#10k draws of height from normal with mean and sd  from above in each case?</span></span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_density</span><span class="op">(</span>fill <span class="op">=</span> <span class="st">"black"</span>, size <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_y_continuous</span><span class="op">(</span><span class="cn">NULL</span>, breaks <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>subtitle <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">Prior</span><span class="op">~</span><span class="va">predictive</span><span class="op">~</span><span class="va">distribution</span><span class="op">~</span><span class="st">"for"</span><span class="op">~</span><span class="fu">italic</span><span class="op">(</span><span class="va">h</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span><span class="op">)</span>,</span>
<span>       x <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>panel.grid <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<p><img src="4_geocentric_linear_models_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid" width="288"></p>
</div>
</div>
<p>As McElreath wrote, we’ve made a “vaguely bell-shaped density with thick tails. It is the expected distribution of heights, averaged over the prior” (p.&nbsp;83).</p>
</section><section id="grid-approximation-of-the-posterior-distribution" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="grid-approximation-of-the-posterior-distribution">Grid approximation of the posterior distribution</h3>
<div class="cell">
<details><summary>All mean and sd height values to consider</summary><div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">200</span></span>
<span></span>
<span><span class="va">d_grid</span> <span class="op">&lt;-</span></span>
<span>  <span class="co"># we'll accomplish with `tidyr::crossing()` what McElreath did with base R `expand.grid()`</span></span>
<span>  <span class="fu">crossing</span><span class="op">(</span>mu    <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from <span class="op">=</span> <span class="fl">140</span>, to <span class="op">=</span> <span class="fl">160</span>, length.out <span class="op">=</span> <span class="va">n</span><span class="op">)</span>,</span>
<span>           sigma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from <span class="op">=</span> <span class="fl">4</span>,   to <span class="op">=</span> <span class="fl">9</span>,   length.out <span class="op">=</span> <span class="va">n</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">glimpse</span><span class="op">(</span><span class="va">d_grid</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output cell-output-stdout">
<pre><code>Rows: 40,000
Columns: 2
$ mu    &lt;dbl&gt; 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140,…
$ sigma &lt;dbl&gt; 4.000000, 4.025126, 4.050251, 4.075377, 4.100503, 4.125628, 4.150754, 4.175879, 4.20…</code></pre>
</div>
</div>
<p><code>d_grid</code> contains every combination of <code>mu</code> and <code>sigma</code> across their specified values. Instead of base R <code><a href="https://rdrr.io/r/base/lapply.html">sapply()</a></code>, we’ll do the computations by making a custom function which we’ll plug into <code>purrr::map2().</code></p>
<div class="cell">
<details><summary>function computes &amp; sums log of normal density for each value of d2$height, given a particular mu and sigma</summary><div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">grid_function</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">mu</span>, <span class="va">sigma</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">d2</span><span class="op">$</span><span class="va">height</span>, mean <span class="op">=</span> <span class="va">mu</span>, sd <span class="op">=</span> <span class="va">sigma</span>, log <span class="op">=</span> <span class="cn">T</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details><summary>maps the log llhd of the data (and params) for each combination in d_grid, converts to a relative probability</summary><div class="sourceCode" id="cb31"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">d_grid</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">d_grid</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>log_likelihood <span class="op">=</span> <span class="fu">map2</span><span class="op">(</span><span class="va">mu</span>, <span class="va">sigma</span>, <span class="va">grid_function</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="co">#maps </span></span>
<span>  <span class="fu">unnest</span><span class="op">(</span><span class="va">log_likelihood</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>prior_mu    <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">mu</span>,    mean <span class="op">=</span> <span class="fl">178</span>, sd  <span class="op">=</span> <span class="fl">20</span>, log <span class="op">=</span> <span class="cn">T</span><span class="op">)</span>,</span>
<span>         prior_sigma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">dunif</a></span><span class="op">(</span><span class="va">sigma</span>, min  <span class="op">=</span> <span class="fl">0</span>,   max <span class="op">=</span> <span class="fl">50</span>, log <span class="op">=</span> <span class="cn">T</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>product <span class="op">=</span> <span class="va">log_likelihood</span> <span class="op">+</span> <span class="va">prior_mu</span> <span class="op">+</span> <span class="va">prior_sigma</span>,</span>
<span>    max_product <span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">product</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>probability <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">product</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">product</span><span class="op">)</span><span class="op">)</span> <span class="co"># exponentiate the log likelihood to get the probability; but the individual probability densities are meaningless. For computational reasons (I think) we state these relative to the max value</span></span>
<span>    <span class="op">)</span></span>
<span>  </span>
<span><span class="va">d_grid</span> <span class="op">%&gt;%</span> <span class="fu">arrange</span><span class="op">(</span><span class="op">-</span><span class="va">probability</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">kable</span><span class="op">(</span>cap <span class="op">=</span> </span>
<span>  <span class="st">"highest prob. rows"</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">kable_styling</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">

<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>highest prob. rows</caption>
 <thead><tr>
<th style="text-align:right;"> mu </th>
   <th style="text-align:right;"> sigma </th>
   <th style="text-align:right;"> log_likelihood </th>
   <th style="text-align:right;"> prior_mu </th>
   <th style="text-align:right;"> prior_sigma </th>
   <th style="text-align:right;"> product </th>
   <th style="text-align:right;"> max_product </th>
   <th style="text-align:right;"> probability </th>
  </tr></thead>
<tbody>
<tr>
<td style="text-align:right;"> 154.5729 </td>
   <td style="text-align:right;"> 7.743719 </td>
   <td style="text-align:right;"> -1219.408 </td>
   <td style="text-align:right;"> -4.600709 </td>
   <td style="text-align:right;"> -3.912023 </td>
   <td style="text-align:right;"> -1227.920 </td>
   <td style="text-align:right;"> -1227.92 </td>
   <td style="text-align:right;"> 1.0000000 </td>
  </tr>
<tr>
<td style="text-align:right;"> 154.5729 </td>
   <td style="text-align:right;"> 7.718593 </td>
   <td style="text-align:right;"> -1219.408 </td>
   <td style="text-align:right;"> -4.600709 </td>
   <td style="text-align:right;"> -3.912023 </td>
   <td style="text-align:right;"> -1227.921 </td>
   <td style="text-align:right;"> -1227.92 </td>
   <td style="text-align:right;"> 0.9999332 </td>
  </tr>
<tr>
<td style="text-align:right;"> 154.5729 </td>
   <td style="text-align:right;"> 7.768844 </td>
   <td style="text-align:right;"> -1219.415 </td>
   <td style="text-align:right;"> -4.600709 </td>
   <td style="text-align:right;"> -3.912023 </td>
   <td style="text-align:right;"> -1227.928 </td>
   <td style="text-align:right;"> -1227.92 </td>
   <td style="text-align:right;"> 0.9927172 </td>
  </tr>
<tr>
<td style="text-align:right;"> 154.5729 </td>
   <td style="text-align:right;"> 7.693467 </td>
   <td style="text-align:right;"> -1219.415 </td>
   <td style="text-align:right;"> -4.600709 </td>
   <td style="text-align:right;"> -3.912023 </td>
   <td style="text-align:right;"> -1227.928 </td>
   <td style="text-align:right;"> -1227.92 </td>
   <td style="text-align:right;"> 0.9923983 </td>
  </tr>
<tr>
<td style="text-align:right;"> 154.6734 </td>
   <td style="text-align:right;"> 7.743719 </td>
   <td style="text-align:right;"> -1219.423 </td>
   <td style="text-align:right;"> -4.594836 </td>
   <td style="text-align:right;"> -3.912023 </td>
   <td style="text-align:right;"> -1227.930 </td>
   <td style="text-align:right;"> -1227.92 </td>
   <td style="text-align:right;"> 0.9905659 </td>
  </tr>
<tr>
<td style="text-align:right;"> 154.6734 </td>
   <td style="text-align:right;"> 7.718593 </td>
   <td style="text-align:right;"> -1219.423 </td>
   <td style="text-align:right;"> -4.594836 </td>
   <td style="text-align:right;"> -3.912023 </td>
   <td style="text-align:right;"> -1227.930 </td>
   <td style="text-align:right;"> -1227.92 </td>
   <td style="text-align:right;"> 0.9904006 </td>
  </tr>
</tbody>
</table>
</div>
<details><summary>maps the log llhd of the data (and params) for each combination in d_grid, converts to a relative probability</summary><div class="sourceCode" id="cb32"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">d_grid</span> <span class="op">%&gt;%</span> <span class="fu">arrange</span><span class="op">(</span><span class="op">-</span><span class="va">probability</span><span class="op">)</span> <span class="op">%&gt;%</span>  <span class="fu">slice_sample</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">10</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">kable</span><span class="op">(</span>cap <span class="op">=</span> </span>
<span>  <span class="st">"random rows"</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">kable_styling</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">

<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>random rows</caption>
 <thead><tr>
<th style="text-align:right;"> mu </th>
   <th style="text-align:right;"> sigma </th>
   <th style="text-align:right;"> log_likelihood </th>
   <th style="text-align:right;"> prior_mu </th>
   <th style="text-align:right;"> prior_sigma </th>
   <th style="text-align:right;"> product </th>
   <th style="text-align:right;"> max_product </th>
   <th style="text-align:right;"> probability </th>
  </tr></thead>
<tbody>
<tr>
<td style="text-align:right;"> 141.2060 </td>
   <td style="text-align:right;"> 7.793970 </td>
   <td style="text-align:right;"> -1738.975 </td>
   <td style="text-align:right;"> -5.606916 </td>
   <td style="text-align:right;"> -3.912023 </td>
   <td style="text-align:right;"> -1748.494 </td>
   <td style="text-align:right;"> -1227.92 </td>
   <td style="text-align:right;"> 0.0000000 </td>
  </tr>
<tr>
<td style="text-align:right;"> 156.0804 </td>
   <td style="text-align:right;"> 5.306533 </td>
   <td style="text-align:right;"> -1298.278 </td>
   <td style="text-align:right;"> -4.515257 </td>
   <td style="text-align:right;"> -3.912023 </td>
   <td style="text-align:right;"> -1306.705 </td>
   <td style="text-align:right;"> -1227.92 </td>
   <td style="text-align:right;"> 0.0000000 </td>
  </tr>
<tr>
<td style="text-align:right;"> 145.3266 </td>
   <td style="text-align:right;"> 4.653266 </td>
   <td style="text-align:right;"> -2049.097 </td>
   <td style="text-align:right;"> -5.249107 </td>
   <td style="text-align:right;"> -3.912023 </td>
   <td style="text-align:right;"> -2058.258 </td>
   <td style="text-align:right;"> -1227.92 </td>
   <td style="text-align:right;"> 0.0000000 </td>
  </tr>
<tr>
<td style="text-align:right;"> 146.2312 </td>
   <td style="text-align:right;"> 4.603015 </td>
   <td style="text-align:right;"> -1938.764 </td>
   <td style="text-align:right;"> -5.176245 </td>
   <td style="text-align:right;"> -3.912023 </td>
   <td style="text-align:right;"> -1947.852 </td>
   <td style="text-align:right;"> -1227.92 </td>
   <td style="text-align:right;"> 0.0000000 </td>
  </tr>
<tr>
<td style="text-align:right;"> 160.0000 </td>
   <td style="text-align:right;"> 4.804020 </td>
   <td style="text-align:right;"> -1554.369 </td>
   <td style="text-align:right;"> -4.319671 </td>
   <td style="text-align:right;"> -3.912023 </td>
   <td style="text-align:right;"> -1562.601 </td>
   <td style="text-align:right;"> -1227.92 </td>
   <td style="text-align:right;"> 0.0000000 </td>
  </tr>
<tr>
<td style="text-align:right;"> 140.8040 </td>
   <td style="text-align:right;"> 6.060301 </td>
   <td style="text-align:right;"> -2155.813 </td>
   <td style="text-align:right;"> -5.644097 </td>
   <td style="text-align:right;"> -3.912023 </td>
   <td style="text-align:right;"> -2165.369 </td>
   <td style="text-align:right;"> -1227.92 </td>
   <td style="text-align:right;"> 0.0000000 </td>
  </tr>
<tr>
<td style="text-align:right;"> 155.7789 </td>
   <td style="text-align:right;"> 8.547739 </td>
   <td style="text-align:right;"> -1226.091 </td>
   <td style="text-align:right;"> -4.531893 </td>
   <td style="text-align:right;"> -3.912023 </td>
   <td style="text-align:right;"> -1234.535 </td>
   <td style="text-align:right;"> -1227.92 </td>
   <td style="text-align:right;"> 0.0013412 </td>
  </tr>
<tr>
<td style="text-align:right;"> 147.8392 </td>
   <td style="text-align:right;"> 5.231156 </td>
   <td style="text-align:right;"> -1584.059 </td>
   <td style="text-align:right;"> -5.051763 </td>
   <td style="text-align:right;"> -3.912023 </td>
   <td style="text-align:right;"> -1593.023 </td>
   <td style="text-align:right;"> -1227.92 </td>
   <td style="text-align:right;"> 0.0000000 </td>
  </tr>
<tr>
<td style="text-align:right;"> 159.0955 </td>
   <td style="text-align:right;"> 6.512563 </td>
   <td style="text-align:right;"> -1315.027 </td>
   <td style="text-align:right;"> -4.361397 </td>
   <td style="text-align:right;"> -3.912023 </td>
   <td style="text-align:right;"> -1323.301 </td>
   <td style="text-align:right;"> -1227.92 </td>
   <td style="text-align:right;"> 0.0000000 </td>
  </tr>
<tr>
<td style="text-align:right;"> 149.1457 </td>
   <td style="text-align:right;"> 7.115578 </td>
   <td style="text-align:right;"> -1325.270 </td>
   <td style="text-align:right;"> -4.955382 </td>
   <td style="text-align:right;"> -3.912023 </td>
   <td style="text-align:right;"> -1334.138 </td>
   <td style="text-align:right;"> -1227.92 </td>
   <td style="text-align:right;"> 0.0000000 </td>
  </tr>
</tbody>
</table>
</div>
</div>
<p>Above, we compute the likelihood of each data point given each combination of parameters under consideration, and multiply these together (or ‘add the log probabilities’). We compute the log probability of these parameters, and add these to the probability of the data under these parameters to get the joint (log) likelihood. Erexponentiate the log likelihood to get the probability. However, with a continuous probabilityindividual probability density values are meaningless; only the relative values matter. For computational reasons (I think) we state each of these relative to the max value of the probabilities.</p>
<p>Above, we present the ‘highest probability’ values as well as some randomly chosen values.</p>
<p>Following Kurz, we can plot ‘where the model thinks the most likely values of our parameters lie’, e.g., in a heatmap plot:</p>
<div class="cell">
<details><summary>Heatmap:</summary><div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">d_grid</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">mu</span>, y <span class="op">=</span> <span class="va">sigma</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_raster</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>fill <span class="op">=</span> <span class="va">probability</span><span class="op">)</span>,</span>
<span>              interpolate <span class="op">=</span> <span class="cn">T</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_fill_viridis_c</span><span class="op">(</span>option <span class="op">=</span> <span class="st">"A"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">mu</span><span class="op">)</span>,</span>
<span>       y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">sigma</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">coord_cartesian</span><span class="op">(</span>xlim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/range.html">range</a></span><span class="op">(</span><span class="va">d_grid</span><span class="op">$</span><span class="va">mu</span><span class="op">)</span><span class="op">*</span><span class="fl">.7</span><span class="op">+</span><span class="fl">50</span>,</span>
<span>                  ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/range.html">range</a></span><span class="op">(</span><span class="va">d_grid</span><span class="op">$</span><span class="va">sigma</span><span class="op">)</span><span class="op">*</span><span class="fl">.7</span> <span class="op">+</span> <span class="fl">3.5</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>panel.grid <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<p><img src="4_geocentric_linear_models_files/figure-html/heat_map-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Note that the posterior distribution of parameters need not be a circle or even symmetric. They may be correlated. Certain parameters may be ‘jointly more’ or ‘jointly less’ likely, as the process that generated the data we see may (e.g.) only tend be likely to come from high mean values when the standard deviation tends to be large.</p>
</section><section id="sampling-from-the-posterior-3.4" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="sampling-from-the-posterior-3.4">Sampling from the posterior (3.4)</h3>
<blockquote class="blockquote">
<p>since there are two parameters, and we want to sample combinations of them, we first randomly sample row numbers in post in proportion to the values in <code>post$prob</code>. Then we pull out the parameter values on those randomly sampled rows draw from grid in proportion to calculated likelihoods.</p>
</blockquote>
<blockquote class="blockquote">
<p>The jargon “marginal” here means “averaging over the other parameters.”</p>
</blockquote>
<blockquote class="blockquote">
<p>And this is quite typical. As sample size increases, posterior densities approach the normal distribution.</p>
</blockquote>
<p>DR: All posterior densities? When andhy?</p>
<p>is the standard deviation - that causes problems. So if you care about –often people do not-you do need to be careful of abusing the quadratic approximation bc quap essentially plus a normal distribution</p>
<p>Finding the posterior distribution with quap</p>
<p>quadratic approximation</p>
<p>posterior-s peak will lie at the maximum a posteriori estimate (MAP), and we can get a useful image of the posterior-s shape by using the quadratic approximation of the posterior distribution at this peak</p>
<p>The quap function works by using the model definition</p>
<p>uses these definitions to define the posterior probability at each combination of parameter values. Then it can climb the posterior distribution and find the peak, its MAP. Finally, it estimates the quadratic curvature at the MAP to produce an approximation of the posterior distribution how quap works, approximately</p>
<p>m4.1 &lt;- quap( flist , data=d2 )</p>
<p>These numbers provide Gaussian approximations for each parameter-s marginal distribution</p>
<p>This means the plausibility of each value of -, after averaging over the plausibilities of each value of -, is given by a Gaussian distribution with mean 154.6 and standard deviation 0.4</p>
<p>But I don-t recommend 95% intervals, because readers will have a hard time not viewing them as significance tests</p>
<p>values. Unless you tell it otherwise, quap starts at random values sampled from the prior. But it-s also possible to specify a starting value for any parameter in the model. In</p>
<p>start &lt;- list( mu=mean(d2<span class="math inline">\(height), sigma=sd(d2\)</span>height) ) m4.1 &lt;- quap( flist , data=d2 , start=start )</p>
<p>nullwhen you define a list of formulas, you should use alist, so the code isn-t executed. But when you define a list of start values for parameters, you should use list, so that</p>
<p>. Once the golem is certain that the mean is near 178-as the prior insists-then the golem has to estimate - conditional on that fact. This results in a different posterior for -, even though all we changed is prior information about the other parameter</p>
<p>nulla quadratic approximation to a posterior distribution with more than one parameter dimension– and - each contribute one dimension-is just a multi-dimensional Gaussian distribution</p>
<p>when R constructs a quadratic approximation, it calculates not only standard deviations for all parameters, but also the covariances among all pairs of parameters</p>
<p>, a list of means and a matrix of variances and covariances are sufficient to describe a multi-dimensional Gaussian distribution. To see</p>
<p>variance-covariance matrix can be factored into two elements: (1) a vector of variances for the parameters and (2) a correlation matrix that tells us how changes in any parameter lead to correlated changes in the others</p>
<p>very close to zero in this example. This indicates that learning - tells us nothing about - and likewise that learning - tells us nothing about -</p>
<p>? Now instead of sampling single values from a simple Gaussian distribution, we sample vectors of values from a multi-dimensional Gaussian distribution</p>
</section></section><section id="linear-prediction-4.4" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="linear-prediction-4.4">Linear prediction (4.4)</h2>
<p>These samples also preserve the covariance between - and -.</p>
<p>make the parameter for the mean of a Gaussian distribution, -, into a linear function of the predictor variable and other, new parameters that we invent.</p>
<p>The linear model strategy instructs the golem to assume that the predictor variable has a constant and additive relationship to the mean of the outcome. The golem then computes the posterior distribution of this constant relationship.</p>
<p>We ask the golem: -Consider all the lines that relate one variable to the other. Rank all of these lines by plausibility, given these data.- The golem answers with a posterior distribution.</p>
<p>definition of -i is deterministic</p>
<p>It is often called a -slope</p>
<p>Better to think of it as a rate of change in expectation</p>
<p>Why have a Gaussian prior with mean zero</p>
<p>To figure out what this prior implies, we have to simulate the prior predictive distribution</p>
<p>goal is to simulate heights from the model, using only the priors</p>
<p>simulate over. The range of observed weights</p>
<p>We know that average height increases with average weight, at least up to a point. Let-s try restricting it to positive values. The easiest way to do this is to define the prior as Log-Normal instead</p>
<p>there are many analyses in which no amount of data makes the prior irrelevant</p>
<p>nullWe don-t pay any attention to p-values in this book. But the danger remains, if we choose our priors conditional on the observed sample, just to get some desired result. The procedure we-ve performed in this chapter is to choose priors conditional on pre-data knowledge of the variables- their constraints, ranges, and theoretical relationships. This is why the actual data are not shown in the earlier section. We are judging our priors against general facts, not the sample. We bayesian p hacking? this needs elaboration</p>
<p>This seems to be Jamie Elsey’s point about reluctance to use any of the data/knowledg from the data in setting the priors, even over hyperparameters. The ‘empirical Bayes’ guy seems to disagree with this.</p>
<p>You can usefully think of y = log(x) as assigning to y the order of magnitude of x. The function x = exp(y) is the reverse, turning a magnitude into a value</p>
<p>Note the exp(log_b) in the definition of mu. This what’s the benefit of this substitution?</p>
<p>There are two broad categories of processing: (1) reading tables and (2) plotting simulations.</p>
<p>emphasize plotting posterior distributions and posterior predictions, instead of attempting to understand a table</p>
<p>Plotting the implications of your models will allow you to inquire about things that are hard to read from tables: (1) Whether or not the model fitting procedure worked correctly (2) The absolute magnitude, rather than merely relative magnitude, of a relationship between outcome and predictor (3) The uncertainty surrounding an average relationship (4) The uncertainty surrounding the implied predictions of the model, as these are distinct from mere parameter uncertainty</p>
<p>Posterior probabilities of parameter values describe the relative compatibility of different states of the world with the data, according to the model</p>
<p>marginal posterior distributions of the parameters ‘marginal’ bc we are integrating or summing across the other parameters when estimating these measures for each parameter</p>
<p>. It is most certainly not evidence that the relationship between weight and height is linear, because the model only considered lines. It just says that, if you are committed to a line, then lines with a slope around 0.9 are plausible ones</p>
<p>to describe the quadratic posterior completely. For that, we also require the variance-covariance matrix We somehow estimated the (MAP?) values of the parameters in a series of simulations (I forgot how). Now we also consider the variance and covariance of these. Is this simply the empirical variance and covariance across simulations? (And then IIRC quap uses these to generate a more complete posterior and sample from it or something). I guess this is helpful because in each simulation I only need to derive a few things, the sort of max confidence values of the parameters, rather than the posterior probability for all possible values. We also can, in principle, use only a few simulations (shown later) and derive an estimate of that covariance matrix and then, assuming normality or something, draw from the joint posterior implied by that covariance matrix to plot things.</p>
<p>nullan informal check on model assumptions. When the model-s predictions don-t come close to key observations or patterns in the plotted data, then you might suspect the model either did not fit correctly or is rather badly specified we do something like this in our 80k work etc when we compare the results from a ‘model’ to mean differences across conditions</p>
<p>nullBut for even slightly more complex models, especially those that include interaction effects (Chapter 8), interpreting posterior distributions is hard</p>
<p>nullposterior distribution considers every possible regression line connecting height to weight. It assigns a relative plausibility to each. This means that each combination of - and - has a posterior probability. It could be that there are many lines with nearly the same posterior probability as the average line. Or it could be instead that the posterior distribution is rather narrow near the average</p>
<p>we could sample a bunch of lines from the posterior distribution. Then we could display those lines on the plot, to visualize the uncertainty in the regression relationship. Drawing randomly from the posterior distributin will of course draw more ‘likely’ lines more often</p>
<p>Each row is a correlated random sample from the joint posterior of all three parameters, using the covariances provided by vcov(m4.3). The</p>
<p>average of very many of these lines is the posterior mean line I don’t understand … is he implying that the MAP line will also be the “average” of the intercept and slope coefficients or something?</p>
<p>The cloud of regression lines displays greater uncertainty at extreme values for weight</p>
<p>Notice that the cloud of regression lines grows more compact as the sample size increases. This is a result of the model growing more confident about the location of the mean. More confident about what mean? Does he mean ‘more confident about the slope’?</p>
<p>I think he means “more confident about the slope of the mean height in weight, as well as about the intercept … thus more confident about the mean height for wach weight</p>
<p>take your quap approximation, sample from the posterior distribution, and then compute - for each case in the data and sample from the posterior distribution. Here</p>
<p>We actually want something slightly different: a distribution of - for each unique weight value on the horizontal axis. It-s only slightly harder to compute that, by just passing link some new data:</p>
<p>Read apply(mu,2,mean) as compute the mean of each column (dimension -2-) of the matrix mu. Nowmu.mean contains the average - at each weight value, and mu.PI contains 89% lower and upper bounds for each weight value</p>
<p>true that it is possible to use analytical formulas to compute intervals like this</p>
<p>approach, and there is some additional insight that comes from knowing the mathematics, the pseudo-empirical approach presented here is very flexible and allows a much broader audience of scientists to pull insight from their statistical modeling. And again, when you start estimating models with MCMC (Chapter 9), this is really the only approach available</p>
<p>Use link to generate distributions of posterior values for -. The default behavior of link is to use the original data, so you have to pass it a list of new horizontal axis values you want to plot posterior predictions across. (2) Use summary functions like mean or PI to find averages and lower and upper bounds of - for each value of the predictor variable. (3) Finally, use plotting functions like lines and shade to draw the lines and intervals. Or you might plot the distributions of the predictions, or do further numerical calculations with them. It-s really up to you. This recipe works for every model we fit in the book Is there a comparable ‘universal recipe’ in the tidy “love letter” adaptation?</p>
<p>generating an 89% prediction interval for actual heights, not just the average height, -. This means we-ll incorporate the standard deviation - and its uncertainty as well</p>
<p>For any unique weight value, you sample from a Gaussian distribution with the correct mean - for that weight, using the correct value of - sampled from the same posterior distribution. what does he mean by ‘correct’ here?</p>
<p>nulldo this for every sample from the posterior, for every weight value of interest, you end up with a collection of simulated heights that embody the uncertainty in the posterior as well as the uncertainty in the Gaussian distribution of heights. There is a tool called sim which does this:</p>
<p>This matrix is much like the earlier one, mu, but it contains simulated heights A vector of simulated heights for each element in the weight sequence</p>
<p>You could plot the boundary for other percents, such as 67% and 97% (also both primes), and add those to the plot. it would be nice to plot several of these together, perhaps a gradual distribution/elevation plot of confidence</p>
<p>For every distribution like dnorm, there is a companion simulation function dnorm – species the density at any point (I guess) rnorm – randomly generates frmo the normal distribution</p>
</section><section id="curves-from-lines-4.5" class="level2" data-number="23.4"><h2 data-number="23.4" class="anchored" data-anchor-id="curves-from-lines-4.5">
<span class="header-section-number">23.4</span> Curves from lines (4.5)</h2>
<p>The first is polynomial regression. The second is b-splines I need to know more about how to use the splines</p>
<p>But in general it is better to pre-process any variable transformations-you don-t need the computer to recalculate the transformations on every iteration of the fitting procedure</p>
<p>The parameter - (a) is still the intercept, so it tells us the expected value of height when weight is at its mean value. But it is no longer equal to the mean height in the sample, since there is no guarantee it should in a polynomial regression</p>
<p>We aren-t learning any causal relationship between height and weight</p>
<p>B-splines do not directly transform the predictor by squaring or cubing it. Instead they invent a series of entirely new, synthetic predictor variables. Each of these synthetic variables exists only to gradually turn a specific parameter on and off within a specific range of the real predictor variable. Each of the synthetic variables is called a basis function</p>
<p>The linear model ends up looking very familiar: -i = - + w1Bi,1 + w2Bi,2 + w3Bi,3 + … where Bi,n is the n-th basis function-s value on row i, and the w parameters are corresponding weights for each</p>
<p>divide the full range of the horizontal axis into four parts, using pivot points called knots. The</p>
<p>These synthetic variables are used to gently transition from one region of the horizontal axis to the next. Essentially, these variables tell you which knot you are close to. Beginning on the left of the top plot, basis function 1 has value 1 and all of the others are set to zero. As we move rightwards towards the second knot, basis 1 declines and basis 2 increases. At knot 2, basis 2 has value 1, and all of the others are set to zero</p>
<p>they make the influence of each parameter quite local. At any point on the horizontal axis in Figure 4.12, only two basis functions have non-zero values</p>
<p>Parameters called weights multiply the basis functions. The spline at any given point is the sum of these weighted basis functions</p>
<p>the knots are just values of year that serve as pivots for our spline. Where should the knots go?</p>
<p>simple example above, place the knots at different evenlyspaced quantiles of the predictor variable. This gives you more knots where there are more observations. We</p>
<p>next choice is polynomial degree. This determines how basis functions combine, which determines how the parameters interact to produce the spline not fully explained</p>
<p>the w priors influence how wiggly the spline can be</p>


<!-- -->

</section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>We don’t really need the y axis when looking at the shapes of a density, so we’ll just remove it with <code>scale_y_continuous()</code>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../chapters/3_sampling_the_imaginary.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Ch 3. Sampling the Imaginary (posterior)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/5_spurious_waffles.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Ch 5. The Many Variables &amp; The Spurious Waffles</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb34" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Chapter 4. Geocentric (linear) Models</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo = F, cache = F}</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span>opts_chunk<span class="sc">$</span><span class="fu">set</span>(<span class="at">fig.retina =</span> <span class="fl">2.5</span>)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">width =</span> <span class="dv">100</span>)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rethinking)</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kableExtra)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(viridis)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidybayes)</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rethinkpriorities)</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seed</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">4</span>)</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Set options</span></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_minimal</span>())</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; geocentric model continues to make useful predictions</span></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Linear regression is the geocentric model of applied statistics</span></span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>introduces linear regression as a Bayesian procedure.</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Other common and useful distributions will be used to build generalized linear models (GLMs).</span></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a><span class="fu">## DR question: GLMs?</span></span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>Admit I am still not completely sure what these are. In Econometrics we talked about 'linear in parameters' models. I've dug into the technical definition of GLMs and it is rather obtuse!</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/paste-C2ABCFA6.png)</span></span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why normal distributions are normal</span></span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Many natural (and unnatural) processes have much heavier tails ... A real and important example is financial time series</span></span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a>After laying out his soccer field coin toss shuffle premise, McElreath wrote:</span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-45"><a href="#cb34-45" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; It's hard to say where any individual person will end up, but you can say with great confidence what the collection of positions will be. The distances will be distributed in approximately normal, or Gaussian, fashion. This is true even though the underlying distribution is binomial. It does this because there are so many more possible ways to realize a sequence of left-right steps that sums to zero. There are slightly fewer ways to realize a sequence that ends up one step left or right of zero, and so on, with the number of possible sequences declining in the characteristic bell curve of the normal distribution. (p. 72)</span></span>
<span id="cb34-46"><a href="#cb34-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-47"><a href="#cb34-47" aria-hidden="true" tabindex="-1"></a><span class="fu">### Normal by addition. {.unnumbered}</span></span>
<span id="cb34-48"><a href="#cb34-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-49"><a href="#cb34-49" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, warning = F, message = F}</span></span>
<span id="cb34-50"><a href="#cb34-50" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: football-steps</span></span>
<span id="cb34-51"><a href="#cb34-51" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "coding football steps"</span></span>
<span id="cb34-52"><a href="#cb34-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-53"><a href="#cb34-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-54"><a href="#cb34-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-55"><a href="#cb34-55" aria-hidden="true" tabindex="-1"></a><span class="co"># we set the seed to make the results of `runif()` reproducible.</span></span>
<span id="cb34-56"><a href="#cb34-56" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">4</span>)</span>
<span id="cb34-57"><a href="#cb34-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-58"><a href="#cb34-58" aria-hidden="true" tabindex="-1"></a>pos <span class="ot">&lt;-</span></span>
<span id="cb34-59"><a href="#cb34-59" aria-hidden="true" tabindex="-1"></a>  <span class="co"># make data with 100 people, 16 steps each with a starting point of `step == 0` (i.e., 17 rows per person)</span></span>
<span id="cb34-60"><a href="#cb34-60" aria-hidden="true" tabindex="-1"></a>  <span class="fu">crossing</span>(<span class="at">person =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>,</span>
<span id="cb34-61"><a href="#cb34-61" aria-hidden="true" tabindex="-1"></a>           <span class="at">step   =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">16</span>) <span class="sc">%&gt;%</span></span>
<span id="cb34-62"><a href="#cb34-62" aria-hidden="true" tabindex="-1"></a>  <span class="co">#DR: 'crossing is a great shortcut to 'create all combinations''</span></span>
<span id="cb34-63"><a href="#cb34-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-64"><a href="#cb34-64" aria-hidden="true" tabindex="-1"></a>  <span class="co"># for all steps above `step == 0` simulate a `deviation`</span></span>
<span id="cb34-65"><a href="#cb34-65" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">deviation =</span> <span class="fu">map_dbl</span>( <span class="co">#DR: `map_dbl` to make it a 'vector of numbers' rather than a list</span></span>
<span id="cb34-66"><a href="#cb34-66" aria-hidden="true" tabindex="-1"></a>    step, <span class="co">#for all 16 step entries</span></span>
<span id="cb34-67"><a href="#cb34-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-68"><a href="#cb34-68" aria-hidden="true" tabindex="-1"></a>    <span class="sc">~</span><span class="fu">if_else</span>(. <span class="sc">==</span> <span class="dv">0</span>, <span class="dv">0</span>, <span class="fu">runif</span>(<span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)))) <span class="sc">%&gt;%</span> <span class="co">#defines a function with '~', 0 for step 0, otherwise a uniform distribution step length each time</span></span>
<span id="cb34-69"><a href="#cb34-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-70"><a href="#cb34-70" aria-hidden="true" tabindex="-1"></a>  <span class="co"># after grouping by `person`, compute the cumulative sum of the deviations, then `ungroup()`</span></span>
<span id="cb34-71"><a href="#cb34-71" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(person) <span class="sc">%&gt;%</span></span>
<span id="cb34-72"><a href="#cb34-72" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">position =</span> <span class="fu">cumsum</span>(deviation)) <span class="sc">%&gt;%</span> <span class="co">#cumsum is great</span></span>
<span id="cb34-73"><a href="#cb34-73" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>()</span>
<span id="cb34-74"><a href="#cb34-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-75"><a href="#cb34-75" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-76"><a href="#cb34-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-79"><a href="#cb34-79" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-80"><a href="#cb34-80" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(pos)</span>
<span id="cb34-81"><a href="#cb34-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-82"><a href="#cb34-82" aria-hidden="true" tabindex="-1"></a><span class="fu">precis</span>(pos)</span>
<span id="cb34-83"><a href="#cb34-83" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-84"><a href="#cb34-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-85"><a href="#cb34-85" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Here's the actual plot code.</span></span>
<span id="cb34-86"><a href="#cb34-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-87"><a href="#cb34-87" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig.height = 2, fig.width = 6.4}</span></span>
<span id="cb34-88"><a href="#cb34-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-89"><a href="#cb34-89" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb34-90"><a href="#cb34-90" aria-hidden="true" tabindex="-1"></a>  p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> pos,</span>
<span id="cb34-91"><a href="#cb34-91" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x =</span> step, <span class="at">y =</span> position, <span class="at">group =</span> person)) <span class="sc">+</span> <span class="co">#'where am I (vertical) at each step (horizontal) ... not sure what 'group' does here</span></span>
<span id="cb34-92"><a href="#cb34-92" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>), <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span> <span class="co">#add vertical lines for x intercepts at steps 4 8 and 16</span></span>
<span id="cb34-93"><a href="#cb34-93" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">color =</span> person <span class="sc">&lt;</span> <span class="dv">4</span>, <span class="at">alpha  =</span> person <span class="sc">&lt;</span> <span class="dv">4</span>)) <span class="sc">+</span> <span class="co">#the main lines of interest</span></span>
<span id="cb34-94"><a href="#cb34-94" aria-hidden="true" tabindex="-1"></a>    <span class="co">#focusing on 4 specific cases</span></span>
<span id="cb34-95"><a href="#cb34-95" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"skyblue4"</span>, <span class="st">"black"</span>)) <span class="sc">+</span></span>
<span id="cb34-96"><a href="#cb34-96" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_alpha_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">7</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb34-97"><a href="#cb34-97" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="st">"step number"</span>, <span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">12</span>, <span class="dv">16</span>)) <span class="sc">+</span> <span class="co"># ticks and labels on the bottom, light gridlines</span></span>
<span id="cb34-98"><a href="#cb34-98" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>)</span>
<span id="cb34-99"><a href="#cb34-99" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb34-100"><a href="#cb34-100" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-101"><a href="#cb34-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-102"><a href="#cb34-102" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb34-103"><a href="#cb34-103" aria-hidden="true" tabindex="-1"></a><span class="fu">## DR question about above code</span></span>
<span id="cb34-104"><a href="#cb34-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-105"><a href="#cb34-105" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>precisely what does 'group' do in the above?</span>
<span id="cb34-106"><a href="#cb34-106" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb34-107"><a href="#cb34-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-108"><a href="#cb34-108" aria-hidden="true" tabindex="-1"></a>Code for plotting all random walks on soccer field, steps, and densities</span>
<span id="cb34-109"><a href="#cb34-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-110"><a href="#cb34-110" aria-hidden="true" tabindex="-1"></a>Plots at 4 and 8 steps:</span>
<span id="cb34-111"><a href="#cb34-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-114"><a href="#cb34-114" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-115"><a href="#cb34-115" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 4.2.a.</span></span>
<span id="cb34-116"><a href="#cb34-116" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 4.2.a.</span></span>
<span id="cb34-117"><a href="#cb34-117" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span></span>
<span id="cb34-118"><a href="#cb34-118" aria-hidden="true" tabindex="-1"></a>  pos <span class="sc">%&gt;%</span></span>
<span id="cb34-119"><a href="#cb34-119" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(step <span class="sc">==</span> <span class="dv">4</span>) <span class="sc">%&gt;%</span></span>
<span id="cb34-120"><a href="#cb34-120" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> position)) <span class="sc">+</span></span>
<span id="cb34-121"><a href="#cb34-121" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">stat =</span> <span class="st">"density"</span>, <span class="at">color =</span> <span class="st">"dodgerblue1"</span>) <span class="sc">+</span></span>
<span id="cb34-122"><a href="#cb34-122" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">6</span>, <span class="dv">6</span>)) <span class="sc">+</span></span>
<span id="cb34-123"><a href="#cb34-123" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"4 steps"</span>)</span>
<span id="cb34-124"><a href="#cb34-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-125"><a href="#cb34-125" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 4.2.b.</span></span>
<span id="cb34-126"><a href="#cb34-126" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span></span>
<span id="cb34-127"><a href="#cb34-127" aria-hidden="true" tabindex="-1"></a>  pos <span class="sc">%&gt;%</span></span>
<span id="cb34-128"><a href="#cb34-128" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(step <span class="sc">==</span> <span class="dv">8</span>) <span class="sc">%&gt;%</span></span>
<span id="cb34-129"><a href="#cb34-129" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> position)) <span class="sc">+</span></span>
<span id="cb34-130"><a href="#cb34-130" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">color =</span> <span class="st">"dodgerblue2"</span>) <span class="sc">+</span></span>
<span id="cb34-131"><a href="#cb34-131" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">6</span>, <span class="dv">6</span>)) <span class="sc">+</span></span>
<span id="cb34-132"><a href="#cb34-132" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"8 steps"</span>)</span>
<span id="cb34-133"><a href="#cb34-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-134"><a href="#cb34-134" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-135"><a href="#cb34-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-136"><a href="#cb34-136" aria-hidden="true" tabindex="-1"></a>Get the SD at 16 steps for plotting the functional normal distribution to compare it to.</span>
<span id="cb34-137"><a href="#cb34-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-138"><a href="#cb34-138" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb34-139"><a href="#cb34-139" aria-hidden="true" tabindex="-1"></a><span class="fu">## DR question: Isn't there an analytical formula we could use here for the SD instead?</span></span>
<span id="cb34-140"><a href="#cb34-140" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb34-141"><a href="#cb34-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-144"><a href="#cb34-144" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-145"><a href="#cb34-145" aria-hidden="true" tabindex="-1"></a><span class="co"># this is an intermediary step to get an SD value</span></span>
<span id="cb34-146"><a href="#cb34-146" aria-hidden="true" tabindex="-1"></a>pos_sd <span class="ot">&lt;-</span> pos <span class="sc">%&gt;%</span></span>
<span id="cb34-147"><a href="#cb34-147" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(step <span class="sc">==</span> <span class="dv">16</span>) <span class="sc">%&gt;%</span></span>
<span id="cb34-148"><a href="#cb34-148" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">sd =</span> <span class="fu">sd</span>(position))</span>
<span id="cb34-149"><a href="#cb34-149" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-150"><a href="#cb34-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-151"><a href="#cb34-151" aria-hidden="true" tabindex="-1"></a>Plot at 16 steps, overlay normal distribution (DR -- I put in the SD as an object from above)</span>
<span id="cb34-152"><a href="#cb34-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-155"><a href="#cb34-155" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-156"><a href="#cb34-156" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 4.2.c.</span></span>
<span id="cb34-157"><a href="#cb34-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-158"><a href="#cb34-158" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 4.2.c.</span></span>
<span id="cb34-159"><a href="#cb34-159" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">&lt;-</span></span>
<span id="cb34-160"><a href="#cb34-160" aria-hidden="true" tabindex="-1"></a>  pos <span class="sc">%&gt;%</span></span>
<span id="cb34-161"><a href="#cb34-161" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(step <span class="sc">==</span> <span class="dv">16</span>) <span class="sc">%&gt;%</span></span>
<span id="cb34-162"><a href="#cb34-162" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> position)) <span class="sc">+</span></span>
<span id="cb34-163"><a href="#cb34-163" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> dnorm,</span>
<span id="cb34-164"><a href="#cb34-164" aria-hidden="true" tabindex="-1"></a>                <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> pos_sd[[<span class="dv">1</span>,<span class="dv">1</span>]]),</span>
<span id="cb34-165"><a href="#cb34-165" aria-hidden="true" tabindex="-1"></a>                <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span>  <span class="co"># 2.180408 came from the previous code block</span></span>
<span id="cb34-166"><a href="#cb34-166" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">color =</span> <span class="st">"transparent"</span>, <span class="at">fill =</span> <span class="st">"dodgerblue3"</span>, <span class="at">alpha =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb34-167"><a href="#cb34-167" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">6</span>, <span class="dv">6</span>)) <span class="sc">+</span></span>
<span id="cb34-168"><a href="#cb34-168" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"16 steps"</span>,</span>
<span id="cb34-169"><a href="#cb34-169" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"density"</span>)</span>
<span id="cb34-170"><a href="#cb34-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-171"><a href="#cb34-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-172"><a href="#cb34-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-173"><a href="#cb34-173" aria-hidden="true" tabindex="-1"></a><span class="co"># combine the ggplots</span></span>
<span id="cb34-174"><a href="#cb34-174" aria-hidden="true" tabindex="-1"></a>p1 <span class="sc">|</span> p2 <span class="sc">|</span> p3</span>
<span id="cb34-175"><a href="#cb34-175" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-176"><a href="#cb34-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-177"><a href="#cb34-177" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb34-178"><a href="#cb34-178" aria-hidden="true" tabindex="-1"></a><span class="fu">## DR question (code): Better way to code 'extract the sd as a number' above?</span></span>
<span id="cb34-179"><a href="#cb34-179" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb34-180"><a href="#cb34-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-181"><a href="#cb34-181" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; While we were at it, we explored a few ways to express densities. The main action was with the </span><span class="in">`geom_line()`</span><span class="at">, </span><span class="in">`geom_density()`</span><span class="at">, and </span><span class="in">`stat_function()`</span><span class="at"> functions, respectively.</span></span>
<span id="cb34-182"><a href="#cb34-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-183"><a href="#cb34-183" aria-hidden="true" tabindex="-1"></a>DR: <span class="in">`geom_line(stat = "density"...`</span> might be the same as <span class="in">`geom_density`</span>, <span class="in">`stat_function`</span> is mainly for analytical densities?</span>
<span id="cb34-184"><a href="#cb34-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-185"><a href="#cb34-185" aria-hidden="true" tabindex="-1"></a><span class="fu">#### But why? {.unnumbered}</span></span>
<span id="cb34-186"><a href="#cb34-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-187"><a href="#cb34-187" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Any process that ads together random values from the same distribution converges to a normal. But it's not easy to grasp why addition should result in a bell curve of sums. Here's a conceptual way to think of the process. Whatever the average value of the source distribution, each sample from it can be thought of as a fluctuation from the average value. When we begin to add these fluctuations together, they also begin to cancel one another out. A large positive fluctuation will cancel a large negative one. The more terms in the sum, the more chances for each fluctuation to be canceled by another, or by a series of smaller ones in the opposite direction. So eventually the most likely sum, in the sense that there are the most ways to realize it, will be a sum in which every fluctuation is canceled by another, a sum of zero (relative to the mean). (pp. 73--74)</span></span>
<span id="cb34-188"><a href="#cb34-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-189"><a href="#cb34-189" aria-hidden="true" tabindex="-1"></a><span class="fu">### Normal by multiplication {.unnumbered}</span></span>
<span id="cb34-190"><a href="#cb34-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-191"><a href="#cb34-191" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; small effects that multiply together are approximately additive, and so they also tend to stabilize on Gaussian distributions</span></span>
<span id="cb34-192"><a href="#cb34-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-193"><a href="#cb34-193" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Large deviates that are multiplied together do not produce Gaussian distributions, but they do tend to produce Gaussian distributions on the log scale</span></span>
<span id="cb34-194"><a href="#cb34-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-195"><a href="#cb34-195" aria-hidden="true" tabindex="-1"></a>**Skipped coding this for now**</span>
<span id="cb34-196"><a href="#cb34-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-197"><a href="#cb34-197" aria-hidden="true" tabindex="-1"></a><span class="fu">### Using Gaussian distributions ("but why?") {.unnumbered}</span></span>
<span id="cb34-198"><a href="#cb34-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-199"><a href="#cb34-199" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Ontological justification. {.unnumbered}</span></span>
<span id="cb34-200"><a href="#cb34-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-201"><a href="#cb34-201" aria-hidden="true" tabindex="-1"></a>The Gaussian is</span>
<span id="cb34-202"><a href="#cb34-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-203"><a href="#cb34-203" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; a widespread pattern, appearing again and again at different scales and in different domains. Measurement errors, variations in growth, and the velocities of molecules all tend towards Gaussian distributions. These processes do this because at their heart, these processes add together fluctuations. And repeatedly adding finite fluctuations results in a distribution of sums that have shed all information about the underlying process, aside from mean and spread. One consequence of this is that statistical models based on Gaussian distributions cannot reliably identify micro-process. (p. 75)</span></span>
<span id="cb34-204"><a href="#cb34-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-205"><a href="#cb34-205" aria-hidden="true" tabindex="-1"></a>\~I.e., we cannot 'prove what we assumed'.</span>
<span id="cb34-206"><a href="#cb34-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-207"><a href="#cb34-207" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Kurz: But they can still be useful.</span></span>
<span id="cb34-208"><a href="#cb34-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-209"><a href="#cb34-209" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Epistemological justification.</span></span>
<span id="cb34-210"><a href="#cb34-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-211"><a href="#cb34-211" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Another route to justifying the Gaussian as our choice of skeleton, and a route that will help us appreciate later why it is often a poor choice, is that it represents a particular state of ignorance. When all we know or are willing to say about a distribution of measures (measures are continuous values on the real number line) is their mean and variance, then the Gaussian distribution arises as the most consistent with our assumptions.</span></span>
<span id="cb34-212"><a href="#cb34-212" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb34-213"><a href="#cb34-213" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; That is to say that the Gaussian distribution is the most natural expression of our state of ignorance, because if all we are willing to assume is that a measure has finite variance, the Gaussian distribution is the shape that can be realized in the largest number of ways and does not introduce any new assumptions. It is the least surprising and least informative assumption to make. In this way, the Gaussian is the distribution most consistent with our assumptions... If you don't think the distribution should be Gaussian, then that implies that you know something else that you should tell your golem about, something that would improve inference. (pp. 75--76)</span></span>
<span id="cb34-214"><a href="#cb34-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-215"><a href="#cb34-215" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb34-216"><a href="#cb34-216" aria-hidden="true" tabindex="-1"></a><span class="fu">## DR question -- can we justify this in our own words</span></span>
<span id="cb34-217"><a href="#cb34-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-218"><a href="#cb34-218" aria-hidden="true" tabindex="-1"></a>E.g., to a skeptical audience or 'client'?</span>
<span id="cb34-219"><a href="#cb34-219" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb34-220"><a href="#cb34-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-221"><a href="#cb34-221" aria-hidden="true" tabindex="-1"></a>From McElreath:</span>
<span id="cb34-222"><a href="#cb34-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-223"><a href="#cb34-223" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; By the ontological justification, the world is full of Gaussian distributions, approximately.</span></span>
<span id="cb34-224"><a href="#cb34-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-225"><a href="#cb34-225" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; By the epistemological justification, the Gaussian represents a particular state of ignorance. When all we know or are willing to say about a distribution of measures (measures are continuous values on the real number line) is their mean and variance, then the Gaussian distribution arises as the most consistent with our assumptions maximum entropy?</span></span>
<span id="cb34-226"><a href="#cb34-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-227"><a href="#cb34-227" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; if all we are willing to assume is that a measure has finite variance, the Gaussian distribution is the shape that can be realized in the largest number of ways</span></span>
<span id="cb34-228"><a href="#cb34-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-229"><a href="#cb34-229" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Overthinking: Gaussian distribution.</span></span>
<span id="cb34-230"><a href="#cb34-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-231"><a href="#cb34-231" aria-hidden="true" tabindex="-1"></a>(Kurz quotes below)</span>
<span id="cb34-232"><a href="#cb34-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-233"><a href="#cb34-233" aria-hidden="true" tabindex="-1"></a>Let $y$ be the criterion (DR: why 'criterion'?), $\mu$ be the mean, and $\sigma$ be the standard deviation. Then the probability density of some Gaussian value $y$ is</span>
<span id="cb34-234"><a href="#cb34-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-235"><a href="#cb34-235" aria-hidden="true" tabindex="-1"></a>$$p(y|\mu, \sigma) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \Bigg (- \frac{(y - \mu)^2}{2 \sigma^2} \Bigg).$$</span>
<span id="cb34-236"><a href="#cb34-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-237"><a href="#cb34-237" aria-hidden="true" tabindex="-1"></a>Why not demystify that monster with a little R code? For simplicity, we'll look at $p(y)$ over a series of $y$ values ranging from -4 to 4, holding $\mu = 0$ and $\sigma = 1$. Then we'll plot.</span>
<span id="cb34-238"><a href="#cb34-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-239"><a href="#cb34-239" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig.width = 4, fig.height = 2.75}</span></span>
<span id="cb34-240"><a href="#cb34-240" aria-hidden="true" tabindex="-1"></a><span class="co"># define our input values</span></span>
<span id="cb34-241"><a href="#cb34-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-242"><a href="#cb34-242" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">y     =</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="sc">-</span><span class="dv">4</span>, <span class="at">to =</span> <span class="dv">4</span>, <span class="at">by =</span> .<span class="dv">1</span>),</span>
<span id="cb34-243"><a href="#cb34-243" aria-hidden="true" tabindex="-1"></a>       <span class="at">mu    =</span> <span class="dv">0</span>,</span>
<span id="cb34-244"><a href="#cb34-244" aria-hidden="true" tabindex="-1"></a>       <span class="at">sigma =</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span></span>
<span id="cb34-245"><a href="#cb34-245" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute p(y) using a hand-made gaussian likelihood</span></span>
<span id="cb34-246"><a href="#cb34-246" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb34-247"><a href="#cb34-247" aria-hidden="true" tabindex="-1"></a>    <span class="at">p_y =</span> (<span class="dv">1</span> <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">2</span> <span class="sc">*</span> pi <span class="sc">*</span> sigma<span class="sc">^</span><span class="dv">2</span>)) <span class="sc">*</span> <span class="fu">exp</span>(<span class="sc">-</span>(y <span class="sc">-</span> mu)<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> (<span class="dv">2</span> <span class="sc">*</span> sigma<span class="sc">^</span><span class="dv">2</span>))) <span class="sc">%&gt;%</span></span>
<span id="cb34-248"><a href="#cb34-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-249"><a href="#cb34-249" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot!</span></span>
<span id="cb34-250"><a href="#cb34-250" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> y, <span class="at">y =</span> p_y)) <span class="sc">+</span></span>
<span id="cb34-251"><a href="#cb34-251" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb34-252"><a href="#cb34-252" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="fu">expression</span>(<span class="fu">italic</span>(p)(<span class="fu">italic</span>(<span class="st">"y|"</span>)<span class="sc">*</span>mu<span class="sc">==</span><span class="dv">0</span><span class="sc">*</span><span class="st">","</span><span class="sc">~</span>sigma<span class="sc">==</span><span class="dv">1</span>)))</span>
<span id="cb34-253"><a href="#cb34-253" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-254"><a href="#cb34-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-255"><a href="#cb34-255" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; You get the same results if you switch out that mutate line with </span><span class="in">`mutate(p_y = dnorm(y)) %&gt;%`</span><span class="at">. To learn more, execute </span><span class="in">`?dnorm`</span><span class="at">.</span></span>
<span id="cb34-256"><a href="#cb34-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-257"><a href="#cb34-257" aria-hidden="true" tabindex="-1"></a>*DR: how does executing it demystify it?*</span>
<span id="cb34-258"><a href="#cb34-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-259"><a href="#cb34-259" aria-hidden="true" tabindex="-1"></a><span class="fu">## A language for describing models (4.2)</span></span>
<span id="cb34-260"><a href="#cb34-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-261"><a href="#cb34-261" aria-hidden="true" tabindex="-1"></a>For example:</span>
<span id="cb34-262"><a href="#cb34-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-263"><a href="#cb34-263" aria-hidden="true" tabindex="-1"></a><span class="in">```{=tex}</span></span>
<span id="cb34-264"><a href="#cb34-264" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{align*}</span></span>
<span id="cb34-265"><a href="#cb34-265" aria-hidden="true" tabindex="-1"></a><span class="in">\text{criterion}_i &amp; \sim \text{Normal}(\mu_i, \sigma) \\</span></span>
<span id="cb34-266"><a href="#cb34-266" aria-hidden="true" tabindex="-1"></a><span class="in">\mu_i  &amp; = \beta \times \text{predictor}_i \\</span></span>
<span id="cb34-267"><a href="#cb34-267" aria-hidden="true" tabindex="-1"></a><span class="in">\beta  &amp; \sim \text{Normal}(0, 10) \\</span></span>
<span id="cb34-268"><a href="#cb34-268" aria-hidden="true" tabindex="-1"></a><span class="in">\sigma &amp; \sim \text{HalfCauchy}(0, 1).</span></span>
<span id="cb34-269"><a href="#cb34-269" aria-hidden="true" tabindex="-1"></a><span class="in">\end{align*}</span></span>
<span id="cb34-270"><a href="#cb34-270" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-271"><a href="#cb34-271" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The combination of variables and their probability distributions defines a joint generative model that can be used both to simulate hypothetical observations as well as analyze real ones.</span></span>
<span id="cb34-272"><a href="#cb34-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-273"><a href="#cb34-273" aria-hidden="true" tabindex="-1"></a>DR: No 'error term' as in standard econometrics statement</span>
<span id="cb34-274"><a href="#cb34-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-275"><a href="#cb34-275" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We no longer have to remember seemingly arbitrary lists of bizarre conditions like homoscedasticity (constant variance), because we can just read these conditions from the model definitions. We specify distributions rather than error terms and conditions</span></span>
<span id="cb34-276"><a href="#cb34-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-277"><a href="#cb34-277" aria-hidden="true" tabindex="-1"></a>DR: Note these specific distributions are 'stronger assumptions', which people may argue are harder to justify</span>
<span id="cb34-278"><a href="#cb34-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-279"><a href="#cb34-279" aria-hidden="true" tabindex="-1"></a><span class="fu">### Re-describing the globe tossing model (in these terms) {.unnumbered}</span></span>
<span id="cb34-280"><a href="#cb34-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-281"><a href="#cb34-281" aria-hidden="true" tabindex="-1"></a>DR: We previously just described it in words iirc</span>
<span id="cb34-282"><a href="#cb34-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-283"><a href="#cb34-283" aria-hidden="true" tabindex="-1"></a>For the globe tossing model, the probability $p$ of a count of water $w$ based on $n$ trials was</span>
<span id="cb34-284"><a href="#cb34-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-285"><a href="#cb34-285" aria-hidden="true" tabindex="-1"></a><span class="in">```{=tex}</span></span>
<span id="cb34-286"><a href="#cb34-286" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{align*}</span></span>
<span id="cb34-287"><a href="#cb34-287" aria-hidden="true" tabindex="-1"></a><span class="in">w &amp; \sim \text{Binomial}(n, p) \\</span></span>
<span id="cb34-288"><a href="#cb34-288" aria-hidden="true" tabindex="-1"></a><span class="in">p &amp; \sim \text{Uniform}(0, 1).</span></span>
<span id="cb34-289"><a href="#cb34-289" aria-hidden="true" tabindex="-1"></a><span class="in">\end{align*}</span></span>
<span id="cb34-290"><a href="#cb34-290" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-291"><a href="#cb34-291" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/paste-F25C2CF0.png)</span></span>
<span id="cb34-292"><a href="#cb34-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-293"><a href="#cb34-293" aria-hidden="true" tabindex="-1"></a>'probability p and data drawn' / 'probability data drawn'</span>
<span id="cb34-294"><a href="#cb34-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-295"><a href="#cb34-295" aria-hidden="true" tabindex="-1"></a>Well, it's what we *compute* in the grid approximation, for each probability.</span>
<span id="cb34-296"><a href="#cb34-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-297"><a href="#cb34-297" aria-hidden="true" tabindex="-1"></a>Remember the denominator is the same for all values of p.</span>
<span id="cb34-298"><a href="#cb34-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-299"><a href="#cb34-299" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb34-300"><a href="#cb34-300" aria-hidden="true" tabindex="-1"></a><span class="fu">## From model description to Bayes theorem</span></span>
<span id="cb34-301"><a href="#cb34-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-304"><a href="#cb34-304" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-305"><a href="#cb34-305" aria-hidden="true" tabindex="-1"></a><span class="co"># how many `p_grid` points would you like?</span></span>
<span id="cb34-306"><a href="#cb34-306" aria-hidden="true" tabindex="-1"></a>n_points <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb34-307"><a href="#cb34-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-308"><a href="#cb34-308" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span></span>
<span id="cb34-309"><a href="#cb34-309" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(<span class="at">p_grid =</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="dv">1</span>, <span class="at">length.out =</span> n_points),</span>
<span id="cb34-310"><a href="#cb34-310" aria-hidden="true" tabindex="-1"></a>         <span class="at">w      =</span> <span class="dv">6</span>,</span>
<span id="cb34-311"><a href="#cb34-311" aria-hidden="true" tabindex="-1"></a>         <span class="at">n      =</span> <span class="dv">9</span>) <span class="sc">%&gt;%</span></span>
<span id="cb34-312"><a href="#cb34-312" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">prior      =</span> <span class="fu">dunif</span>(p_grid, <span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb34-313"><a href="#cb34-313" aria-hidden="true" tabindex="-1"></a>         <span class="at">likelihood =</span> <span class="fu">dbinom</span>(w, n, p_grid)) <span class="sc">%&gt;%</span></span>
<span id="cb34-314"><a href="#cb34-314" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">posterior =</span> likelihood <span class="sc">*</span> prior <span class="sc">/</span> <span class="fu">sum</span>(likelihood <span class="sc">*</span> prior))</span>
<span id="cb34-315"><a href="#cb34-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-316"><a href="#cb34-316" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(d)</span>
<span id="cb34-317"><a href="#cb34-317" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-318"><a href="#cb34-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-319"><a href="#cb34-319" aria-hidden="true" tabindex="-1"></a>In case you were curious, here's what they look like.</span>
<span id="cb34-320"><a href="#cb34-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-321"><a href="#cb34-321" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig.width = 8, fig.height = 2.25}</span></span>
<span id="cb34-322"><a href="#cb34-322" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span></span>
<span id="cb34-323"><a href="#cb34-323" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>w, <span class="sc">-</span>n) <span class="sc">%&gt;%</span></span>
<span id="cb34-324"><a href="#cb34-324" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>(key, value, <span class="sc">-</span>p_grid) <span class="sc">%&gt;%</span></span>
<span id="cb34-325"><a href="#cb34-325" aria-hidden="true" tabindex="-1"></a>  <span class="co"># this line allows us to dictate the order the panels will appear in</span></span>
<span id="cb34-326"><a href="#cb34-326" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">key =</span> <span class="fu">factor</span>(key, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"prior"</span>, <span class="st">"likelihood"</span>, <span class="st">"posterior"</span>))) <span class="sc">%&gt;%</span></span>
<span id="cb34-327"><a href="#cb34-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-328"><a href="#cb34-328" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> p_grid, <span class="at">ymin =</span> <span class="dv">0</span>, <span class="at">ymax =</span> value, <span class="at">fill =</span> key)) <span class="sc">+</span></span>
<span id="cb34-329"><a href="#cb34-329" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_ribbon</span>() <span class="sc">+</span></span>
<span id="cb34-330"><a href="#cb34-330" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"purple"</span>)) <span class="sc">+</span></span>
<span id="cb34-331"><a href="#cb34-331" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="cn">NULL</span>, <span class="at">breaks =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb34-332"><a href="#cb34-332" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>) <span class="sc">+</span></span>
<span id="cb34-333"><a href="#cb34-333" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>key, <span class="at">scales =</span> <span class="st">"free"</span>)</span>
<span id="cb34-334"><a href="#cb34-334" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-335"><a href="#cb34-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-336"><a href="#cb34-336" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The posterior is a combination of the prior and the likelihood. When the prior is flat across the parameter space, the posterior is just the likelihood re-expressed as a probability. As we go along, you'll see that we almost never use flat priors in practice.</span></span>
<span id="cb34-337"><a href="#cb34-337" aria-hidden="true" tabindex="-1"></a><span class="at">:::</span></span>
<span id="cb34-338"><a href="#cb34-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-339"><a href="#cb34-339" aria-hidden="true" tabindex="-1"></a><span class="fu">## A Gaussian model of height (4.3)</span></span>
<span id="cb34-340"><a href="#cb34-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-341"><a href="#cb34-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-342"><a href="#cb34-342" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb34-343"><a href="#cb34-343" aria-hidden="true" tabindex="-1"></a><span class="fu">## Willem Sleeger's ["Figuring out Bayesian statistics"](https://willemsleegers.com/content/posts/5-my-bayesian-workflow-and-tutorial/my-bayesian-workflow-and-tutorial.html) covers much of this same ground.</span></span>
<span id="cb34-344"><a href="#cb34-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-345"><a href="#cb34-345" aria-hidden="true" tabindex="-1"></a>His blog entry is better formatted and talks through some parts of the intuition more. You may find it more useful than the present notes. I incorporate some of it below (with acknowledgement)</span>
<span id="cb34-346"><a href="#cb34-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-347"><a href="#cb34-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-348"><a href="#cb34-348" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb34-349"><a href="#cb34-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-350"><a href="#cb34-350" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; single measurement variable to model as a Gaussian distribution. There will be two parameters describing the distribution's shape, the mean - and the standard deviation </span></span>
<span id="cb34-351"><a href="#cb34-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-352"><a href="#cb34-352" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; the 'estimate' here will be the entire posterior distribution, not any point within it</span></span>
<span id="cb34-353"><a href="#cb34-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-354"><a href="#cb34-354" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; And as a result, the posterior distribution will be a distribution of Gaussian distributions. or of the parameters of these</span></span>
<span id="cb34-355"><a href="#cb34-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-356"><a href="#cb34-356" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; There are an infinite number of possible Gaussian distributions. Some have small means. Others have large means. Some are wide, with a large $\sigma$. Others are narrow. We want our Bayesian machine to consider every possible distribution, each defined by a combination of $\mu$ and $\sigma$, and rank them by posterior plausibility. (p. 79)</span></span>
<span id="cb34-357"><a href="#cb34-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-358"><a href="#cb34-358" aria-hidden="true" tabindex="-1"></a><span class="fu">### The data (height) {.unnumbered}</span></span>
<span id="cb34-359"><a href="#cb34-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-360"><a href="#cb34-360" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Let's get the Howell (</span><span class="co">[</span><span class="ot">2000</span><span class="co">](https://www.routledge.com/Demography-of-the-Dobe-Kung/Howell/p/book/9780202306490)</span><span class="at">, </span><span class="co">[</span><span class="ot">2010</span><span class="co">](https://www.ucpress.edu/book/9780520262348/life-histories-of-the-dobe-kung)</span><span class="at">) data from McElreath's </span><span class="co">[</span><span class="ot">rethinking package</span><span class="co">](https://xcelab.net/rm/statistical-rethinking/)</span><span class="at">.</span></span>
<span id="cb34-361"><a href="#cb34-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-362"><a href="#cb34-362" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, message = F}</span></span>
<span id="cb34-363"><a href="#cb34-363" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rethinking)</span>
<span id="cb34-364"><a href="#cb34-364" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(Howell1)</span>
<span id="cb34-365"><a href="#cb34-365" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> Howell1</span>
<span id="cb34-366"><a href="#cb34-366" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-367"><a href="#cb34-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-368"><a href="#cb34-368" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb34-369"><a href="#cb34-369" aria-hidden="true" tabindex="-1"></a><span class="fu">## Code notes on brms</span></span>
<span id="cb34-370"><a href="#cb34-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-371"><a href="#cb34-371" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; (Kurz) Here we open our main statistical package, Bürkner's </span><span class="co">[</span><span class="ot">brms</span><span class="co">](https://github.com/paul-buerkner/brms)</span><span class="at">. But before we do, we'll want to detach the rethinking package. R will not allow users to use a function from one package that shares the same name as a different function from another package if both packages are open at the same time. The rethinking and </span><span class="in">`brms`</span><span class="at"> packages are designed for similar purposes and, unsurprisingly, overlap in the names of their functions. To prevent problems, it is a good idea to make sure rethinking is detached before using brms. To learn more on the topic, see </span><span class="co">[</span><span class="ot">this R-bloggers post</span><span class="co">](https://www.r-bloggers.com/r-and-package-masking-a-real-life-example/)</span><span class="at">.</span></span>
<span id="cb34-372"><a href="#cb34-372" aria-hidden="true" tabindex="-1"></a><span class="at">:::</span></span>
<span id="cb34-373"><a href="#cb34-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-374"><a href="#cb34-374" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, message = F}</span></span>
<span id="cb34-375"><a href="#cb34-375" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(Howell1)</span>
<span id="cb34-376"><a href="#cb34-376" aria-hidden="true" tabindex="-1"></a><span class="fu">detach</span>(package<span class="sc">:</span>rethinking, <span class="at">unload =</span> T)</span>
<span id="cb34-377"><a href="#cb34-377" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(brms)</span>
<span id="cb34-378"><a href="#cb34-378" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-379"><a href="#cb34-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-380"><a href="#cb34-380" aria-hidden="true" tabindex="-1"></a>Go ahead and investigate the data with <span class="in">`str()`</span>, the tidyverse analogue for which is <span class="in">`glimpse()`</span>.</span>
<span id="cb34-381"><a href="#cb34-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-384"><a href="#cb34-384" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-385"><a href="#cb34-385" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span></span>
<span id="cb34-386"><a href="#cb34-386" aria-hidden="true" tabindex="-1"></a>  <span class="fu">str</span>()</span>
<span id="cb34-387"><a href="#cb34-387" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-388"><a href="#cb34-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-389"><a href="#cb34-389" aria-hidden="true" tabindex="-1"></a>Here are the <span class="in">`height`</span> values</span>
<span id="cb34-390"><a href="#cb34-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-393"><a href="#cb34-393" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-394"><a href="#cb34-394" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span></span>
<span id="cb34-395"><a href="#cb34-395" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(height) <span class="sc">%&gt;%</span></span>
<span id="cb34-396"><a href="#cb34-396" aria-hidden="true" tabindex="-1"></a>  <span class="fu">head</span>()</span>
<span id="cb34-397"><a href="#cb34-397" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-398"><a href="#cb34-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-399"><a href="#cb34-399" aria-hidden="true" tabindex="-1"></a>We can use <span class="in">`filter()`</span> to make an adults-only data frame.</span>
<span id="cb34-400"><a href="#cb34-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-403"><a href="#cb34-403" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-404"><a href="#cb34-404" aria-hidden="true" tabindex="-1"></a>d2 <span class="ot">&lt;-</span></span>
<span id="cb34-405"><a href="#cb34-405" aria-hidden="true" tabindex="-1"></a>  d <span class="sc">%&gt;%</span></span>
<span id="cb34-406"><a href="#cb34-406" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(age <span class="sc">&gt;=</span> <span class="dv">18</span>)</span>
<span id="cb34-407"><a href="#cb34-407" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-408"><a href="#cb34-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-409"><a href="#cb34-409" aria-hidden="true" tabindex="-1"></a>There are a lot of ways we can make sure our <span class="in">`d2`</span> has 352 rows. Here's one.</span>
<span id="cb34-410"><a href="#cb34-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-413"><a href="#cb34-413" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-414"><a href="#cb34-414" aria-hidden="true" tabindex="-1"></a>d2 <span class="sc">%&gt;%</span></span>
<span id="cb34-415"><a href="#cb34-415" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>()</span>
<span id="cb34-416"><a href="#cb34-416" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-417"><a href="#cb34-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-418"><a href="#cb34-418" aria-hidden="true" tabindex="-1"></a><span class="fu">### The model {.unnumbered}</span></span>
<span id="cb34-419"><a href="#cb34-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-420"><a href="#cb34-420" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; as mentioned earlier in this chapter, the empirical distribution needn't be actually Gaussian in order to justify using a Gaussian probability distribution why not?</span></span>
<span id="cb34-421"><a href="#cb34-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-422"><a href="#cb34-422" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb34-423"><a href="#cb34-423" aria-hidden="true" tabindex="-1"></a><span class="fu">## DR question: Why? is this because we are considering *means*?</span></span>
<span id="cb34-424"><a href="#cb34-424" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb34-425"><a href="#cb34-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-426"><a href="#cb34-426" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb34-427"><a href="#cb34-427" aria-hidden="true" tabindex="-1"></a><span class="fu">## On iid as an 'epistemological assumption'</span></span>
<span id="cb34-428"><a href="#cb34-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-429"><a href="#cb34-429" aria-hidden="true" tabindex="-1"></a>The i.i.d. assumption is about how the golem represents its uncertainty. It is an epistemological assumption. It is not a physical assumption about the world, an ontological one. E. T. Jaynes (1922-1998) called this the mind projection fallacy, the mistake of confusing epistemological claims with ontological claims.71 The point isnt that epistemology trumps reality, but that in ignorance of such correlations the best distribution may be i.i.d.72</span>
<span id="cb34-430"><a href="#cb34-430" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb34-431"><a href="#cb34-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-432"><a href="#cb34-432" aria-hidden="true" tabindex="-1"></a>The likelihood for our model is</span>
<span id="cb34-433"><a href="#cb34-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-434"><a href="#cb34-434" aria-hidden="true" tabindex="-1"></a>$$h_i \sim \operatorname{Normal}(\mu, \sigma),$$</span>
<span id="cb34-435"><a href="#cb34-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-436"><a href="#cb34-436" aria-hidden="true" tabindex="-1"></a>our $\mu$ prior will be</span>
<span id="cb34-437"><a href="#cb34-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-438"><a href="#cb34-438" aria-hidden="true" tabindex="-1"></a>$$\mu \sim \operatorname{Normal}(178, 20),$$</span>
<span id="cb34-439"><a href="#cb34-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-440"><a href="#cb34-440" aria-hidden="true" tabindex="-1"></a>and our prior for $\sigma$ will be</span>
<span id="cb34-441"><a href="#cb34-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-442"><a href="#cb34-442" aria-hidden="true" tabindex="-1"></a>$$\sigma \sim \operatorname{Uniform}(0, 50).$$</span>
<span id="cb34-443"><a href="#cb34-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-444"><a href="#cb34-444" aria-hidden="true" tabindex="-1"></a>Here's the shape of the prior for $\mu$ in $N(178, 20)$.</span>
<span id="cb34-445"><a href="#cb34-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-446"><a href="#cb34-446" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig.width = 3, fig.height = 2.5}</span></span>
<span id="cb34-447"><a href="#cb34-447" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">100</span>, <span class="at">to =</span> <span class="dv">250</span>, <span class="at">by =</span> .<span class="dv">1</span>)),</span>
<span id="cb34-448"><a href="#cb34-448" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> <span class="fu">dnorm</span>(x, <span class="at">mean =</span> <span class="dv">178</span>, <span class="at">sd =</span> <span class="dv">20</span>))) <span class="sc">+</span></span>
<span id="cb34-449"><a href="#cb34-449" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb34-450"><a href="#cb34-450" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"density"</span>)</span>
<span id="cb34-451"><a href="#cb34-451" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-452"><a href="#cb34-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-453"><a href="#cb34-453" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb34-454"><a href="#cb34-454" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why these particular values?</span></span>
<span id="cb34-455"><a href="#cb34-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-456"><a href="#cb34-456" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The prior for $\mu$ is a broad Gaussian prior, centered on 178 cm, with 95% of probability between 178 - 40 cm. Why 178 cm? Your author is 178 cm tall. And the range from 138 cm to 218 cm encompasses a huge range of plausible mean heights for human populations. So domain-specific information has gone into this prior.</span></span>
<span id="cb34-457"><a href="#cb34-457" aria-hidden="true" tabindex="-1"></a><span class="at">:::</span></span>
<span id="cb34-458"><a href="#cb34-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-459"><a href="#cb34-459" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb34-460"><a href="#cb34-460" aria-hidden="true" tabindex="-1"></a><span class="fu">## DR question: why is 0 in the distribution for the std. error?</span></span>
<span id="cb34-461"><a href="#cb34-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-462"><a href="#cb34-462" aria-hidden="true" tabindex="-1"></a>I guess this is fixed later?</span>
<span id="cb34-463"><a href="#cb34-463" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb34-464"><a href="#cb34-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-465"><a href="#cb34-465" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb34-466"><a href="#cb34-466" aria-hidden="true" tabindex="-1"></a><span class="fu">## brms's `get_prior` </span></span>
<span id="cb34-467"><a href="#cb34-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-468"><a href="#cb34-468" aria-hidden="true" tabindex="-1"></a>At this point Sleegers' notes consider the <span class="in">`brms`</span> function <span class="in">`get_prior`</span> for the mean only model <span class="in">`height ~ 1`</span>.  <span class="in">`get_prior`</span>  peeks at the data to consider an (?appropriate) prior. </span>
<span id="cb34-469"><a href="#cb34-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-472"><a href="#cb34-472" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-473"><a href="#cb34-473" aria-hidden="true" tabindex="-1"></a>brms<span class="sc">::</span><span class="fu">get_prior</span>(height <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> d2)</span>
<span id="cb34-474"><a href="#cb34-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-475"><a href="#cb34-475" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-476"><a href="#cb34-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-477"><a href="#cb34-477" aria-hidden="true" tabindex="-1"></a>It's not clear to me what <span class="in">`get_prior`</span> is doing here, or what its logic is. It would seem to be using the data to suggest priors, which McElreath seems to be against (but the 'empirical bayes' people seem to like). What exactly is the justification for doing this? the people who designed this package must have had something in mind.</span>
<span id="cb34-478"><a href="#cb34-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-479"><a href="#cb34-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-480"><a href="#cb34-480" aria-hidden="true" tabindex="-1"></a>Anyways, <span class="in">`get, suggesting_prior`</span> suggests specific student=t distributions for the intercept (mean) and for sigma. These t-distributions have three parameters, one of which (the 'degrees of freedom') affects the skewness/fatness of tails relative to the normal distribution.</span>
<span id="cb34-481"><a href="#cb34-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-482"><a href="#cb34-482" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb34-483"><a href="#cb34-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-484"><a href="#cb34-484" aria-hidden="true" tabindex="-1"></a>**Why simulate the prior probability distribution?**</span>
<span id="cb34-485"><a href="#cb34-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-486"><a href="#cb34-486" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Once you've chosen priors for $h$, $\mu$ and $\sigma$,  these imply a joint prior distribution of individual heights. By simulating from this distribution, you can see what your choices imply about observable height. This helps you diagnose bad choices.</span></span>
<span id="cb34-487"><a href="#cb34-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-488"><a href="#cb34-488" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... it can be quite hard to anticipate how priors influence the observable variables</span></span>
<span id="cb34-489"><a href="#cb34-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-490"><a href="#cb34-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-491"><a href="#cb34-491" aria-hidden="true" tabindex="-1"></a>The prior doesn't affect the results much if you have a reasonably diffuse prior and lots of data.  However: </span>
<span id="cb34-492"><a href="#cb34-492" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; There are plenty of inference problems for which the data alone are not sufficient, no matter how numerous. Bayes lets us proceed in these cases. But only if we use our scientific knowledge to construct sensible priors. Using scientific knowledge to build priors is not cheating. The important thing is that your prior not be based on the values in the data, but only on what you know about the data before you see it.</span></span>
<span id="cb34-493"><a href="#cb34-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-494"><a href="#cb34-494" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb34-495"><a href="#cb34-495" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb34-496"><a href="#cb34-496" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb34-497"><a href="#cb34-497" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; so to get the joint likelihood across all the data, we have to compute the probability for each $h_i$ </span><span class="co">[</span><span class="ot">observed height</span><span class="co">]</span><span class="at"> and then multiply all these likelihoods together</span></span>
<span id="cb34-498"><a href="#cb34-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-499"><a href="#cb34-499" aria-hidden="true" tabindex="-1"></a>\</span>
<span id="cb34-500"><a href="#cb34-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-501"><a href="#cb34-501" aria-hidden="true" tabindex="-1"></a>Below: ggplot of the prior for $\sigma$, a uniform distribution with a minimum value of 0 and a maximum value of 50.^<span class="co">[</span><span class="ot">We don't really need the y axis when looking at the shapes of a density, so we'll just remove it with `scale_y_continuous()`.</span><span class="co">]</span></span>
<span id="cb34-502"><a href="#cb34-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-503"><a href="#cb34-503" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig.width = 3, fig.height = 2.5}</span></span>
<span id="cb34-504"><a href="#cb34-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-505"><a href="#cb34-505" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="sc">-</span><span class="dv">10</span>, <span class="at">to =</span> <span class="dv">60</span>, <span class="at">by =</span> .<span class="dv">1</span>)) <span class="sc">%&gt;%</span> <span class="co">#just the grid of sigma  space to plot over</span></span>
<span id="cb34-506"><a href="#cb34-506" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> <span class="fu">dunif</span>(x, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">50</span>))) <span class="sc">+</span> <span class="co">#I can 'create the y variable' within the ggplot</span></span>
<span id="cb34-507"><a href="#cb34-507" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb34-508"><a href="#cb34-508" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="cn">NULL</span>, <span class="at">breaks =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb34-509"><a href="#cb34-509" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">panel.grid =</span> <span class="fu">element_blank</span>())</span>
<span id="cb34-510"><a href="#cb34-510" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-511"><a href="#cb34-511" aria-hidden="true" tabindex="-1"></a>We can simulate from both priors at once to get a prior probability distribution of <span class="in">`heights`</span>.</span>
<span id="cb34-512"><a href="#cb34-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-513"><a href="#cb34-513" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig.width = 3, fig.height = 2.5}</span></span>
<span id="cb34-514"><a href="#cb34-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-515"><a href="#cb34-515" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fl">1e4</span></span>
<span id="cb34-516"><a href="#cb34-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-517"><a href="#cb34-517" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">4</span>)</span>
<span id="cb34-518"><a href="#cb34-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-519"><a href="#cb34-519" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">sample_mu    =</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">178</span>,       <span class="at">sd  =</span> <span class="dv">20</span>), <span class="co">#10k draws from normal for mean height</span></span>
<span id="cb34-520"><a href="#cb34-520" aria-hidden="true" tabindex="-1"></a>       <span class="at">sample_sigma =</span> <span class="fu">runif</span>(n, <span class="at">min  =</span> <span class="dv">0</span>,         <span class="at">max =</span> <span class="dv">50</span>)) <span class="sc">%&gt;%</span>  <span class="co">#10k draws from uniform for sd of height</span></span>
<span id="cb34-521"><a href="#cb34-521" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">x =</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> sample_mu, <span class="at">sd  =</span> sample_sigma)) <span class="sc">%&gt;%</span></span>
<span id="cb34-522"><a href="#cb34-522" aria-hidden="true" tabindex="-1"></a>  <span class="co">#10k draws of height from normal with mean and sd  from above in each case?</span></span>
<span id="cb34-523"><a href="#cb34-523" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x)) <span class="sc">+</span></span>
<span id="cb34-524"><a href="#cb34-524" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">fill =</span> <span class="st">"black"</span>, <span class="at">size =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb34-525"><a href="#cb34-525" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="cn">NULL</span>, <span class="at">breaks =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb34-526"><a href="#cb34-526" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="fu">expression</span>(Prior<span class="sc">~</span>predictive<span class="sc">~</span>distribution<span class="sc">~</span><span class="st">"for"</span><span class="sc">~</span><span class="fu">italic</span>(h[i])),</span>
<span id="cb34-527"><a href="#cb34-527" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb34-528"><a href="#cb34-528" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">panel.grid =</span> <span class="fu">element_blank</span>())</span>
<span id="cb34-529"><a href="#cb34-529" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-530"><a href="#cb34-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-531"><a href="#cb34-531" aria-hidden="true" tabindex="-1"></a>As McElreath wrote, we've made a "vaguely bell-shaped density with thick tails. It is the expected distribution of heights, averaged over the prior" (p. 83).</span>
<span id="cb34-532"><a href="#cb34-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-533"><a href="#cb34-533" aria-hidden="true" tabindex="-1"></a><span class="fu">### Grid approximation of the posterior distribution {-}</span></span>
<span id="cb34-534"><a href="#cb34-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-537"><a href="#cb34-537" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-538"><a href="#cb34-538" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: grid_approx_height</span></span>
<span id="cb34-539"><a href="#cb34-539" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "All mean and sd height values to consider"</span></span>
<span id="cb34-540"><a href="#cb34-540" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb34-541"><a href="#cb34-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-542"><a href="#cb34-542" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">200</span></span>
<span id="cb34-543"><a href="#cb34-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-544"><a href="#cb34-544" aria-hidden="true" tabindex="-1"></a>d_grid <span class="ot">&lt;-</span></span>
<span id="cb34-545"><a href="#cb34-545" aria-hidden="true" tabindex="-1"></a>  <span class="co"># we'll accomplish with `tidyr::crossing()` what McElreath did with base R `expand.grid()`</span></span>
<span id="cb34-546"><a href="#cb34-546" aria-hidden="true" tabindex="-1"></a>  <span class="fu">crossing</span>(<span class="at">mu    =</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">140</span>, <span class="at">to =</span> <span class="dv">160</span>, <span class="at">length.out =</span> n),</span>
<span id="cb34-547"><a href="#cb34-547" aria-hidden="true" tabindex="-1"></a>           <span class="at">sigma =</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">4</span>,   <span class="at">to =</span> <span class="dv">9</span>,   <span class="at">length.out =</span> n))</span>
<span id="cb34-548"><a href="#cb34-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-549"><a href="#cb34-549" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(d_grid)</span>
<span id="cb34-550"><a href="#cb34-550" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-551"><a href="#cb34-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-552"><a href="#cb34-552" aria-hidden="true" tabindex="-1"></a><span class="in">`d_grid`</span> contains every combination of <span class="in">`mu`</span> and <span class="in">`sigma`</span> across their specified values. Instead of base R <span class="in">`sapply()`</span>, we'll do the computations by making a custom function which we'll plug into <span class="in">`purrr::map2().`</span></span>
<span id="cb34-553"><a href="#cb34-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-554"><a href="#cb34-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-557"><a href="#cb34-557" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-558"><a href="#cb34-558" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: grid_function</span></span>
<span id="cb34-559"><a href="#cb34-559" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "function computes &amp; sums log of normal density for each value of d2$height, given a particular mu and sigma"</span></span>
<span id="cb34-560"><a href="#cb34-560" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb34-561"><a href="#cb34-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-562"><a href="#cb34-562" aria-hidden="true" tabindex="-1"></a>grid_function <span class="ot">&lt;-</span> <span class="cf">function</span>(mu, sigma) {</span>
<span id="cb34-563"><a href="#cb34-563" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dnorm</span>(d2<span class="sc">$</span>height, <span class="at">mean =</span> mu, <span class="at">sd =</span> sigma, <span class="at">log =</span> T) <span class="sc">%&gt;%</span> </span>
<span id="cb34-564"><a href="#cb34-564" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sum</span>()</span>
<span id="cb34-565"><a href="#cb34-565" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb34-566"><a href="#cb34-566" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-567"><a href="#cb34-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-570"><a href="#cb34-570" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-571"><a href="#cb34-571" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: d_grid_ll</span></span>
<span id="cb34-572"><a href="#cb34-572" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "maps the log llhd of the data (and params) for each combination in d_grid, converts to a relative probability"</span></span>
<span id="cb34-573"><a href="#cb34-573" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb34-574"><a href="#cb34-574" aria-hidden="true" tabindex="-1"></a>d_grid <span class="ot">&lt;-</span></span>
<span id="cb34-575"><a href="#cb34-575" aria-hidden="true" tabindex="-1"></a>  d_grid <span class="sc">%&gt;%</span> </span>
<span id="cb34-576"><a href="#cb34-576" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">log_likelihood =</span> <span class="fu">map2</span>(mu, sigma, grid_function)) <span class="sc">%&gt;%</span> <span class="co">#maps </span></span>
<span id="cb34-577"><a href="#cb34-577" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(log_likelihood) <span class="sc">%&gt;%</span> </span>
<span id="cb34-578"><a href="#cb34-578" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">prior_mu    =</span> <span class="fu">dnorm</span>(mu,    <span class="at">mean =</span> <span class="dv">178</span>, <span class="at">sd  =</span> <span class="dv">20</span>, <span class="at">log =</span> T),</span>
<span id="cb34-579"><a href="#cb34-579" aria-hidden="true" tabindex="-1"></a>         <span class="at">prior_sigma =</span> <span class="fu">dunif</span>(sigma, <span class="at">min  =</span> <span class="dv">0</span>,   <span class="at">max =</span> <span class="dv">50</span>, <span class="at">log =</span> T)) <span class="sc">%&gt;%</span> </span>
<span id="cb34-580"><a href="#cb34-580" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">product =</span> log_likelihood <span class="sc">+</span> prior_mu <span class="sc">+</span> prior_sigma,</span>
<span id="cb34-581"><a href="#cb34-581" aria-hidden="true" tabindex="-1"></a>    <span class="at">max_product =</span><span class="fu">max</span>(product)) <span class="sc">%&gt;%</span></span>
<span id="cb34-582"><a href="#cb34-582" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">probability =</span> <span class="fu">exp</span>(product <span class="sc">-</span> <span class="fu">max</span>(product)) <span class="co"># exponentiate the log likelihood to get the probability; but the individual probability densities are meaningless. For computational reasons (I think) we state these relative to the max value</span></span>
<span id="cb34-583"><a href="#cb34-583" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb34-584"><a href="#cb34-584" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb34-585"><a href="#cb34-585" aria-hidden="true" tabindex="-1"></a>d_grid <span class="sc">%&gt;%</span> <span class="fu">arrange</span>(<span class="sc">-</span>probability) <span class="sc">%&gt;%</span> </span>
<span id="cb34-586"><a href="#cb34-586" aria-hidden="true" tabindex="-1"></a>  <span class="fu">head</span>() <span class="sc">%&gt;%</span> <span class="fu">kable</span>(<span class="at">cap =</span> </span>
<span id="cb34-587"><a href="#cb34-587" aria-hidden="true" tabindex="-1"></a>  <span class="st">"highest prob. rows"</span>) <span class="sc">%&gt;%</span> <span class="fu">kable_styling</span>()</span>
<span id="cb34-588"><a href="#cb34-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-589"><a href="#cb34-589" aria-hidden="true" tabindex="-1"></a>d_grid <span class="sc">%&gt;%</span> <span class="fu">arrange</span>(<span class="sc">-</span>probability) <span class="sc">%&gt;%</span>  <span class="fu">slice_sample</span>(<span class="at">n =</span> <span class="dv">10</span>) <span class="sc">%&gt;%</span> <span class="fu">kable</span>(<span class="at">cap =</span> </span>
<span id="cb34-590"><a href="#cb34-590" aria-hidden="true" tabindex="-1"></a>  <span class="st">"random rows"</span>) <span class="sc">%&gt;%</span> <span class="fu">kable_styling</span>()</span>
<span id="cb34-591"><a href="#cb34-591" aria-hidden="true" tabindex="-1"></a>    <span class="in">```</span></span>
<span id="cb34-592"><a href="#cb34-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-593"><a href="#cb34-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-594"><a href="#cb34-594" aria-hidden="true" tabindex="-1"></a>Above, we compute the likelihood of each data point given each combination of parameters under consideration, and multiply these together (or 'add the log probabilities'). We compute the log probability of these parameters, and add these to the probability of the data under these parameters to get the joint (log) likelihood.  Erexponentiate the log likelihood to get the probability. However, with a continuous probabilityindividual probability density values  are meaningless; only the relative values matter. For computational reasons (I think) we state each of these relative to the max value of the probabilities.</span>
<span id="cb34-595"><a href="#cb34-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-596"><a href="#cb34-596" aria-hidden="true" tabindex="-1"></a>Above, we present the 'highest probability' values as well as some randomly chosen values.</span>
<span id="cb34-597"><a href="#cb34-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-598"><a href="#cb34-598" aria-hidden="true" tabindex="-1"></a>Following Kurz, we can plot 'where the model thinks the most likely values of our parameters lie', e.g., in a heatmap plot:</span>
<span id="cb34-599"><a href="#cb34-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-602"><a href="#cb34-602" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-603"><a href="#cb34-603" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: heat_map</span></span>
<span id="cb34-604"><a href="#cb34-604" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Heatmap: "</span></span>
<span id="cb34-605"><a href="#cb34-605" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb34-606"><a href="#cb34-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-607"><a href="#cb34-607" aria-hidden="true" tabindex="-1"></a>d_grid <span class="sc">%&gt;%</span> </span>
<span id="cb34-608"><a href="#cb34-608" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> mu, <span class="at">y =</span> sigma)) <span class="sc">+</span> </span>
<span id="cb34-609"><a href="#cb34-609" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_raster</span>(<span class="fu">aes</span>(<span class="at">fill =</span> probability),</span>
<span id="cb34-610"><a href="#cb34-610" aria-hidden="true" tabindex="-1"></a>              <span class="at">interpolate =</span> T) <span class="sc">+</span></span>
<span id="cb34-611"><a href="#cb34-611" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_viridis_c</span>(<span class="at">option =</span> <span class="st">"A"</span>) <span class="sc">+</span></span>
<span id="cb34-612"><a href="#cb34-612" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(mu),</span>
<span id="cb34-613"><a href="#cb34-613" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="fu">expression</span>(sigma)) <span class="sc">+</span></span>
<span id="cb34-614"><a href="#cb34-614" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">range</span>(d_grid<span class="sc">$</span>mu)<span class="sc">*</span>.<span class="dv">7</span><span class="sc">+</span><span class="dv">50</span>,</span>
<span id="cb34-615"><a href="#cb34-615" aria-hidden="true" tabindex="-1"></a>                  <span class="at">ylim =</span> <span class="fu">range</span>(d_grid<span class="sc">$</span>sigma)<span class="sc">*</span>.<span class="dv">7</span> <span class="sc">+</span> <span class="fl">3.5</span>) <span class="sc">+</span></span>
<span id="cb34-616"><a href="#cb34-616" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">panel.grid =</span> <span class="fu">element_blank</span>())</span>
<span id="cb34-617"><a href="#cb34-617" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-618"><a href="#cb34-618" aria-hidden="true" tabindex="-1"></a>Note that the posterior distribution of parameters need not be a circle or even symmetric. They may be correlated. Certain parameters may be 'jointly more' or 'jointly less' likely, as the process that generated the data we see may (e.g.) only tend be likely to come from high mean values when the standard deviation tends to be large.  </span>
<span id="cb34-619"><a href="#cb34-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-620"><a href="#cb34-620" aria-hidden="true" tabindex="-1"></a><span class="fu">### Sampling from the posterior (3.4) {-}</span></span>
<span id="cb34-621"><a href="#cb34-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-622"><a href="#cb34-622" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; since there are two parameters, and we want to sample combinations of them, we first randomly sample row numbers in post in proportion to the values in </span><span class="in">`post$prob`</span><span class="at">. Then we pull out the parameter values on those randomly sampled rows draw from grid in proportion to calculated likelihoods.</span></span>
<span id="cb34-623"><a href="#cb34-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-624"><a href="#cb34-624" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The jargon “marginal” here means “averaging over the other parameters.” </span></span>
<span id="cb34-625"><a href="#cb34-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-626"><a href="#cb34-626" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; And this is quite typical. As sample size increases, posterior densities approach the normal distribution. </span></span>
<span id="cb34-627"><a href="#cb34-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-628"><a href="#cb34-628" aria-hidden="true" tabindex="-1"></a>DR: All posterior densities? When andhy?</span>
<span id="cb34-629"><a href="#cb34-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-630"><a href="#cb34-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-631"><a href="#cb34-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-632"><a href="#cb34-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-633"><a href="#cb34-633" aria-hidden="true" tabindex="-1"></a>is the standard deviation - that causes problems. So if you care about --often people do not-you do need to be careful of abusing the quadratic approximation bc quap essentially plus a normal distribution</span>
<span id="cb34-634"><a href="#cb34-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-635"><a href="#cb34-635" aria-hidden="true" tabindex="-1"></a>Finding the posterior distribution with quap</span>
<span id="cb34-636"><a href="#cb34-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-637"><a href="#cb34-637" aria-hidden="true" tabindex="-1"></a>quadratic approximation</span>
<span id="cb34-638"><a href="#cb34-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-639"><a href="#cb34-639" aria-hidden="true" tabindex="-1"></a>posterior-s peak will lie at the maximum a posteriori estimate (MAP), and we can get a useful image of the posterior-s shape by using the quadratic approximation of the posterior distribution at this peak</span>
<span id="cb34-640"><a href="#cb34-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-641"><a href="#cb34-641" aria-hidden="true" tabindex="-1"></a>The quap function works by using the model definition</span>
<span id="cb34-642"><a href="#cb34-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-643"><a href="#cb34-643" aria-hidden="true" tabindex="-1"></a>uses these definitions to define the posterior probability at each combination of parameter values. Then it can climb the posterior distribution and find the peak, its MAP. Finally, it estimates the quadratic curvature at the MAP to produce an approximation of the posterior distribution how quap works, approximately</span>
<span id="cb34-644"><a href="#cb34-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-645"><a href="#cb34-645" aria-hidden="true" tabindex="-1"></a>m4.1 <span class="sc">\&lt;</span>- quap( flist , data=d2 )</span>
<span id="cb34-646"><a href="#cb34-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-647"><a href="#cb34-647" aria-hidden="true" tabindex="-1"></a>These numbers provide Gaussian approximations for each parameter-s marginal distribution</span>
<span id="cb34-648"><a href="#cb34-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-649"><a href="#cb34-649" aria-hidden="true" tabindex="-1"></a>This means the plausibility of each value of -, after averaging over the plausibilities of each value of -, is given by a Gaussian distribution with mean 154.6 and standard deviation 0.4</span>
<span id="cb34-650"><a href="#cb34-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-651"><a href="#cb34-651" aria-hidden="true" tabindex="-1"></a>But I don-t recommend 95% intervals, because readers will have a hard time not viewing them as significance tests</span>
<span id="cb34-652"><a href="#cb34-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-653"><a href="#cb34-653" aria-hidden="true" tabindex="-1"></a>values. Unless you tell it otherwise, quap starts at random values sampled from the prior. But it-s also possible to specify a starting value for any parameter in the model. In</span>
<span id="cb34-654"><a href="#cb34-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-655"><a href="#cb34-655" aria-hidden="true" tabindex="-1"></a>start <span class="sc">\&lt;</span>- list( mu=mean(d2$height), sigma=sd(d2$height) ) m4.1 <span class="sc">\&lt;</span>- quap( flist , data=d2 , start=start )</span>
<span id="cb34-656"><a href="#cb34-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-657"><a href="#cb34-657" aria-hidden="true" tabindex="-1"></a>nullwhen you define a list of formulas, you should use alist, so the code isn-t executed. But when you define a list of start values for parameters, you should use list, so that</span>
<span id="cb34-658"><a href="#cb34-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-659"><a href="#cb34-659" aria-hidden="true" tabindex="-1"></a>. Once the golem is certain that the mean is near 178-as the prior insists-then the golem has to estimate - conditional on that fact. This results in a different posterior for -, even though all we changed is prior information about the other parameter</span>
<span id="cb34-660"><a href="#cb34-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-661"><a href="#cb34-661" aria-hidden="true" tabindex="-1"></a>nulla quadratic approximation to a posterior distribution with more than one parameter dimension-- and - each contribute one dimension-is just a multi-dimensional Gaussian distribution</span>
<span id="cb34-662"><a href="#cb34-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-663"><a href="#cb34-663" aria-hidden="true" tabindex="-1"></a>when R constructs a quadratic approximation, it calculates not only standard deviations for all parameters, but also the covariances among all pairs of parameters</span>
<span id="cb34-664"><a href="#cb34-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-665"><a href="#cb34-665" aria-hidden="true" tabindex="-1"></a>, a list of means and a matrix of variances and covariances are sufficient to describe a multi-dimensional Gaussian distribution. To see</span>
<span id="cb34-666"><a href="#cb34-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-667"><a href="#cb34-667" aria-hidden="true" tabindex="-1"></a>variance-covariance matrix can be factored into two elements: (1) a vector of variances for the parameters and (2) a correlation matrix that tells us how changes in any parameter lead to correlated changes in the others</span>
<span id="cb34-668"><a href="#cb34-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-669"><a href="#cb34-669" aria-hidden="true" tabindex="-1"></a>very close to zero in this example. This indicates that learning - tells us nothing about - and likewise that learning - tells us nothing about -</span>
<span id="cb34-670"><a href="#cb34-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-671"><a href="#cb34-671" aria-hidden="true" tabindex="-1"></a>? Now instead of sampling single values from a simple Gaussian distribution, we sample vectors of values from a multi-dimensional Gaussian distribution</span>
<span id="cb34-672"><a href="#cb34-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-673"><a href="#cb34-673" aria-hidden="true" tabindex="-1"></a><span class="fu">## Linear prediction (4.4) {-}</span></span>
<span id="cb34-674"><a href="#cb34-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-675"><a href="#cb34-675" aria-hidden="true" tabindex="-1"></a>These samples also preserve the covariance between - and -.</span>
<span id="cb34-676"><a href="#cb34-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-677"><a href="#cb34-677" aria-hidden="true" tabindex="-1"></a>make the parameter for the mean of a Gaussian distribution, -, into a linear function of the predictor variable and other, new parameters that we invent.</span>
<span id="cb34-678"><a href="#cb34-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-679"><a href="#cb34-679" aria-hidden="true" tabindex="-1"></a>The linear model strategy instructs the golem to assume that the predictor variable has a constant and additive relationship to the mean of the outcome. The golem then computes the posterior distribution of this constant relationship.</span>
<span id="cb34-680"><a href="#cb34-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-681"><a href="#cb34-681" aria-hidden="true" tabindex="-1"></a> We ask the golem: -Consider all the lines that relate one variable to the other. Rank all of these lines by plausibility, given these data.- The golem answers with a posterior distribution.</span>
<span id="cb34-682"><a href="#cb34-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-683"><a href="#cb34-683" aria-hidden="true" tabindex="-1"></a>definition of -i is deterministic</span>
<span id="cb34-684"><a href="#cb34-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-685"><a href="#cb34-685" aria-hidden="true" tabindex="-1"></a>It is often called a -slope</span>
<span id="cb34-686"><a href="#cb34-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-687"><a href="#cb34-687" aria-hidden="true" tabindex="-1"></a>Better to think of it as a rate of change in expectation</span>
<span id="cb34-688"><a href="#cb34-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-689"><a href="#cb34-689" aria-hidden="true" tabindex="-1"></a>Why have a Gaussian prior with mean zero</span>
<span id="cb34-690"><a href="#cb34-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-691"><a href="#cb34-691" aria-hidden="true" tabindex="-1"></a>To figure out what this prior implies, we have to simulate the prior predictive distribution</span>
<span id="cb34-692"><a href="#cb34-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-693"><a href="#cb34-693" aria-hidden="true" tabindex="-1"></a>goal is to simulate heights from the model, using only the priors</span>
<span id="cb34-694"><a href="#cb34-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-695"><a href="#cb34-695" aria-hidden="true" tabindex="-1"></a>simulate over. The range of observed weights</span>
<span id="cb34-696"><a href="#cb34-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-697"><a href="#cb34-697" aria-hidden="true" tabindex="-1"></a>We know that average height increases with average weight, at least up to a point. Let-s try restricting it to positive values. The easiest way to do this is to define the prior as Log-Normal instead</span>
<span id="cb34-698"><a href="#cb34-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-699"><a href="#cb34-699" aria-hidden="true" tabindex="-1"></a>there are many analyses in which no amount of data makes the prior irrelevant</span>
<span id="cb34-700"><a href="#cb34-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-701"><a href="#cb34-701" aria-hidden="true" tabindex="-1"></a>nullWe don-t pay any attention to p-values in this book. But the danger remains, if we choose our priors conditional on the observed sample, just to get some desired result. The procedure we-ve performed in this chapter is to choose priors conditional on pre-data knowledge of the variables- their constraints, ranges, and theoretical relationships. This is why the actual data are not shown in the earlier section. We are judging our priors against general facts, not the sample. We bayesian p hacking? this needs elaboration</span>
<span id="cb34-702"><a href="#cb34-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-703"><a href="#cb34-703" aria-hidden="true" tabindex="-1"></a>This seems to be Jamie Elsey's point about reluctance to use any of the data/knowledg from the data in setting the priors, even over hyperparameters. The 'empirical Bayes' guy seems to disagree with this.</span>
<span id="cb34-704"><a href="#cb34-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-705"><a href="#cb34-705" aria-hidden="true" tabindex="-1"></a>You can usefully think of y = log(x) as assigning to y the order of magnitude of x. The function x = exp(y) is the reverse, turning a magnitude into a value</span>
<span id="cb34-706"><a href="#cb34-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-707"><a href="#cb34-707" aria-hidden="true" tabindex="-1"></a>Note the exp(log_b) in the definition of mu. This what's the benefit of this substitution?</span>
<span id="cb34-708"><a href="#cb34-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-709"><a href="#cb34-709" aria-hidden="true" tabindex="-1"></a>There are two broad categories of processing: (1) reading tables and (2) plotting simulations.</span>
<span id="cb34-710"><a href="#cb34-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-711"><a href="#cb34-711" aria-hidden="true" tabindex="-1"></a>emphasize plotting posterior distributions and posterior predictions, instead of attempting to understand a table</span>
<span id="cb34-712"><a href="#cb34-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-713"><a href="#cb34-713" aria-hidden="true" tabindex="-1"></a>Plotting the implications of your models will allow you to inquire about things that are hard to read from tables: (1) Whether or not the model fitting procedure worked correctly (2) The absolute magnitude, rather than merely relative magnitude, of a relationship between outcome and predictor (3) The uncertainty surrounding an average relationship (4) The uncertainty surrounding the implied predictions of the model, as these are distinct from mere parameter uncertainty</span>
<span id="cb34-714"><a href="#cb34-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-715"><a href="#cb34-715" aria-hidden="true" tabindex="-1"></a>Posterior probabilities of parameter values describe the relative compatibility of different states of the world with the data, according to the model</span>
<span id="cb34-716"><a href="#cb34-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-717"><a href="#cb34-717" aria-hidden="true" tabindex="-1"></a>marginal posterior distributions of the parameters 'marginal' bc we are integrating or summing across the other parameters when estimating these measures for each parameter</span>
<span id="cb34-718"><a href="#cb34-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-719"><a href="#cb34-719" aria-hidden="true" tabindex="-1"></a>. It is most certainly not evidence that the relationship between weight and height is linear, because the model only considered lines. It just says that, if you are committed to a line, then lines with a slope around 0.9 are plausible ones</span>
<span id="cb34-720"><a href="#cb34-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-721"><a href="#cb34-721" aria-hidden="true" tabindex="-1"></a>to describe the quadratic posterior completely. For that, we also require the variance-covariance matrix We somehow estimated the (MAP?) values of the parameters in a series of simulations (I forgot how). Now we also consider the variance and covariance of these. Is this simply the empirical variance and covariance across simulations? (And then IIRC quap uses these to generate a more complete posterior and sample from it or something). I guess this is helpful because in each simulation I only need to derive a few things, the sort of max confidence values of the parameters, rather than the posterior probability for all possible values. We also can, in principle, use only a few simulations (shown later) and derive an estimate of that covariance matrix and then, assuming normality or something, draw from the joint posterior implied by that covariance matrix to plot things.</span>
<span id="cb34-722"><a href="#cb34-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-723"><a href="#cb34-723" aria-hidden="true" tabindex="-1"></a>nullan informal check on model assumptions. When the model-s predictions don-t come close to key observations or patterns in the plotted data, then you might suspect the model either did not fit correctly or is rather badly specified we do something like this in our 80k work etc when we compare the results from a 'model' to mean differences across conditions</span>
<span id="cb34-724"><a href="#cb34-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-725"><a href="#cb34-725" aria-hidden="true" tabindex="-1"></a>nullBut for even slightly more complex models, especially those that include interaction effects (Chapter 8), interpreting posterior distributions is hard</span>
<span id="cb34-726"><a href="#cb34-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-727"><a href="#cb34-727" aria-hidden="true" tabindex="-1"></a>nullposterior distribution considers every possible regression line connecting height to weight. It assigns a relative plausibility to each. This means that each combination of - and - has a posterior probability. It could be that there are many lines with nearly the same posterior probability as the average line. Or it could be instead that the posterior distribution is rather narrow near the average</span>
<span id="cb34-728"><a href="#cb34-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-729"><a href="#cb34-729" aria-hidden="true" tabindex="-1"></a>we could sample a bunch of lines from the posterior distribution. Then we could display those lines on the plot, to visualize the uncertainty in the regression relationship. Drawing randomly from the posterior distributin will of course draw more 'likely' lines more often</span>
<span id="cb34-730"><a href="#cb34-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-731"><a href="#cb34-731" aria-hidden="true" tabindex="-1"></a>Each row is a correlated random sample from the joint posterior of all three parameters, using the covariances provided by vcov(m4.3). The</span>
<span id="cb34-732"><a href="#cb34-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-733"><a href="#cb34-733" aria-hidden="true" tabindex="-1"></a>average of very many of these lines is the posterior mean line I don't understand ... is he implying that the MAP line will also be the "average" of the intercept and slope coefficients or something?</span>
<span id="cb34-734"><a href="#cb34-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-735"><a href="#cb34-735" aria-hidden="true" tabindex="-1"></a>The cloud of regression lines displays greater uncertainty at extreme values for weight</span>
<span id="cb34-736"><a href="#cb34-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-737"><a href="#cb34-737" aria-hidden="true" tabindex="-1"></a>Notice that the cloud of regression lines grows more compact as the sample size increases. This is a result of the model growing more confident about the location of the mean. More confident about what mean? Does he mean 'more confident about the slope'?</span>
<span id="cb34-738"><a href="#cb34-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-739"><a href="#cb34-739" aria-hidden="true" tabindex="-1"></a>I think he means "more confident about the slope of the mean height in weight, as well as about the intercept ... thus more confident about the mean height for wach weight</span>
<span id="cb34-740"><a href="#cb34-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-741"><a href="#cb34-741" aria-hidden="true" tabindex="-1"></a>take your quap approximation, sample from the posterior distribution, and then compute - for each case in the data and sample from the posterior distribution. Here</span>
<span id="cb34-742"><a href="#cb34-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-743"><a href="#cb34-743" aria-hidden="true" tabindex="-1"></a> We actually want something slightly different: a distribution of - for each unique weight value on the horizontal axis. It-s only slightly harder to compute that, by just passing link some new data:</span>
<span id="cb34-744"><a href="#cb34-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-745"><a href="#cb34-745" aria-hidden="true" tabindex="-1"></a>Read apply(mu,2,mean) as compute the mean of each column (dimension -2-) of the matrix mu. Nowmu.mean contains the average - at each weight value, and mu.PI contains 89% lower and upper bounds for each weight value</span>
<span id="cb34-746"><a href="#cb34-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-747"><a href="#cb34-747" aria-hidden="true" tabindex="-1"></a>true that it is possible to use analytical formulas to compute intervals like this</span>
<span id="cb34-748"><a href="#cb34-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-749"><a href="#cb34-749" aria-hidden="true" tabindex="-1"></a>approach, and there is some additional insight that comes from knowing the mathematics, the pseudo-empirical approach presented here is very flexible and allows a much broader audience of scientists to pull insight from their statistical modeling. And again, when you start estimating models with MCMC (Chapter 9), this is really the only approach available</span>
<span id="cb34-750"><a href="#cb34-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-751"><a href="#cb34-751" aria-hidden="true" tabindex="-1"></a>Use link to generate distributions of posterior values for -. The default behavior of link is to use the original data, so you have to pass it a list of new horizontal axis values you want to plot posterior predictions across. (2) Use summary functions like mean or PI to find averages and lower and upper bounds of - for each value of the predictor variable. (3) Finally, use plotting functions like lines and shade to draw the lines and intervals. Or you might plot the distributions of the predictions, or do further numerical calculations with them. It-s really up to you. This recipe works for every model we fit in the book Is there a comparable 'universal recipe' in the tidy "love letter" adaptation?</span>
<span id="cb34-752"><a href="#cb34-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-753"><a href="#cb34-753" aria-hidden="true" tabindex="-1"></a>generating an 89% prediction interval for actual heights, not just the average height, -. This means we-ll incorporate the standard deviation - and its uncertainty as well</span>
<span id="cb34-754"><a href="#cb34-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-755"><a href="#cb34-755" aria-hidden="true" tabindex="-1"></a>For any unique weight value, you sample from a Gaussian distribution with the correct mean - for that weight, using the correct value of - sampled from the same posterior distribution. what does he mean by 'correct' here?</span>
<span id="cb34-756"><a href="#cb34-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-757"><a href="#cb34-757" aria-hidden="true" tabindex="-1"></a>nulldo this for every sample from the posterior, for every weight value of interest, you end up with a collection of simulated heights that embody the uncertainty in the posterior as well as the uncertainty in the Gaussian distribution of heights. There is a tool called sim which does this:</span>
<span id="cb34-758"><a href="#cb34-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-759"><a href="#cb34-759" aria-hidden="true" tabindex="-1"></a>This matrix is much like the earlier one, mu, but it contains simulated heights A vector of simulated heights for each element in the weight sequence</span>
<span id="cb34-760"><a href="#cb34-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-761"><a href="#cb34-761" aria-hidden="true" tabindex="-1"></a>You could plot the boundary for other percents, such as 67% and 97% (also both primes), and add those to the plot. it would be nice to plot several of these together, perhaps a gradual distribution/elevation plot of confidence</span>
<span id="cb34-762"><a href="#cb34-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-763"><a href="#cb34-763" aria-hidden="true" tabindex="-1"></a>For every distribution like dnorm, there is a companion simulation function dnorm -- species the density at any point (I guess) rnorm -- randomly generates frmo the normal distribution</span>
<span id="cb34-764"><a href="#cb34-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-765"><a href="#cb34-765" aria-hidden="true" tabindex="-1"></a><span class="fu">## Curves from lines (4.5)</span></span>
<span id="cb34-766"><a href="#cb34-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-767"><a href="#cb34-767" aria-hidden="true" tabindex="-1"></a>The first is polynomial regression. The second is b-splines I need to know more about how to use the splines</span>
<span id="cb34-768"><a href="#cb34-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-769"><a href="#cb34-769" aria-hidden="true" tabindex="-1"></a>But in general it is better to pre-process any variable transformations-you don-t need the computer to recalculate the transformations on every iteration of the fitting procedure</span>
<span id="cb34-770"><a href="#cb34-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-771"><a href="#cb34-771" aria-hidden="true" tabindex="-1"></a>The parameter - (a) is still the intercept, so it tells us the expected value of height when weight is at its mean value. But it is no longer equal to the mean height in the sample, since there is no guarantee it should in a polynomial regression</span>
<span id="cb34-772"><a href="#cb34-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-773"><a href="#cb34-773" aria-hidden="true" tabindex="-1"></a>We aren-t learning any causal relationship between height and weight</span>
<span id="cb34-774"><a href="#cb34-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-775"><a href="#cb34-775" aria-hidden="true" tabindex="-1"></a>B-splines do not directly transform the predictor by squaring or cubing it. Instead they invent a series of entirely new, synthetic predictor variables. Each of these synthetic variables exists only to gradually turn a specific parameter on and off within a specific range of the real predictor variable. Each of the synthetic variables is called a basis function</span>
<span id="cb34-776"><a href="#cb34-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-777"><a href="#cb34-777" aria-hidden="true" tabindex="-1"></a>The linear model ends up looking very familiar: -i = - + w1Bi,1 + w2Bi,2 + w3Bi,3 + ... where Bi,n is the n-th basis function-s value on row i, and the w parameters are corresponding weights for each</span>
<span id="cb34-778"><a href="#cb34-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-779"><a href="#cb34-779" aria-hidden="true" tabindex="-1"></a>divide the full range of the horizontal axis into four parts, using pivot points called knots. The</span>
<span id="cb34-780"><a href="#cb34-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-781"><a href="#cb34-781" aria-hidden="true" tabindex="-1"></a>These synthetic variables are used to gently transition from one region of the horizontal axis to the next. Essentially, these variables tell you which knot you are close to. Beginning on the left of the top plot, basis function 1 has value 1 and all of the others are set to zero. As we move rightwards towards the second knot, basis 1 declines and basis 2 increases. At knot 2, basis 2 has value 1, and all of the others are set to zero</span>
<span id="cb34-782"><a href="#cb34-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-783"><a href="#cb34-783" aria-hidden="true" tabindex="-1"></a>they make the influence of each parameter quite local. At any point on the horizontal axis in Figure 4.12, only two basis functions have non-zero values</span>
<span id="cb34-784"><a href="#cb34-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-785"><a href="#cb34-785" aria-hidden="true" tabindex="-1"></a>Parameters called weights multiply the basis functions. The spline at any given point is the sum of these weighted basis functions</span>
<span id="cb34-786"><a href="#cb34-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-787"><a href="#cb34-787" aria-hidden="true" tabindex="-1"></a>the knots are just values of year that serve as pivots for our spline. Where should the knots go?</span>
<span id="cb34-788"><a href="#cb34-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-789"><a href="#cb34-789" aria-hidden="true" tabindex="-1"></a>simple example above, place the knots at different evenlyspaced quantiles of the predictor variable. This gives you more knots where there are more observations. We</span>
<span id="cb34-790"><a href="#cb34-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-791"><a href="#cb34-791" aria-hidden="true" tabindex="-1"></a>next choice is polynomial degree. This determines how basis functions combine, which determines how the parameters interact to produce the spline not fully explained</span>
<span id="cb34-792"><a href="#cb34-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-793"><a href="#cb34-793" aria-hidden="true" tabindex="-1"></a>the w priors influence how wiggly the spline can be</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>