[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RP: Methods",
    "section": "",
    "text": "As always, focus on ‘what repeatedly comes up in our work’, linking the practical cases↩︎"
  },
  {
    "objectID": "chapters/rp_typical_examples_data.html",
    "href": "chapters/rp_typical_examples_data.html",
    "title": "2  RP Surveys+: Ex. projects/q’s/data",
    "section": "",
    "text": "In this section we aim to outline a set of projects and methodologically-relevant questions that typify the work of Rethink Priority’s Survey and Movement Building Team1, as well as the work of adjacent projects like the EA Market Testing Team.\nWe will also extract, reference, and outline some example data, to use in the chapters below.\n9 Dec 2022: So far, the chapters below draw from the EA Survey data. However, as that data itself cannot be in the public domain, this is less useful for people who don’t have access to our Github repo, and less than ideal in general.\nSome sources of data and examples could include:"
  },
  {
    "objectID": "chapters/rp_typical_examples_data.html#some-commontypical-cases-and-problems",
    "href": "chapters/rp_typical_examples_data.html#some-commontypical-cases-and-problems",
    "title": "2  RP Surveys+: Ex. projects/q’s/data",
    "section": "\n2.1 Some common/typical cases and problems",
    "text": "2.1 Some common/typical cases and problems\nCommunity Survey omnibus\nTypical example: EA Survey3\nResearch questions:\n\nCurrent community composition (e.g., ‘what share are non-male’)\nActions taken and level of involvement\nRetrospective ‘how did you get involved?’\nRelative preferences and attitudes (e.g., longtermist or non-longtermist)\nChanges in all of this over time\nActions vs intentions\nIssue-oriented survey of the population\nTypical examples:\n\nEA Pulse,\nsurveys of awareness of EA;\nattitudes towards long-termism;\n\npolling on animal welfare legislation, etc.\n\nResearch questions:\n\nAwareness and knowledge of a subject\nAttitude to an issue\nPotential voting choices and other behavior\nDifferences across personality, demographic traits, etc.\nClustering, dimension reduction, factor analysis: How do responses ‘cohere’\nDifferences over time, particularly in response to campaigns (before-after)\n\nData may include:\n\nOrdered Likert responses and indices\nOpen-response\nHypothetical choices among a set of optinos\nBinary, continuous, other\nNaturally occuring behavioral data (signups, donations, etc)\nTypical examples:\n\nAll fundraisers for effective charities started on JustGiving during a certain period, with donation amounts\n\nThis data is shared in the current github repo, and read in with code below 4\n\nread in Just Giving datalibrary(here)\nlibrary(readr)\n\njg_eff_sample <- readRDS(here::here(\"sample_data\", \"fdd_fd_sample_eff.rds\"))\n\n\n\nMeat and animal product consumption data5\nA time series6 of signups for 80,000 hours newsletter, particularly considering the difference before and after a report in the Guardian newspaper\n\nWe share\nMarketing trials/experiments and surveys\nThis is distinct from the ‘naturally occuring’ data just mentioned, because we are intervening to ‘do or test something’ and measure the response.\nTypical examples:\n\nEA Market Testing comparison of GWWC Facebook ads to encourage email signup\nA Prolific survey to helping an EA filmmaker choose a movie title and blurb7\n\nAn effective charity wants to know how to present their impact information in mail appeals, if at all. They may test this with an actual set of appeals (field experiment, aka ‘split test’).\n\nResearch questions and goals:\n\nBaseline response rates and cost per outcome\nFind ‘best approach within a large space’ (reinforcement learning)\nTest different categories of approaches in a limited set (‘which works better’)\nFind most promising audiences (profiling)\nInteraction (tailored messaging, etc.)\n\nData could include\n\nAttitudinal (Likert) response data\nSuggestive outcome data (e.g., ‘clicks on the web site’)\nActual ‘A/B testing’ data for outcomes and responses of interest"
  },
  {
    "objectID": "chapters/coding_data.html",
    "href": "chapters/coding_data.html",
    "title": "3  Coding, data",
    "section": "",
    "text": "Codelibrary(pacman)\np_load(rethinkpriorities)\nlibrary(rethinkpriorities)\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(tidyverse)"
  },
  {
    "objectID": "chapters/coding_data.html#coding-and-organisational-issues",
    "href": "chapters/coding_data.html#coding-and-organisational-issues",
    "title": "3  Coding, data",
    "section": "\n3.1 Coding and organisational issues",
    "text": "3.1 Coding and organisational issues\n\nData protection (e.g., EA Survey data pre-2021 is not publicly shareable!)\nGood data management\nReproducability\nGit and github\ntrackdown to convert to Gdoc for feedback\nFolder structure, use of packages; esp Renv\nFunctions etc pulled from dr-rstuff repo\nI (DR) love lower_snake_case"
  },
  {
    "objectID": "chapters/coding_data.html#automation-and-dynamic-documents-qmd-etc.",
    "href": "chapters/coding_data.html#automation-and-dynamic-documents-qmd-etc.",
    "title": "3  Coding, data",
    "section": "Automation and ‘dynamic documents’ (qmd etc.)",
    "text": "Automation and ‘dynamic documents’ (qmd etc.)\nSee, e.g., quarto.org and reinstein quarto template\nHow to leave comments and collaborate? - Easier if hosted, use Netlify for private hosting - Then use hypothes.is comments\nAlternatives on Github are a bit workaroundy\n\nBut I just want to see the code\nAlways make a ‘just the code’ version of the file with knitr::purl(here(“filename.qmd”))\n\n3.1.1 Inline code and soft-coding\n‘Soft-code’ as much as possible to avoid conflicting versions when data updates, and to make everything reproduceable and transparent\nInline code in Rmd/qmd is great but it can be a double-edged sword.\nSometimes its better to ‘do the important and complicated coding’ in a chunk before this, not in the inline code itself because\n\nthe ‘bookdown’ doesn’t show the code generating the inline computation … so a separate chunk makes it more transparent for external readers\ninline code isn’t spaced well and its hard to read and debug."
  },
  {
    "objectID": "chapters/coding_data.html#data-management",
    "href": "chapters/coding_data.html#data-management",
    "title": "3  Coding, data",
    "section": "\n3.2 Data management",
    "text": "3.2 Data management\n\nTrack it from its ‘source’; use API to grab directly from Qualtrics (etc.) if possible\nA main.R file in the root directory should run everything\nData import; external ‘dictionary’ can be helpful (see, e.g., here for EAS integrated with Google sheet; R code here brings it in\n\nimport, cleaning, variable creation separate from analysis (unless its a very ‘one-off-for-analysis’ thing)\n\nimport and cleaning in .R rather than .Rmd or qmd perhaps\n\n\n‘raw’ data in separate folder from ‘munged’ data\ncodebook package – make a codebook\n\nminimize ‘versions’ of the data frames … code and use ‘filter objects’ instead\n\nsee ‘lists of filters’ but actually defining the filter with quo() seems better."
  },
  {
    "objectID": "chapters/coding_data.html#standard-cleaning-steps",
    "href": "chapters/coding_data.html#standard-cleaning-steps",
    "title": "3  Coding, data",
    "section": "\n3.3 Standard cleaning steps",
    "text": "3.3 Standard cleaning steps\njanitor::remove_empty() # removes empty rows and columns"
  },
  {
    "objectID": "chapters/coding_data.html#naming-columns-and-objects",
    "href": "chapters/coding_data.html#naming-columns-and-objects",
    "title": "3  Coding, data",
    "section": "\n3.4 Naming columns and objects",
    "text": "3.4 Naming columns and objects\njanitor::clean_names() is a helpful shortcut to snake case\nWe sometimes input a ‘dictionary’ for renaming many columns at a time. 2\nnames(rename2020) <- eas_dictionary$true_name"
  },
  {
    "objectID": "chapters/coding_data.html#labelling-columns",
    "href": "chapters/coding_data.html#labelling-columns",
    "title": "3  Coding, data",
    "section": "\n3.5 Labelling columns",
    "text": "3.5 Labelling columns\nSome example code below\nPut list of labels and renamings in objects in a separate location … to avoid duplication and clutter:\n\nCodekey_eas_all_labels <- c( #note these are converted to a list with as.list before assigning them\n    donation_usd = \"Donation (USD)\",\n    l_don_usd = \"Log Don. (USD)\",\n    l_don_av_2yr = \"Log Don. 'avg.'\",\n    ln_age = \"Log age\",\n    don_av2_yr = \"Don. 'avg'\",\n    donation_plan_usd = \"Don. plan (USD)\")\n\n\nVariable labels are helpful\n\nCodeeas_all <-  eas_all %>% \n  labelled::set_variable_labels(.labels = as.list(key_eas_all_labels), .strict=FALSE)\n\nError in as.vector(y): object 'eas_all' not found"
  },
  {
    "objectID": "chapters/coding_data.html#naming-lists-of-columns-for-input-into-models",
    "href": "chapters/coding_data.html#naming-lists-of-columns-for-input-into-models",
    "title": "3  Coding, data",
    "section": "\n3.6 Naming lists of columns for input into models",
    "text": "3.6 Naming lists of columns for input into models\nSee vignette on ‘modeling workflow’ for some examples"
  },
  {
    "objectID": "chapters/coding_data.html#simple-summary-tools-i-was-not-aware-of",
    "href": "chapters/coding_data.html#simple-summary-tools-i-was-not-aware-of",
    "title": "3  Coding, data",
    "section": "\n3.7 Simple summary tools I was not aware of",
    "text": "3.7 Simple summary tools I was not aware of\nFrom Willem’s intro to R workshop script\n\ncount, count_data, describe_datadiamonds %>%\n count(cut) %>%\n mutate(pct = n / sum(n))\n\n# A tibble: 5 × 3\n  cut           n    pct\n  <ord>     <int>  <dbl>\n1 Fair       1610 0.0298\n2 Good       4906 0.0910\n3 Very Good 12082 0.224 \n4 Premium   13791 0.256 \n5 Ideal     21551 0.400 \n\ncount, count_data, describe_data# Use tidystats\nlibrary(tidystats)\n\ncount_data(diamonds, cut)\n\n# A tibble: 5 × 3\n  cut           n   pct\n  <ord>     <int> <dbl>\n1 Fair       1610  2.98\n2 Good       4906  9.10\n3 Very Good 12082 22.4 \n4 Premium   13791 25.6 \n5 Ideal     21551 40.0 \n\ncount, count_data, describe_datadiamonds %>%\n group_by(color) %>%\n count_data(cut)\n\n# A tibble: 35 × 4\n# Groups:   color [7]\n   color cut           n   pct\n   <ord> <ord>     <int> <dbl>\n 1 D     Fair        163  2.41\n 2 D     Good        662  9.77\n 3 D     Very Good  1513 22.3 \n 4 D     Premium    1603 23.7 \n 5 D     Ideal      2834 41.8 \n 6 E     Fair        224  2.29\n 7 E     Good        933  9.52\n 8 E     Very Good  2400 24.5 \n 9 E     Premium    2337 23.9 \n10 E     Ideal      3903 39.8 \n# … with 25 more rows\n\ncount, count_data, describe_datadescribe_data(diamonds, price)\n\n# A tibble: 1 × 13\n  var   missing     N     M    SD    SE   min   max range median  mode  skew\n  <chr>   <int> <int> <dbl> <dbl> <dbl> <int> <int> <int>  <dbl> <int> <dbl>\n1 price       0 53940 3933. 3989.  17.2   326 18823 18497   2401   605  1.62\n# … with 1 more variable: kurtosis <dbl>\n\n\nThis one I knew, of course, the typical ‘grouped summaries’\n\nCodediamonds %>%\n group_by(color) %>%\n summarize(\n   M = mean(price),\n   SD = sd(price),\n   min = min(price)\n ) %>%\n .kable() %>% .kable_styling()\n\n\n\n\n color \n    M \n    SD \n    min \n  \n\n\n D \n    3,169.954 \n    3,356.591 \n    357 \n  \n\n E \n    3,076.752 \n    3,344.159 \n    326 \n  \n\n F \n    3,724.886 \n    3,784.992 \n    342 \n  \n\n G \n    3,999.136 \n    4,051.103 \n    354 \n  \n\n H \n    4,486.669 \n    4,215.944 \n    337 \n  \n\n I \n    5,091.875 \n    4,722.388 \n    334 \n  \n\n J \n    5,323.818 \n    4,438.187 \n    335"
  },
  {
    "objectID": "chapters/presentation_method_discussion.html",
    "href": "chapters/presentation_method_discussion.html",
    "title": "4  Presentation, data-viz",
    "section": "",
    "text": "Note: this has been moved from a discussion of the EA survey in the ea-data repo, and is being expanded here\nNOTE: this is being moved to our ‘how to visualize’ … more still needs to be moved"
  },
  {
    "objectID": "chapters/presentation_method_discussion.html#tables",
    "href": "chapters/presentation_method_discussion.html#tables",
    "title": "4  Presentation, data-viz",
    "section": "\n4.1 Tables",
    "text": "4.1 Tables\nData tables, so users can sort view them\n\nCode(\nmtcars_data_table <-  mtcars %>%\n mutate(across(is.numeric, round, digits = 2)) %>%\n  DT::datatable(class = 'cell-border stripe')\n)"
  },
  {
    "objectID": "chapters/presentation_method_discussion.html#aside-ea-forum-organisation-collophon-formatting",
    "href": "chapters/presentation_method_discussion.html#aside-ea-forum-organisation-collophon-formatting",
    "title": "4  Presentation, data-viz",
    "section": "\n4.2 Aside: EA Forum organisation, collophon, formatting",
    "text": "4.2 Aside: EA Forum organisation, collophon, formatting"
  },
  {
    "objectID": "chapters/presentation_method_discussion.html#visualizations",
    "href": "chapters/presentation_method_discussion.html#visualizations",
    "title": "4  Presentation, data-viz",
    "section": "\n4.3 Visualizations",
    "text": "4.3 Visualizations\nMOVED to how-to-visualize\nSee also Gdoc: visualisation discussions\nIdeal coding practice for visualisations\nMOVED much content to how-to-visualize\nWill move the below over as well\nCase: Relative shares of a total; e.g. ‘shares of EA population by referrer’, or ‘donation totals by country’\nTreemaps\n\nNOTE: The below will not run without data, but we cannot make data ‘public’ here … we can swap this for built in R or simulated data in this discussion\n\n\nCodegeom_treemap_opts <- list(geom_treemap(alpha = 0.7),\n  geom_treemap_text(fontface = \"italic\", colour = \"white\", place = \"centre\",\n                    grow = TRUE, min.size = 1 ),\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5))\n  )\n\n(\n  don_share_by_size <- eas_20 %>% select(donation_2019_c, donation_2019_c_split) %>%\n  group_by(donation_2019_c_split) %>%\n  summarise(total_don = sum(donation_2019_c, na.rm=TRUE)) %>%\n  mutate(don_share = round(total_don/sum(total_don)*100)) %>%\n  filter(!is.na(donation_2019_c_split)) %>%\n  ggplot(aes(area = total_don, fill= donation_2019_c_split,\n             # Include percentage of total donation\n             label = paste(donation_2019_c_split, paste0(don_share, \"%\"), sep = \"\\n\"))) +\n    geom_treemap_opts +\n  ggtitle(\"Share of total 2019 reported donation amount, by donation size\")\n)\n\n\n\n\nA good alternative to the treemap … conveying both absolute amounts and rates in the same graph?\nCase: Likert scale (individual items) and relative responses to these\nThe combination of stacked bar charts and overlayed error/CI bars for some ‘middle split’ seems informative:\n\nCodeknitr::include_url(\"https://rethinkpriorities.github.io/ea_data_public/eas-engagement.html#summary-charts\")\n\n\n\nAbove: from EAS engagement post, bookdown supplement\nIssue: Dealing with scales and large differences in magnitudes\nIf axes are cut and don’t go to zero, we should mention this prominently in the plot.1\nBut what if we have a few values that are way above others, and ‘mess up the scale’?\nSometimes a logarithmic scale is helpful, but that can also lead to confusion. Otherwise…\n\n\n“any CI’s or other things that go outside of the limits should just be extended to the edge and not dropped)”\n\ndoable with scale_y_continuous( oob = scales::squish) + scale_x_continuous( oob = scales::squish) or coord_cartesian\n\n\n\nor otherwise allow a clear ‘break’ in the axis?\nNot possible to break the axis ggplot https://stackoverflow.com/questions/7194688/using-ggplot2-can-i-insert-a-break-in-the-axis\n… what about ‘extending to the edge’?\n\nRemove outliers as a last resort; prominently mention this where doing so.\nCase: Means of continuous variables: differences across groups\nSome (imperfect) examples from donation post:\n\n\n\nVertical lines for ‘medians and means’ help put this in context\nCase: Categorical outcomes by continuous outcomes (and vice/versa)\n\nCodeknitr::include_url(\"https://rethinkpriorities.github.io/ea_data_public/eas-engagement.html#engagement-by-age-and-tenure\")\n\n\n\nAbove, from the EAS engagement post bookdown, we combine a geom_point with a geom_smooth.2\n\n\n\n\n\n\nggplotly pros and cons\n\n\n\n\n\nHere, I think the use of %>% ggplotly yields only one helpful feature (but it’s very helpful): hovering over a dot or point on the line brings up an exact value label. I think this helps a reader understand ‘what the heck this is’. On the other hand, Plotly makes it easy to distort the sizes of the graphs in inconvenient ways, and using ggplotly can be incompatible with certain other features.\n\n\n\nCase: Visualizing (‘regression’) model coefficients and confidence intervals\nHow should we report the result of a predictive, descriptive, or causal model (or set of models) in a digestable way? Consider ‘Forest plots’ …\nWith binary/categorical features (or binned continuous features), these can be useful for showing\n\n‘model predictions’ for different groups and subsets,\nor estimates for ‘differences between these groups’,\n\nalong with the uncertainty of these.\nWith continuous features, we could either : - make predictions for bins or quantiles of the features (ages 18-35, 36-55, etc), even if these are entered into the model as continuous variables.. - report on the relevant ‘slopes’ or ‘effects’3\nNote: In reporting slopes for both continuous and categorical features, Gelman recommends a particular standardization of the former, to put these on the same scale.\nWe run four models below.\n\nCode# A simple (but nonlinear) model\ndon_2020_qp <- \neas_all %>% \n  glm(donation_2020_c ~ log(income_c_imp)  + log(2020-year_involved_n +1)   + d_student, family=quasipoisson, data =.)\n\n#note: this seems to drop people involved for less than 1 year\n\n#As in the first model, but for nonstudents only'\ndon_2020_ns_qp <- eas_all %>% \n  filter(!d_student) %>% \n  glm(donation_2020_c ~ log(income_c_imp)  + log(2020-year_involved_n +1)  , family=quasipoisson, data =.)\n\n# similar model but with first-heard lumped 'feature'\ndon_2020_fh_qp <- \neas_all %>% \n  glm(donation_2020_c ~ log(income_c_imp)  + log(2020-year_involved_n +1)   + d_student + first_hear_ea_lump, family=quasipoisson, data =.)\n\n\n# first-heard lumped 'feature' and referrer control\ndon_2020_fh_ref_qp <- \neas_all %>% \n  glm(donation_2020_c ~ log(income_c_imp)  + log(years_involved_norm)  + d_student + first_hear_ea_lump + referrer_cat, family=quasipoisson, data =.)\n\n\n#here we are predicting a different outcome ('share of income'); so we cannot the compare predictions to those above easily, only slopes, if normalized perhaps\n# don_share_2020_ns_qp <- eas_all %>%  \n#   glm(don_share_inc_19_imp ~ log(income_c_imp)  + log(years_involved_norm)  + d_live_usa, family = quasibinomial('logit'), data = .)\n\nhuxtable::huxreg(\"Simple\" = don_2020_qp,\n  \"Nonstudents\" = don_2020_ns_qp,\n  \"First-hear\" = don_2020_fh_qp, \n  \"First-hear and referrer\" = don_2020_fh_ref_qp,\n  omit_coefs = c(\"(Intercept)\"),\n        statistics = c(\"N. obs.\" = \"nobs\"),\n  tidy_args = list(exponentiate = TRUE), \n       error_format = \"[{conf.low}-{conf.high}]\", ci_level = 0.95\n  ) %>%\n  set_caption(\"Quasi-poisson models of 2020 donation amounts\") %>%\n  set_caption_pos(\"top\") %>% \n  rethinkpriorities::huxreg_opts() \n\n\n\nQuasi-poisson models of 2020 donation amounts\n\n\n\n\n\n\n\nSimple\nNonstudents\nFirst-hear\nFirst-hear and referrer\n\n\nlog(income_c_imp)\n3.549 ***\n3.581 ***\n3.437 ***\n3.731 ***\n\n\n\n[3.196-3.938]   \n[3.144-4.078]   \n[3.088-3.836]   \n[3.150-4.476]   \n\n\nlog(2020 - year_involved_n + 1)\n1.818 ***\n1.825 ***\n1.890 ***\n        \n\n\n\n[1.429-2.350]   \n[1.345-2.538]   \n[1.501-2.407]   \n        \n\n\nd_student\n0.836    \n        \n0.781    \n0.749    \n\n\n\n[0.394-1.576]   \n        \n[0.405-1.377]   \n[0.241-1.801]   \n\n\nfirst_hear_ea_lump80,000 Hours\n        \n        \n0.485    \n0.473    \n\n\n\n        \n        \n[0.219-0.948]   \n[0.126-1.290]   \n\n\nfirst_hear_ea_lumpBook, article, or blog post\n        \n        \n0.887    \n0.672    \n\n\n\n        \n        \n[0.598-1.316]   \n[0.380-1.178]   \n\n\nfirst_hear_ea_lumpEducational course\n        \n        \n0.466    \n0.187    \n\n\n\n        \n        \n[0.049-1.760]   \n[0.001-1.503]   \n\n\nfirst_hear_ea_lumpGiveWell\n        \n        \n0.713    \n0.704    \n\n\n\n        \n        \n[0.424-1.164]   \n[0.359-1.319]   \n\n\nfirst_hear_ea_lumpGiving What We Can\n        \n        \n1.047    \n0.718    \n\n\n\n        \n        \n[0.426-2.190]   \n[0.255-1.674]   \n\n\nfirst_hear_ea_lumpLessWrong\n        \n        \n0.318 ***\n0.269 ***\n\n\n\n        \n        \n[0.164-0.570]   \n[0.124-0.533]   \n\n\nfirst_hear_ea_lumpLocal or university EA group\n        \n        \n1.261    \n1.178    \n\n\n\n        \n        \n[0.760-2.036]   \n[0.614-2.202]   \n\n\nfirst_hear_ea_lumpPersonal contact\n        \n        \n0.921    \n1.072    \n\n\n\n        \n        \n[0.596-1.403]   \n[0.611-1.849]   \n\n\nfirst_hear_ea_lumpPodcast\n        \n        \n0.889    \n0.356    \n\n\n\n        \n        \n[0.450-1.628]   \n[0.056-1.214]   \n\n\nfirst_hear_ea_lumpSlate Star Codex\n        \n        \n0.441 *  \n0.751    \n\n\n\n        \n        \n[0.197-0.864]   \n[0.230-1.980]   \n\n\nfirst_hear_ea_lumpTED talk\n        \n        \n1.018    \n1.075    \n\n\n\n        \n        \n[0.370-2.261]   \n[0.254-3.091]   \n\n\nlog(years_involved_norm)\n        \n        \n        \n2.125 ***\n\n\n\n        \n        \n        \n[1.587-2.877]   \n\n\nreferrer_catEA Forum\n        \n        \n        \n0.332 ***\n\n\n\n        \n        \n        \n[0.189-0.591]   \n\n\nreferrer_catEA Newsletter\n        \n        \n        \n0.936    \n\n\n\n        \n        \n        \n[0.531-1.664]   \n\n\nreferrer_catEmail; opt-in from prev. EAS\n        \n        \n        \n0.250 ***\n\n\n\n        \n        \n        \n[0.125-0.484]   \n\n\nreferrer_catGiving What We Can\n        \n        \n        \n0.309 *  \n\n\n\n        \n        \n        \n[0.082-0.854]   \n\n\nreferrer_catGroups (local)\n        \n        \n        \n0.341 ** \n\n\n\n        \n        \n        \n[0.172-0.664]   \n\n\nreferrer_catLess Wrong or SlateStarCodex-Reddit\n        \n        \n        \n0.095 ***\n\n\n\n        \n        \n        \n[0.029-0.267]   \n\n\nreferrer_catReddit\n        \n        \n        \n0.093 ** \n\n\n\n        \n        \n        \n[0.014-0.337]   \n\n\nreferrer_catShared link\n        \n        \n        \n0.229 ** \n\n\n\n        \n        \n        \n[0.078-0.557]   \n\n\nreferrer_catSocial media (FB/memes)\n        \n        \n        \n0.264    \n\n\n\n        \n        \n        \n[0.025-1.173]   \n\n\nreferrer_catOther\n        \n        \n        \n0.289    \n\n\n\n        \n        \n        \n[0.000-3.258]   \n\n\nN. obs.\n1380        \n892        \n1378        \n564        \n\n *** p < 0.001;  ** p < 0.01;  * p < 0.05.\n\n\n\nAbove, we see a bunch of coefficients from each model.4 This model is nonlinear; we exponentiate the coefficients and report the confidence intervals. I believe these can be interpreted as proportional differentials (‘impacts’ if causal).5\nNext, we want to plot predictions; perhaps we simply want to compare the donations of the different groups (“everything else equal”) according to where they ‘first heard of EA’. To do this for a single model, we need to extract the model predictions, converting them to a data frame, and then plot these with ggplot.\nHow can we extract the predictions? It’s easier if we first make these model objects into tidy tibbles using broom.\nFirst 1. generate a list of ‘named model names’,\n\nCodemodels_list <- list(\"Base\" = don_2020_qp, \"Nonstudent\" = don_2020_ns_qp, \"First heard\" = don_2020_fh_qp, \"First heard, Referrer\" = don_2020_fh_ref_qp) #do this across a list, to save lines of code and have better control\n\n\n\n\n\n\n\n\nThis approach didn’t seem to work here\n\n\n\n\n\nExtract the predicted values and standard errors from each model, appending it to the data with broom::augment. This gives us a list of 4 tibbles of this, 1 for each model.\n\nCode# extract the predicted values and standard errors from each model, as a list\n\nlibrary(broom)\n\naugmented_models <-  purrr::map(models_list, ~augment(.x))\n\n\nBut to plot this, we want to …\n\nMake it a single data frame, differentiating by model (add a column to the dataframe that contains the model name)\n\n\nCode \n# convert the list of predicted values to a data frame\n\naugmented_models_df <- bind_rows(augmented_models, .id = \"model\") \n\nError: <text>:1:1: unexpected input\n1:  \n    ^\n\n\n\nNow we can ggplot\n\n\nCodeaugmented_models_df %>% \nggplot(data =., aes(x = model, y = .fitted, fill = model)) +\n    geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +\n    scale_y_continuous(limits = c(0, 1))+ \n    coord_flip()\n\nError in ggplot(data = ., aes(x = model, y = .fitted, fill = model)): object 'augmented_models_df' not found\n\n\nNo, that’s not what we wanted. This seems to be giving us something about overall predictions for each model, scaled on some normalized axis (quantiles of outcome?).\n\n\n\nWe want plots for the predicted donations for each of the first heard (FH) categories, e.g., first heard from Less Wrong. But what exactly does that mean? We do not want the average prediction for a person who first heard of EA from LW; this person may tend to have ‘non-average’ values of the other features, such as income and time in EA.\nDo we simply want the ‘predicted value for a person coming from LW with ’average values of the other features’? With a linear model this is probably what we want, but our model is nonlinear. What we want is something like the average over all observations for their predicted donation if they had first heard of EA from LW. We can then compare it to the prediction for the same survey population ‘if they had first heard from GiveWell’, etc.\nThese are called the Estimated Marginal Means. I believe the ‘estimated marginal means’ package emmmeans gets us this.\nWhat about the confidence intervals or other bounds? We are not focused on the bounds of the predicted donations for an individual from the (hypothetical) population discussed above, but instead, on the bounds on the mean donation for this population. We want to state bounds with a procedure such that the true value is captured, say, 95% of the time. We have analytic (or other valid) procedures that tell us the bounds on the relevant model parameters (or ‘coefficients’). But we are stating this in terms of predicted mean outcomes. One approach might be to simply apply these 95% bound coefficients to the data, again, averaged over all observations for all the other features.\n\nCodelibrary(emmeans)\n\ncounts <- c(18, 17, 15, 20, 10, 20, 25, 13, 12)\nteam <- gl(3, 1, 9, labels  = c(\"Boston\", \"NY\", \"Texas\"))\ntreatment <- gl(3, 3, labels  = c(\"low\", \"medium\", \"hi\"))\nskill <- runif(9)\n\ndata <- tibble(treatment, team, counts, skill)\nmodel <- glm(counts ~ team + treatment, family = poisson(), data = data)\n#summary(model)\n\nmodel2 <- glm(counts ~ team + skill, family = poisson(), data = data)\n\nemm <- emmeans(\n  model,\n  specs = \"team\",\n  type = \"response\"\n)\n\nemm2 <- emmeans(\n  model2,\n  specs = \"team\",\n  type = \"response\"\n)\n\nemm2_cont <- emmeans(\n  model2,\n  specs = \"skill\",\n  at = list(skill = c(0.25, 0.75))\n  ,\n  type = \"response\",\n) %>% \n  as_tibble() \n\ndf <- bind_rows(\n  list(\"base\" = as_tibble(emm),\n  \"multi\" = as_tibble(emm2),\n    \"multi\" = emm2_cont),\n  .id = \"modelnum\",\n) %>% \n  mutate(\n    skill = paste(\"Skill: \", as.character(skill)), \n    group= as.factor(coalesce(team, skill)),\n    ordering = as.numeric(is.na(team))*1000+rate,\n    group= fct_reorder(group, ordering, .desc = T),\n  ) %>% \n      select(-ordering)\n\n\nggplot(df, aes(x = group, y = rate, color = modelnum)) +\n  geom_pointrange(\n    aes(ymin = asymp.LCL, ymax = asymp.UCL),\n    position = position_dodge(.5)\n  ) +\n  geom_point(position = position_dodge(.5)) +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\nExample from fancier code… adapt in?\n\n\n\n\n\nForest plots as seen in donation post here\n\nOur group_fp_do function makes this easy to do.\nSee how we group the coefficients into plots by ‘theme’\n\nIn some contexts, lines for ‘base rates’ can be helpful\n … context, scroll up from here"
  },
  {
    "objectID": "chapters/survey_designs_methods.html",
    "href": "chapters/survey_designs_methods.html",
    "title": "5  Survey design, item dev.",
    "section": "",
    "text": "This section will cover specific ‘survey’ related content, as well as some content that overlaps with experiments and trials.^M[ore technical and involved discussion of overlapping content will often be deferred to the ‘experiments’ related sections, with placeholders and links.]"
  },
  {
    "objectID": "chapters/survey_designs_methods.html#some-key-resources-and-things-to-incorporte",
    "href": "chapters/survey_designs_methods.html#some-key-resources-and-things-to-incorporte",
    "title": "5  Survey design, item dev.",
    "section": "\n5.1 Some key resources and things to incorporte",
    "text": "5.1 Some key resources and things to incorporte\n\nKatja Grace, ‘things I believe after making some surveys’\nLink or incorporate William Elsey’s work HERE,\nand Reinstein’s sloppy notes here focusing on probability sampling and sampling rare populations; see embeds below\n\n:::"
  },
  {
    "objectID": "chapters/survey_designs_methods.html#willem-sleegers-to-incorporate",
    "href": "chapters/survey_designs_methods.html#willem-sleegers-to-incorporate",
    "title": "5  Survey design, item dev.",
    "section": "\n5.2 (Willem Sleegers, to incorporate)",
    "text": "5.2 (Willem Sleegers, to incorporate)\nitem development\nwayback link\n\nCodeknitr::include_url(\"https://willemsleegers.github.io/how-to-science/content/methodology/survey-design/item-development.html\")"
  },
  {
    "objectID": "chapters/survey_designs_methods.html#reinstein-misc-notes-to-incorporate",
    "href": "chapters/survey_designs_methods.html#reinstein-misc-notes-to-incorporate",
    "title": "5  Survey design, item dev.",
    "section": "Reinstein misc notes to incorporate",
    "text": "Reinstein misc notes to incorporate\n\nCodeknitr::include_url(\"https://daaronr.github.io/metrics_discussion/surveys.html\")"
  },
  {
    "objectID": "chapters/survey_designs_methods.html#suggested-sections",
    "href": "chapters/survey_designs_methods.html#suggested-sections",
    "title": "5  Survey design, item dev.",
    "section": "\n5.3 Suggested sections…",
    "text": "5.3 Suggested sections…\n‘Qualitative’ issues (hopefully backed by evidence)\n\nHow to ask good survey questions\nAvoiding pitfalls (framing, agreeability bias, comprehension, attention, etc.)\nValidating the measure is capturing ‘the thing you intend’\n\n\n‘Quantitative issues’\n\nConstructing reliable indices and scales\n\nSampling issues and representativeness\n\nElsey on “Mr P”\nReinstein on ‘sampling’ and ‘measuring a rare population’ with reference to the EA survey"
  },
  {
    "objectID": "chapters/survey_designs_methods.html#discussions-to-integrate",
    "href": "chapters/survey_designs_methods.html#discussions-to-integrate",
    "title": "5  Survey design, item dev.",
    "section": "\n5.4 Discussions to integrate",
    "text": "5.4 Discussions to integrate\nGoogle discussion doc\nFraming effects Disputing claim that “One doesn’t need to worry about framing effects” because of Morewedge et al, 2015; Khan et al, 2006; Baumer et al, 2015 private slack conversation link\nOrdering attitude measures In asking a series of attitude measures that have a natural order, present them in that order or randomize (e.g., here https://docs.google.com/document/d/16lmVZu9sjoHLceh8Zdgs8A_n4sxmZudahn7825ctRW4/edit#heading=h.gwnnifgo8qd) … I found it interesting, I am enthusiastic, I would definitely sign up, etc.)\n\n\n\n\n\n\nDiscussion of ordering attitude measures\n\n\n\n\n\n\nDR: I see the case for increasing order is ‘it makes it more clear for them to follow/more coherent to respond. On the other hand, it could lead to less careful consideration of each response and a mechanical ’I better increase it’\n\n\nDM: If we want to test how many people will say they would be likely to do 1/2/3/4/5/6 I think we want to minimise the influence of order (or anything extraneous) on their likelihood of selecting 1/2/3/4/5/6. Ordering them by perceived commitment seems like it can only serve to nudge them in untoward ways to be more or less likely to select some.\n\n\nAlso I don’t think we have good grounds to predict what the order of increasing commitment is or to assume this is invariant across respondents.\n\n\nDR: OK, I see your point, if we actually want to know, and see these as measures of how many people will do/think each of these things. As a ‘measure of commitment and affinity’ the ordering makes some sense to me.\n\n\n\n\nQuiz questions, ‘information search’ (googling) and honesty\nWe often want to measure knowledge or recall but in online surveys people can look things up. Will they do so, and how does it depend on the incentive? Can this be deterred?\n\n\n\n\n\n\nDiscussion of honesty and Hugh-Jones work\n\n\n\n\n\nRe whether people cheat on unincentivized survey responses to quiz questions online when googling is possible, I consulted my ‘experiment gang’. David Hugh-Jones had a paper published in 2016 in which (he says) most US and UK people were honest and didn’t look things up[ even when there was an incentive. I’m trying to dig out those precise numbers but to me…\nThis is possibly a lower bound on the level of honesty without compensation if we asked, say quiz questions that indicate familiarity with 80,000 hours. (of course it’s not airtight, you could make a case that people might be more honest in a case where there is compensation because they feel like it’s more of a moral issue or that there will be more curiosity in this case, or that lookup was more work in his case… but still)\n\n\n\n\n\n\n\n\n\nDetecting and Deterring Information Search in Online Surveys Matthew H. Graham*\n\n\n\n\n\nFrom Twitter:\n\nLooking up the answers to knowledge questions in online surveys is far too prominent.\n\n\nA simple promise not to cheat can cut down a lot of it.\n\n\nSo can implementing a supplied script to detect when the participant leaves the screen."
  },
  {
    "objectID": "chapters/experiments_trials_design.html",
    "href": "chapters/experiments_trials_design.html",
    "title": "6  Experiments: Qualitative design, implementation",
    "section": "",
    "text": "See Reinstein notes on ‘why run an experiment’ (somewhat more relevant to ‘lab experiments’)\nSee informal ‘practitioners’ discussion below, from ‘EA market testing’"
  },
  {
    "objectID": "chapters/experiments_trials_design.html#other-relevant-resources",
    "href": "chapters/experiments_trials_design.html#other-relevant-resources",
    "title": "6  Experiments: Qualitative design, implementation",
    "section": "6.1 Other relevant resources",
    "text": "6.1 Other relevant resources\nJohn List ‘Some Tips for “Start-Ups” to do Better Field Experiments and Getting Your Work Published’"
  },
  {
    "objectID": "chapters/experiments_trials_design.html#proposed-structure-of-section",
    "href": "chapters/experiments_trials_design.html#proposed-structure-of-section",
    "title": "6  Experiments: Qualitative design, implementation",
    "section": "6.2 Proposed structure of section",
    "text": "6.2 Proposed structure of section\n\nBasic design choices and terminology\nTypes of experiments: ‘lab and field’\nFormulating hypotheses and ‘learning and adaptation goals’\nHypothesis testing versus ‘reinforcement learning’ goals\nDesign concerns and pitfalls\n\nConfounded designs\nAttrition and failed randomization\n‘Demand effects’\nNaturalness versus cleanliness\n\nPractical and implementation issues\n\nSurvey and experiment platforms (see ‘surveys’) chapter\nField experiments, A/B and lift tests, and marketing trials\nDesigning, coding, and implementing experiments: IT issues\nFailure and success modes\nCapturing outcome data\nPre-registration and pre-analysis plans\n… See Reinstein discussion notes on the benefits and costs here"
  },
  {
    "objectID": "chapters/qualitative-design-issues_plus.html",
    "href": "chapters/qualitative-design-issues_plus.html",
    "title": "7  Practical issues",
    "section": "",
    "text": "Here we link to (and can help build) the ‘EA Market testing Gitbook’ for building a non-technical and applied discussion. We may add more technical and detailed material on these issues below (in the present bookdown).\n\n‘EA Market testing Gitbook’"
  },
  {
    "objectID": "chapters/qualitative-design-issues_plus.html#participant-platforms",
    "href": "chapters/qualitative-design-issues_plus.html#participant-platforms",
    "title": "7  Practical issues",
    "section": "7.2 Participant platforms",
    "text": "7.2 Participant platforms"
  },
  {
    "objectID": "chapters/qualitative-design-issues_plus.html#implementationsurvey-construction-platforms-and-issues",
    "href": "chapters/qualitative-design-issues_plus.html#implementationsurvey-construction-platforms-and-issues",
    "title": "7  Practical issues",
    "section": "7.3 Implementation/survey construction platforms and issues",
    "text": "7.3 Implementation/survey construction platforms and issues"
  },
  {
    "objectID": "chapters/qualitative-design-issues_plus.html#survey-design-tools-it",
    "href": "chapters/qualitative-design-issues_plus.html#survey-design-tools-it",
    "title": "7  Practical issues",
    "section": "7.4 Survey design tools (IT)",
    "text": "7.4 Survey design tools (IT)"
  },
  {
    "objectID": "chapters/qualitative-design-issues_plus.html#compensation-and-rewards",
    "href": "chapters/qualitative-design-issues_plus.html#compensation-and-rewards",
    "title": "7  Practical issues",
    "section": "7.5 Compensation and rewards",
    "text": "7.5 Compensation and rewards\n\nOverall compensation\nRewards for specific responses"
  },
  {
    "objectID": "chapters/qualitative-design-issues_plus.html#misc-notes-on-platforms-and-tools",
    "href": "chapters/qualitative-design-issues_plus.html#misc-notes-on-platforms-and-tools",
    "title": "7  Practical issues",
    "section": "7.6 Misc notes on platforms and tools",
    "text": "7.6 Misc notes on platforms and tools\nIntegrating RStudio and Qualtrics: Importing Surveys to and Exporting Data from Qualtrics"
  },
  {
    "objectID": "chapters/exp_design_quant.html",
    "href": "chapters/exp_design_quant.html",
    "title": "8  Quant. issues/VOI",
    "section": "",
    "text": "Some quantitative issues\n\n\n\n\n\n\n‘Treatment’ assignment (blocking, randomization, etc)\nAdaptive, sequential and dynamic designs\nPlanning, diagnosing and adjusting a design\nPower analyses (and other ‘diagnosands’)"
  },
  {
    "objectID": "chapters/exp_design_quant.html#treatment-assignment-blocking-randomization-etc",
    "href": "chapters/exp_design_quant.html#treatment-assignment-blocking-randomization-etc",
    "title": "8  Quant. issues/VOI",
    "section": "\n8.1 ‘Treatment’ assignment (blocking, randomization, etc)",
    "text": "8.1 ‘Treatment’ assignment (blocking, randomization, etc)\nAdaptive, sequential and dynamic designs\n\nReinstein notes and links here\nElsey: http://www.sequentialtesting.com/"
  },
  {
    "objectID": "chapters/exp_design_quant.html#planning-diagnosing-adjusting-a-design",
    "href": "chapters/exp_design_quant.html#planning-diagnosing-adjusting-a-design",
    "title": "8  Quant. issues/VOI",
    "section": "\n8.2 Planning, diagnosing, adjusting a design",
    "text": "8.2 Planning, diagnosing, adjusting a design\n\n8.2.1 Specifying models, hypotheses, and statistical testing and inference approaches\n1"
  },
  {
    "objectID": "chapters/exp_design_quant.html#gains-from-small-sample-sizes-in-a-bayesian-context",
    "href": "chapters/exp_design_quant.html#gains-from-small-sample-sizes-in-a-bayesian-context",
    "title": "8  Quant. issues/VOI",
    "section": "\n8.3 Gains from small sample sizes in a Bayesian context",
    "text": "8.3 Gains from small sample sizes in a Bayesian context\nJamie’s notes here2"
  },
  {
    "objectID": "chapters/exp_design_quant.html#power-analyses-other-diagnosands",
    "href": "chapters/exp_design_quant.html#power-analyses-other-diagnosands",
    "title": "8  Quant. issues/VOI",
    "section": "\n8.4 Power analyses (& other ‘diagnosands’)",
    "text": "8.4 Power analyses (& other ‘diagnosands’)\n3\nKey resources and explainers\n\nReinstein notes and code\nPower analysis discussion and workflow section\nThis is a proposal and tools for a ‘path to do’ power testing (by Jamie Elsey; David Reinstein will weigh in too). I think this first focuses on a frequentist approach, but it’s likely to also introduce a Bayesian approach."
  },
  {
    "objectID": "chapters/exp_design_quant.html#voi_trials",
    "href": "chapters/exp_design_quant.html#voi_trials",
    "title": "8  Quant. issues/VOI",
    "section": "\n8.5 Value of information, optimal trial design",
    "text": "8.5 Value of information, optimal trial design\nWhy VOI instead of Power?\n\nN = 680 per cell is enough to have 95% power to detect a Cohen’s d of 0.20 … multiple comparisons. etc\n\nThe power calculation approach might implicitly assue we need \\(p=0.01\\) otherwise “we can’t reject the null” … which is vulnerable to being rounded up to “we can’t make any inference.”\nI (Reinstein) think this the wrong way to consider it. If we think it’s even 55% likely that message A is better than message B, we should choose message A. We have learned something.4\nReframing the discussion:\n… Not “how many observations do we need for this to be useful at all” we should be thinking/considering\n.. Instead “even small amounts of data permit useful learning”… so “how much data should we collect until it is no longer worth the cost … to maximize our net gain from better decisionmaking?”5\nThis is a VOI calculation.\nVOI and Optimal Design and inference: framework\nFor a decision-maker (‘DM’), knowing one condition is (e.g.) 80% likely to perform better than another may be very valuable. Still, the benefit of this depends on:\n\nHow does the ‘better’ metric (e.g., mean of Likert measures) translate into proximate and ultimate outcomes? 6\n‘How much better is better?’; e.g., what is the expected gain in the ultimate outcome, incorporating the costs of being ‘wrong in either direction’?\nHow much is the ultimate outcome valued?7\n\nUltimately, we want to choose our design and analysis strategy to maximize the “Value of Information” (VOI), minus the cost of acquiring this information. Call this strategy (\\(\\Sigma\\))8 and the information set at the end of the trial be \\(\\Theta(\\Sigma)\\)9. This VOI is essentially the difference between: 10\n\n\\(V_1(\\Sigma) \\equiv E\\Big[V\\big(\\mathbf{\\Phi}|D(\\Theta(\\Sigma)\\big)\\Big]\\)\n\nThe expected value of the outcomes \\(\\mathbf{\\Phi}\\) according to the DM’s criteria11, given the decisions \\(D\\) that will be made, in light of the information \\(\\Theta\\) gained from trials that use design \\(\\Sigma\\).\n\n\\(V_0 \\equiv E[V(\\mathbf{\\Phi}|D(\\Theta_0))]\\)\n\nThe ‘value of the outcomes…’ given decisions made without any further information, i.e., with the information set \\(\\Theta_0\\) that we will have in the absence of the trial.\nHence the VOI is:12\n\\[V(\\Sigma) = V_1 - V_0\\]\nThus we want to choose our design \\(\\Sigma\\) to maximize the VOI net of the costs of running the design (\\(C(\\Sigma)\\)):\n\\[\\Pi(\\Sigma) :=   V_1(\\Sigma) -  V_0 - C(\\Sigma).\\]\nOf course, the design does not affect \\(V_0\\), so we only need to worry about finding the design \\(\\Sigma\\) that achieves the highest value of:\n\\[V_1(\\Sigma) - C(\\Sigma) = E\\Big[V\\big(\\mathbf{\\Phi}|D(\\Theta(\\Sigma)\\big)\\Big] - C(\\Sigma).\\]\nOperationalizing this\nWe need to come up with explicit measures of:13\n\nThe specific design choices we are considering, i.e., some subset of \\(\\Sigma\\).\n\n(E.g., ‘how many conditions, which conditions, and how many individuals assigned to each condition’.)\n\nThe information we can gain from the trials, making inferences from an observed vector \\(\\mathbf{Y}\\), e.g.,\n\n\n\n\\(s_j \\in [0,10]\\) the survey mean of our ‘interest in the DM’s organization’ questions among Prolific participants, for each of conditions \\(j \\in t_0, t_1, ..., t_k\\), and,\n\n\\(c_j \\in [0,1]\\) the ‘share that click on the organization’s web link’ …\n\n\nThe joint (prior) distribution over the information we can observe and the real-world outcomes/rates we are interested in, \\(F(\\mathbf{Y}, \\mathbf{\\Phi})\\). More simply, some quantification of ‘how can we make inferences about the real world outcomes given what we will observe in the study?’\n\nE.g.,\n\nwe expect the real world conversion rate for a condition14 to be distributed with a Beta distribution centered at the mean of a weighted function of the true ‘Prolific click rate’ in that condition and the true average ‘interest in the organization score’, multiplied by some scaling factor.\n\n\nThe (relative) value of the world under each specific outcome. E.g., the dollar value the DM puts on some real-world conversion rate \\(\\phi\\)15\nThe cost of our possible design decisions. E.g., the cost per additional participant.\n\nNext: If we can fill in some of this, we may be able to make considered but heroic assumptions over the rest, and generate a VOI measure as a function of our design choices.16\n\n\n\n\n\n\nWhy go down this rabbit hole?\n\n\n\n\n\nMost of the gains from this will not be specific to this particular trial but will come in helping us developing an approach to later work for this partner and others.\n\n\n\n\n8.5.1 Building a tangible approach to VOI\n\nA specific proposal for comparing the VOI of designs\nEssentially17\n\nSet (and carefully consider) a reasonable prior distribution over the ‘message performance in the real world’18… across a range/random draw of possible messages (say, \\(M\\) total messages, indexed by \\(m\\)).\n\nE.g., let the individual probability of signup have a Beta distribution with a mean \\(u_m\\) for message \\(m\\). Assume these means \\(u_m\\) are drawn from a multivariate normal distribution.\n\nMake heroic assumptions over the connection between the observed metric19 and the true outcome (example above).20\n\nSimulate the distribution of value for each design we are considering \\(k \\in K\\)21. For each design \\(k\\), for each simulation \\(t\\), we will do the following.\n\n\nSimulate ‘what we report to the client’:\n\nDraw the parameters of the ‘true value’ of performance (of each message) from the prior (call it \\(\\Phi_t\\) for sim \\(t\\), perhaps a vector of length \\(M\\)). Given these parameters…\nConsider a ‘Sample population’ \\(N(k)\\) equal to the size as specified in design \\(k\\). Simulate assigning each of \\(N(k)\\) individuals (indexed by \\(i\\)) to a ‘message treatment \\(m=m_i\\)’, with this assignment following design \\(k\\).\nDraw ‘real outcomes for each sample individual \\(i\\)’, if they were to be assigned to message \\(m_i\\) in the real world, i.e., \\(Y_i(m_i)\\),[^exp_design_quant-5]\nDraw the ‘survey metric’ \\(s_{im} = Y_{im} + error\\)22 for each individual, were they to be assigned to message treatment \\(m_i\\). Let the latter vector for all individuals in the simulated sample \\(t\\) be \\(S_t(\\Phi_t))\\).\nEstimate \\(\\beta(S_t)\\), the ‘metric we would report to the client’ as a function of the above vector \\(S_t(I_t))\\). This could be something simple, like a rank-ordering of the mean values of the Likert measure for each treatment.\n\n\nCalculate the ‘client’s message choice’ given this information. For simplicity (for now), assume the client optimizes in some way. E.g., assume they assign all real-world people to the one treatment that performed best by our reported metric \\(\\beta(\\Phi_t)\\)), Call this message choice \\(m^{*}_t\\).\n\nEstimate the value achieved by this choice:\n\nLet \\(W\\) be the number of people affected23. Draw \\(W\\) individuals from the simulated population, still using parameters \\(\\Phi_t\\) for these draws\nImagine each of these are assigned message \\(m^{*}_t\\).24 Draw ‘simulated real outcomes’ for each of these individuals for this message. I.e., \\(Y_{i}(m^{*}_t)\\).\n\n\nReport the total value achieved (most simply \\(W \\times y_{i}(m^{*}_t)\\)) and call this value \\(V_t(k)\\): the “value achieved in simulation \\(t\\) under design \\(k\\).”\n\n\nDo this simulation for each design choice \\(k\\), and report the distribution of value \\(V_t(k)\\). We can then do this simulation for each of a range of relevant design choices \\(k\\) 25 to compare performance.26\nCoded: Simple example of the VOI simulation\nWe next code and present a “bare bones example” \n\nIn this case I imagine having to choose between 5 messages with different (unknown) efficacy. You can just guess which is best, or field the messages to 20, 100, or 1000 people each. Then as the decision criteria you just suggest picking the one with the best mean.\n\n\nSuppose this costs 2 USD per participant\n\n\nThen I imagine some conversion from their scores to their likelihood of signing up to something, and I say the value of a sign up is $40 (this could be changed, or estimated for various values in principle). 27\n\n28\n\nThen I imagine the messages are ‘for real’ going to be fielded to 200,000 people - and I simulate them being fielded for each item at the ratio those items were [chosen as best in our simulations] based on each design.\n\n29\n\n\n\n\n\n\nWays to extend this\n\n\n\n\n\n\nMessage performance in real-world (or trial) drawn from a distribution, design randomly chooses some number of messages to test\nDraw from (prior distribution of beliefs over) the variance between message performance\n… and over the connection between message performance in trial and real world\n\n\n\n\n\nCoding this\n\nCodelibrary(pacman)\n# devtools::install_github(\"rethinkpriorities/rnoodling\")\n\np_load(dplyr, furrr, purrr, tidystats, ggplot2, rethinkpriorities, rnoodling, install=FALSE)\n\n\n\nCode.options = furrr_options() \n\nplan(multisession)\n\n\nWe create a function simulate_and_infer, following the description above, to:\n\nSimulate responses\n\n\nAs standard normal draws\nEach message performing a bit differently on average, with ordering \\(e > d > a> c > b\\) 30\n\nDifferences of (Cohen’s d) 0.1 between each message, with a maximal difference of 0.4.31\n\n\n\nTell you which ‘group’ (e.g., which of 5 messages ‘a’ through ‘e’) would be chosen by the client (assuming they simply pick the one with the highest mean response in the trial data)\n\nThe dataframe32 generated has one row per simulation, keeping only a counter for the simulation, the design sample size, and the ‘chosen as best-performing’ message.\n\nCodesimulate_and_infer <- function(n, sim, dscale=1) { #note the key parameter 'n', the number of sample draws, i.e., number participants in the experiment for this design\n  \n  # simulate 5 groups\n  sim_data <- tibble(group = c(rep(\"a\", n),\n                               rep(\"b\", n),\n                               rep(\"c\", n),\n                               rep(\"d\", n),\n                               rep(\"e\", n)),\n                     response = dscale*c(\n                       rnorm(n, 0, 1),\n                       rnorm(n, -.2, 1),\n                       rnorm(n, -.1, 1),\n                       rnorm(n, .1, 1),\n                       rnorm(n, .2, 1)),\n                     nsim = sim)\n  \n  # estimate the mean per group\n  decision <- sim_data %>% \n    group_by(group) %>% \n    summarise(mean = mean(response))\n  \n  # in this case our inference is simply to pick the one with the highest mean\n  output <- tibble(selected = decision$group[which(decision$mean == max(decision$mean))],\n                   sim = sim,\n                   n_per_group = n)\n  \n  return(output)\n  \n}\n\n\nNext we set some options and parameters for the simulation. Here, we consider three possible designs, with either:\n\n20 participants per group,\n100 per group,\nor 1000 per group.\n\nWe generate the tibble from this simulation in the code below.\n\nCodetwenty_hundred_thousand <- furrr::future_map2_dfr(\n  .x = c(\n    rep(20, 1000),\n    rep(100, 1000),\n    rep(1000, 1000)), #sample size in each simulation\n  .y = 1:3000, #number of simulations\n  .f = simulate_and_infer,\n  .options = furrr::furrr_options(seed = 1234)\n)\n\n\nReporting the results:\n\nCode# probability that best items are selected changes with sample size\ntwenty_hundred_thousand %>% \n  group_by(n_per_group) %>% \n  arrange(selected) %>% \n  count_data(selected) %>% \n  head(12) %>% \n  DT::datatable(options = list(\n              \"pageLength\" = 12, info = FALSE))\n\n\n\n\n\n\nAbove, we see that the larger the trial sample size, the more often the best message is chosen. Note that with the 20-observation trials, even the worst message (b) is sometimes chosen. With the 1000-observation trials the best message (e) is chosen nearly all the time in our simulations, with only an occasional choice of the second-best message (d).\n\nBut what is the value of each design?\nNext, we need to model the connection between the trial outcome (e.g., Likert scores) and real-world outcomes (e.g., signing up for coaching, aka ‘conversion’).33 As discussed, we assume that the outcomes in the trial are consistently positively related to the probability of the real-world outcomes.34\nWe set up a function below (score_to_conversion) to calculate the value (and cost) from a particular ‘trial design35 that recommends a particular message’…\n… which will be applied to a population of some exposure_size, yielding some number of conversions, each with some conversion_value.\nIt also considers/allows setting:\n\nthe baseline average conversion probability conv_prob\nthe cost per-trial-participant (cost_per_ppn)\nsome number of simulations36\na scaling factor for differences between designs (dscale)\na scaling factor for the connection between the trial response and the real world outcome (connection)37\n\n\nCodescore_to_conversion <-\n  function(group = \"a\", design = \"guess\", sim = 1, exposure_size = 200000, conversion_value = 40, cost_per_ppn = 2, conv_prob = 0.015, dscale=1, connection=1) {\n    \n    average_score <- \n      if (group == \"a\") { 0 }\n    else if (group == \"b\") { -.2*dscale }\n    else if (group == \"c\") { -.1*dscale }\n    else if (group == \"d\") { .1*dscale }\n    else if (group == \"e\") { .2*dscale }\n#DR: shouldn't this be able to be  automatically same object that we put into the `simulate_and_infer` function... so we only need to adjust one thing as we do sensitivity checking?\n    \n    respondents <- tibble(\n       individual_scores = rnorm(n = exposure_size,\n        mean = average_score),\n    individual_probability = conv_prob * (1 + (1 * individual_scores))) %>% #DR: I adjusted how these were assigned because Jamie's version was throwing an error for me `Error in eval_tidy(xs[[j]], mask) : object 'individual_scores' not found`. Probably needs doublechecking\n\n      mutate(individual_probability = \n        plogis(\n          qlogis(conv_prob)\n          + connection * individual_scores\n          )\n    ) %>% \n#DR: Conceptually, I think it makes more sense to me to go the other way. First generate the true probability, and then make the scores a function of that. But perhaps it doesn't matter.\n      mutate(\n        conversion = rbinom(n(), 1, individual_probability)\n        )\n    \n    outcomes <- summarise(\n      respondents,\n      total_conversions = sum(conversion)) %>%\n      mutate(\n        value_conversions = total_conversions * conversion_value,\n        exposure_size = exposure_size,\n        design = design,\n        sim = sim\n      ) %>%\n      mutate(\n        cost = case_when(\n          design == \"guess\" ~ 0,\n          design != 0 ~ as.numeric(design) * 5 * cost_per_ppn\n        ), #here, 5 treatments .. and \"design\" simply specifies the sample size per treatment \n        value_min_cost = value_conversions - cost\n      )\n    \n    return(outcomes)\n    \n  }\n\n\nNow we can make a distribution of value for each design when the recommended message is ‘fielded’ in the real world. We start with the simulation default of a message fielded to 200,000 real people. We consider each of the three sample-size designs from before (20, 100, 1000) as well as a simple ‘guess’ based on no trials at all. We estimate the expected value from each design. Again, we imagine that messages are used when they come out ‘best’ in a trial. To get the expected value, for each design, we weight this by the simulated probability that a message ‘comes out best’.\n\n\nCodevalue_for_messages <- pmap_dfr(.l = list(group = c(twenty_hundred_thousand$selected, c(rep(\"a\", 200),\n                                                                                   rep(\"b\", 200),\n                                                                                   rep(\"c\", 200),\n                                                                                   rep(\"d\", 200),\n                                                                                   rep(\"e\", 200))),\n                                                design = c(twenty_hundred_thousand$n_per_group, rep(\"guess\", 1000)),\n                                                sim = c(twenty_hundred_thousand$sim, 3001:4000)),\n                                      .f = score_to_conversion)\n\n#Note -- the above takes a few minutes, and throws a lot of warnings 'cost = case_when(...)`. NAs introduced by coercion'`\n\n\nNext, we plot the ‘gross’ expected value achieved as a result of each design (not considering costs):\n\nCode# expected value of the conversions\nvalue_for_messages %>% group_by(design) %>% \n  summarise(mean = mean(value_conversions),\n            lower = quantile(value_conversions, .05),\n            upper = quantile(value_conversions, .95)) %>%  .kable(digits=0) %>% \n    .kable_styling()\n\n\n\n\n design \n    mean \n    lower \n    upper \n  \n\n\n 100 \n    221,189 \n    191,672 \n    232,402 \n  \n\n 1000 \n    227,767 \n    222,760 \n    232,800 \n  \n\n 20 \n    210,261 \n    169,990 \n    231,800 \n  \n\n guess \n    190,780 \n    154,640 \n    229,844 \n  \n\n\n\nCodeggplot(value_for_messages) +\n  geom_density(aes(x = value_conversions, fill = design), alpha = .33)\n\n\n\n\nAbove, we see that the 1000-observation design yields the highest (gross) value. This must be the case: barring extremely weird draws, a larger sample will more closely resemble the true population distribution. Thus, if the trial measure is informative, the more ‘profitable’ message will be chosen more often. However, this benefit might not outweigh the cost of a larger trial sample; we this consider the ‘net’ value below.\n\nCode# expected value of conversions minus cst of the design\nlibrary(\"ie2misc\")\n\nggplot(value_for_messages) +\n  geom_density(aes(x = value_min_cost, fill = design), alpha = .33)\n\n\n\nCode( \n  sum_value <- value_for_messages %>% group_by(design) %>% \n  summarise(mean = mean(value_min_cost),\n    sd = sd(value_min_cost),\n    mad = madstat(value_min_cost),\n            lower = quantile(value_min_cost, .05),\n            upper = quantile(value_min_cost, .95)) %>% \n    .kable(digits=0) %>% \n    .kable_styling()\n) \n\n\n\n\n design \n    mean \n    sd \n    mad \n    lower \n    upper \n  \n\n\n 100 \n    220,189 \n    12,390 \n    10,010 \n    190,672 \n    231,402 \n  \n\n 1000 \n    217,767 \n    3,705 \n    2,574 \n    212,760 \n    222,800 \n  \n\n 20 \n    210,061 \n    20,918 \n    17,229 \n    169,790 \n    231,600 \n  \n\n guess \n    190,780 \n    25,471 \n    21,929 \n    154,640 \n    229,844 \n  \n\n\n\n\nAs the graph and table above suggest, the largest (1000-observation) sample design yields a lower payoff in expected value (but substantially less dispersion)."
  },
  {
    "objectID": "chapters/power_analysis_framework_2.html",
    "href": "chapters/power_analysis_framework_2.html",
    "title": "9  Power analysis & workflow",
    "section": "",
    "text": "By Jamie Elsey, adapted and extended by David Reinstein"
  },
  {
    "objectID": "chapters/power_analysis_framework_2.html#discussion-and-framework",
    "href": "chapters/power_analysis_framework_2.html#discussion-and-framework",
    "title": "9  Power analysis & workflow",
    "section": "\n9.1 Discussion and framework",
    "text": "9.1 Discussion and framework\nIntroduction and goals\nWe propose the fundamentals for a workflow for power analyses, beginning with a standard frequentist analysis.[^power_analysis_framework_2-2] [^power_analysis_framework_2-2]: Although this started from a Bayesian paradigm, the overarching framework is applicable to any analytic approach. Starting frequentist will make the code easier to run as well. Note that as Bayesian power analyses can be very time consuming (at the moment/with our current setup), in some cases even if you ultimately might perform a Bayesian analysis, it may make some sense to run frequentist approaches first. In many cases the estimates from Bayesian and frequentist approaches will tend to converge. I (Jamie) suspect general estimates from frequentist approaches would be quite similar to those of Bayesian approaches, if the goal of the analysis (what you want to make an inference about) is the same. I will make an accompanying document with an example of a Bayesian power analysis that should be computationally feasible to accompany this. The general approach laid out here can be the basis for developing and build a library of common analyses and inference goals.\nWorking on a Bayesian approach in the vignette HERE\n\n\n\n\n\n\nHelpful future additions to the toolkit would include\n\n\n\n\n\n\nclear and flexible ways to generate hypothetical data sets, and\nadding further analytic designs.\n\nDR: I think tools like DeclareDesign can help with this.\n\n\n\nThis is not intended as an exhaustive introduction to the fundamentals of statistical power–I assume you are reasonably well-versed in some general principles of statistical inference and hypothesis testing, as well as in the basic idea of what power analysis is.1\nWe will focus solely on ‘simulation-based’ power analysis,2 and not on ways of mathematically (analytically) deriving power.\nDefinition of power\nIn frequentist null hypothesis significance testing (NHST), power is typically defined as ‘the probability that we can reject the null hypothesis, if the alternative hypothesis is indeed true’ (i.e., power is the ‘true positive rate’). More precisely, we may express this as ‘power against a particular alternative hypothesis’.\n\n\n\n\n\n\nWikipedia: The statistical power of a binary hypothesis test…\n\n\n\n\n\nThe statistical power of a binary hypothesis test is the probability that the test correctly rejects the null hypothesis \\(H_0\\) when a specific alternative hypothesis \\(H_{1}\\) is true. It is commonly denoted by \\(1-\\beta\\), and represents the chances of a “true positive” detection conditional on the actual existence of an effect to detect. … as the power of a test increases, the probability \\(\\beta\\) of making a type II error by wrongly failing to reject the null hypothesis decreases.\nlink accessed 12 Apr 2022\n\n\n\n\n\n\n\n\n\nHowever some Bayesians question this\n\n\n\n\n\nSome Bayesian statisticians question the validity and usefulness of this presentation of power analysis. This engages deep issues in the conception of probability. From the frequentist perspective you imagine a fixed underlying true parameter and consider the many data sets that might hypothetically arise from it - ‘the true parameter is fixed and the data varies’.\nIn Bayesian reasoning you note that you have ‘uncertainty about the underlying parameter’ but you consider the data as fixed. A Bayesian approach does not tend to consider ‘the probability that the data arises in a particular way under a null hypothesis’; however ‘Bayesian hypothesis testing’ is discussed in the literature. (Presumably, it envisions ‘rejecting a hypothesis’ where the posterior probability that the hypothesis holds is sufficiently small).\n\n\n\nBefore we conduct an experiment (or run a survey, or collect data for an analysis), there are many hypothetical future data sets we might observe. Whether we favor a frequentist or Bayesian approach, it seems reasonable to ask:\n\nGiven the range of data that I might observe, how likely is it that I can make certain conclusions?.\n\n3\nWe can broaden the idea of power to indicate the probability that our proposed sample yields information that allows us to make some specific kind of inference about the data-generating process.4 Given a particular underlying effect (or range of effects, or lack thereof), and a particular sample size (or range of sample sizes), we may ask ‘what is the probability of’… ?:5\n\n‘determining’ that there is a non-zero difference between two conditions,6\n\ndetecting some ‘smallest effect size of interest’,\na ‘false positive’,7\n\nachieving a desired level of precision around a particular parameter estimate, or\nfinding an effect ‘likely equivalent to zero’ or ‘unlikely to be far from zero’.8\n\n\n\n\n\n\n\n\n‘Power of an equivalence test’ (key aside)\n\n\n\n\n\nWe may wish to estimate the ‘likelihood that we will determine that the effect is in a near-zero range’, when the true effect size is in fact zero or very small. This is called an ‘equivalence test’; thus we may want to compute the ‘power of an equivalence test given a particular distribution of true effect sizes’. I’ll call the latter the ‘equivalence distribution’ for now.\n\nDetermine the equivalence bounds of interest … the lower and higher bounds of ‘an effect close to zero’\nDetermine how we wish to compute our equivalence test (e.g., the ‘two one-sided t-tests’ advocated by Lakens) given these equivalence bounds\nSet the ‘equivalence distribution’\nSimulate a large number of data sets arising under the equivalence distribution. (DR: in practice I think this is often stated as an exact zero effect, but this might be naive.)\nCompute the equivalence test for each simulated data set.\nCount the share of these that ‘determine the effect is near zero’. This share is the ‘power of our equivalence test against the equivalence distribution’\n\nSee: One code example of this here using resampling from previous comparable data, password required (ask David Reinstein), search for the equiv_data function and its use in the code.\n\n\n\nHence, a power analysis helps frame our prospective research project in relation to the goals we would like to achieve.9 It provides us with an estimate (and it is indeed an estimate, not a certainty!) of the probability that we will be able to make the inference we wish to make, given various factors both inside and outside of our control. (See discussion in fold.)\n\n\n\n\n\n\nWhat is ‘in our control’?\n\n\n\n\n\nWe might think of sample size as in our control, which it generally is, but this is typically limited by practical considerations outside of our control (such as the availability of a sample or financial constraints). Conversely, effect size is often considered out of our control; however we can sometimes increase the ‘dosage’ we give, or try to select a particular group of participants who might be particularly susceptible to our effect of interest.\n\n\n\nGeneral workflow for simulation-based power analysis\nSimulation-based power analysis proceeds in four primary steps:\n\n\nGenerate a large number of ‘simulated data sets’ for the analysis.10\n\n\nWe generate data sets generated to be specific to the study’s goal. E.g., suppose we want to know the ‘power to detect an effect size of 0.2 SD’.11 Here, for considering the true positive and false negative rates, we should generate data that reflects this effect size. If we also want to measure the rate of type-2 error12 we should also generate data reflecting a ‘null effect’.\n\nRun the proposed analysis over the many simulated data sets. (Do this as efficiently as possible, as it can take a long time.) This analysis should either return the estimand of interest, or ensure that we can easily compute it (in Step 3). The output should be kept as flexible as possible (while conserving computer memory). This will allow us to assess multiple inference goals on the same output.13\nSummarize the output returned in Step 2. Compute the share of simulated data sets that meet particular inference goals or decision criteria. (Example described in fold)\n\n\n\n\n\n\n\nExample: a simple case of simulating data to measure power\n\n\n\n\n\nE.g., we might simulate 1000 data sets based on an effect size of 0.3 standard deviations and perform a standard t-test of the difference between treatment and control for each of these. We might then find that for 743 of 1000 of these simulated data, the test ‘rejected the null hypothesis’, suggesting a power of 74.3%.\n\n\n\n\nThis can show the likelihood of achieving a range of goals (under various sample sizes and design and testing choices) including the rates of misleading conclusions. In frequentist analyses, steps 2 and 3 can often be done together (e.g., the p-value is returned along with the other output). In my experience (JE) with typical Bayesian designs, these stages are best kept separate so that we can first do the more time-consuming Step 2, and then more freely explore the various inference goals in Step 3.\n\n\nAssess the output and determine whether the proposed analyses and inference goals are realistic and likely to yield informative results. If not, consider alternative design or data-collection choices (including sample sizes, treatments, and treatment assignments) or inference goals, and return to Step 1.14\n\n\n\n\n\n\n\n\nComputing and optimizing across a design space\n\n\n\n\n\nTypically, we are doing this exercise to diagnose the “best design” given our constraints15, or to consider whether any trial is worth doing at all. The procedure above diagnoses a particular design, and it could be iterated across a range of designs until you find the reasonably best one. However, you may be able to assess a range of designs at once with a cleverly coded set of simulations. Even in the exercise below, When you run simulations for a “large” sample size, you will be able to also compute the power and other diagnosands for a smaller sample size. More generally, you may want to 1. Defined the space of designs you wish to explore, and 2. ask your code to produce simulations for all of these at the same time, 3. compute and display the power (etc.) across this space of designs, and finally 4. search this space for the ‘best’ design by some criterion. (DR: I’m working onthis approach here, ask me for the password.)\n\n\n\nIn the following sections, we present some generally useful packages and functions for power analyses. We give annotated code examples, highlighting useful aspects. We do this in the context of a frequentist power analysis for between-groups analysis.16\n[^power_analysis_framework_2-19] [^power_analysis_framework_2-19]: Willem has some nice procedures for generating repeated-measures data in his walkthrough, see also his ‘power curves’ using the library MASS.\n\n\n\n\n\n\n‘But I want to know what size affect I have power to detect?’\n\n\n\n\n\nInstead of computing the power to detect in effect of a certain size, it can sometimes be more informative to understand the ‘smallest effect size you have a certain (e.g., 80%) power to detect. This is called ’inverting the calculation’. My impression (DR) is that this is more difficult To-do through simulation, although some packages do offer this option.\n\n\n\nPossible pitfalls/misunderstandings of power analysis\nPower analyses are a useful tool for considering the basic plausibility of achieving certain goals. But they are not an ‘omniscient oracle’, and may be somewhat of a blunt instrument. They might better be termed ‘power projections’ or ‘power estimates’. Even if we estimate that we have 99.9% power for detecting some a particular effect, we may have specified our simulations in an unrealistic way. When we actually run an experiment, we might see (e.g.) far more underlying variation or measurement error than we predicted, leading our analyses to be fairly uninformative. Conversely, we might be overly conservative with our power analysis; perhaps a design we thought was ‘underpowered’ actually has a high probability of producing very compelling results.\nConsidering: when can power analyses be helpful? and when can they be limited\n\n\n\n\n\n\nIntricate data exploration versus broad inference\n\n\n\n\n\nWhen we see real data and real interesting patterns emerge, we are likely to go further in modeling and investigating these patterns than some of the more simple analyses and comparisons we conduct in the initial power analysis. I suspect power analyses are not so good for determining all the intricate, in-depth things we might plumb in a dataset. They are probably better at assessing the tractability of a broad inference goal.\n\n\n\n\n\n\n\n\n\nComplex models, forking paths, and tractibility\n\n\n\n\n\nWe might want to give some consideration to how far we wish to go in the initial data simulation step to think about all sorts of hypothetical data sets. As models become more complex, the number of different parameters that might vary - with possible effects on power - starts to balloon. E.g., even in a simple repeated-measures example, do we wish to vary not only the effect size but all sorts of different correlations from pre- to post-treatment within subjects? If we are simulating ordinal data, then power might change depending on how we initially suggest binning the outcomes, but there are infinitely many ways we might think the data might look… Discussion and consideration of how far we should go with these things is welcome and could be useful! (DR: I’m not sure what you are getting at here. Are you saying that ‘the space of designs and proposed analyses we can explore is extremely large, and we may need to make some ad-hoc choices to avoid this getting unmanageable’?)\n\n\n\n\n\n\n\n\n\nWhat ‘effect size’? Not ‘the effect size you are seeing so far’\n\n\n\n\n\nTo do a single power calculation you have to specify an effect size (or a distribution). What effect size should you choose?\nResearchers have been known to collect a small amount of data and measure the area of fact they seem to be observing, and then use this for their power calculations. IMHO (DR) this is misguided. The effect size you compute in a small sample clearly may not be the true effect size (otherwise what’s the point in doing statistical inference?). Even if this is the most likely effect size it does not justify why you would want to design your experiment to have the right amount of power to detect this particular sized effect.\nAnother approach is to power it based on the ‘smallest effect size of interest’ (SESOI) and there is some discussion of this in the methods literature. This might be interpreted based on the notion that if we are powered to detect such a small effect size and we failed to detect it we might say ‘ok there is no meaningful effect’. But this is not quite right; to make such statistical conclusions you would have to do an equivalence test, and thus you would focus your power calculation on that. Perhaps a better justification for targeting a SESOI is based on a loose ‘value of information approach’: we should use our funds to detect ‘interesting effects’; if any effect is below this, it’s not worth spending the money to detect. But this just pushes the question backwards, and forces us to consider the cost and benefits of collecting data.\nWe discussed the explicit ‘value of information’ (VOI) calculation for trial design here."
  },
  {
    "objectID": "chapters/power_analysis_framework_2.html#concrete-implementation-of-framework",
    "href": "chapters/power_analysis_framework_2.html#concrete-implementation-of-framework",
    "title": "9  Power analysis & workflow",
    "section": "\n9.2 Concrete implementation of framework",
    "text": "9.2 Concrete implementation of framework\nStep 1: Generate a large number of data sets for the analysis\nConfirm that we can generate the basic data we want\nIn many simulations we end up generating some rather large data files: thus, we first increase the memory limit allotted to R.17\n\n\n[1] Inf\n\n\nNow we want to generate a hypothetical data set. In this case, we will be comparing four groups, each shown a different message in a between-subjects design (three active groups vs. a control condition with a neutral message).\nIn this example, the ‘different groups’ in our design play a second role in helping us assess our power against several different possible effect sizes.18 \n\nCode# tidyverse simply used for data wrangling and plotting\nlibrary(tidyverse)\n\n\nCode: The four.group.datamaker function creates a tibble (data frame) of\n\na (std) normally distributed outcome\nan ordinal categorization of this outcome\nwith four groups, each with (potentially) different outcome means\nwith ppg rows per group\nwith defaults as described\n\n\nCodefour.group.datamaker <- function(sim = 1, a = 0, b = .1, c = .2, d = .4, ppg=1500) {\n\n#DR I renamed it `ppg` for 'population per group' because `pop` confused me\n\n  # first a tibble (data frame) with 1500 ppts, with the different groups showing\n  # effect sizes in Cohen's d of .1, .2, and .4\n  four.groups <- tibble(a.control = rnorm(ppg, a, 1),\n                        b.small = rnorm(ppg, b, 1),\n                        c.medsmall = rnorm(ppg, c, 1),\n                        d.medium = rnorm(ppg, d, 1),\n                        counter = 1:ppg) %>% #we previously called the counter 'sample.size' because of its later use\n\n    # turn the data into long form\n    pivot_longer(cols = 'a.control':'d.medium', names_to = 'group', values_to = 'response') %>%\n\n    # put cutpoints in the data to make it more similar to the ordinal responses we would get\n    mutate(ordinal = case_when(response < -1.5 ~ 1,\n                               response < -.5 ~ 2,\n                               response < .5 ~ 3,\n                               response < 1.5 ~ 4,\n                               response >= 1.5 ~ 5),\n\n           # for the purposes of this demo we will not analyse it as ordinal as it takes longer\n           # to run the regressions, but if you did so you would also want to make the response\n           # a factor\n           ordinal = as.factor(ordinal),\n           sim = sim)\n\n  return(four.groups)\n\n }\n\n\nWe plot the data generated in one instance, to check our code and setup:\n\nCode# test that the function works to make one data set before making many!\ntest.data <- four.group.datamaker()\n\nggplot(data = test.data) +\n  geom_density(aes(x = response, fill = group), alpha = .3) +\n  theme(\n    aspect.ratio = .5\n  ) +\n  ggtitle('Test four groups, continuous outcome')\n\n\n\nCodeggplot(data = test.data) +\n  geom_histogram(aes(x = as.numeric(ordinal)), alpha = .6,\n                 position = position_dodge(), bins = 10) +\n  facet_wrap(~group, nrow=1) +\n  theme(\n    aspect.ratio = 1\n  ) +\n  ggtitle('Test four groups, categorical outcome')\n\n\n\n\nWe can see from the plots that the function appears to be working. When developing a data set for the first time, one would usually go further with some ‘diagnostic’ checks to confirm that the data is behaving as you intended.19\nEfficiently generate many data sets\nNow we just need to run the function above many times over. This could be done using loops, but a very useful set of R functions in the tidyverse is the purrr package of map functions. Even better, a package called furrr is available to run such map functions in parallel to further reduce time.20 For furrr to do this, we need to tell it to plan for ‘multisession’, and give it a seed in the code below. 21\n\nCodep_load(furrr)\nlibrary(furrr)\n\nplan(multisession)\noptions <- furrr_options(seed = 48238)\n\n\nCode: We map over all 500 elements of the vector nsims to create 500 simulated data sets.22\n\nCode# we will pass N = 500 simulations to the map function\nnsims <- 1:500\n\n# the map function will run our data-making function over nsims=500 simulations\nsim.data <- furrr::future_map_dfr(\n  .x = nsims,\n  .f = four.group.datamaker,\n  a = 0,\n  b = .1,\n  c = .2,\n  d = .4,\n  .options = options\n)\n\n# split the simulated data into the separate simulations\n\nsim.data <- sim.data %>% group_by(sim) %>% group_split()\n\n\nNow, we have 500 simulated data sets representing our hypothetical outcome data, and can perform analyses on them. Here’s a peek at part of one of these data sets, the third simulation:\n\nCodehead(sim.data[[3]])\n\n\n\n\n\n\n\n\n\ncounter\ngroup\nresponse\nordinal\nsim\n\n\n1\na.control\n-0.395 \n3\n3\n\n\n1\nb.small\n-0.396 \n3\n3\n\n\n1\nc.medsmall\n-0.0762\n3\n3\n\n\n1\nd.medium\n0.994 \n4\n3\n\n\n2\na.control\n-0.931 \n2\n3\n\n\n2\nb.small\n-1.58  \n1\n3\n\n\n\n\nStep 2: Run the proposed analysis over the many data sets and return the estimands of interest\nBefore we run an analysis over the many simulated data sets, we should to check that our models can run on these, and that they will return the estimands we are looking for.\n\n\n\n\n\n\nFor a Bayesian analysis\n\n\n\n\n\nFor a Bayesian analysis, this step might involve dropping parts of the posterior that are not relevant23 and ensuring we get the parts we care about (e.g., we might insert additional code to retrieve a posterior distribution for Cohen’s d).24\n\n\n\nThere are a range of estimands we might care about, depending on our estimation and inference goals. For example, we might consider returning a p-value, or the upper and lower bounds for confidence intervals, or R-squared estimates, etc. The key point: before you drop the large set of simulated data sets and the regression results for each, make sure you every estimated that might be of interest; you don’t want to have to to re-run the entire analysis.\nIn addition, you want to run the analysis on different sample sizes of the data, to generate a power curve plot. I.e., a plot showing how your power increases as you increase the sample size.\nIn the example function below, we run a simple linear regression, predicting the response from group.\n\nCodelinear.reg.maker <- function(data, breaks) { #runs a particular regression over a set of cuts of larger and larger subsets of  multiple data sets\n\n  # this function cuts the data set it is given into different sample sizes\n  cut.samples <- function(break.point, data) {\n    cut.data <- filter(data, counter <= break.point) %>%\n      mutate(sample.size = break.point)\n    return(cut.data)\n  }\n\n  data.cuts <- map_dfr(.x = breaks, .f = cut.samples, data = data)\n\n  # the data is split according to the sample size\n  # to feed to the regression model\n  data.cuts <- data.cuts %>% group_by(sample.size) %>% group_split()\n\n  # this function runs the regression\n  run.reg <- function(data) {\n\n    four.group.form <- as.numeric(ordinal) ~ 1 + group\n\n    four.group.reg <-\n      lm(formula = four.group.form,\n           data = data)\n\n    # we extract confidence intervals for the parameters of interest\n    ci99 <- confint(four.group.reg, level = .99)\n    ci95 <- confint(four.group.reg, level = .95)\n\n    # we create an 'output' to show the confidence intervals around the effects\n    # and some additional inference info, e.g., 'nonzero' indicates whether\n    # the lower bound of the CI excludes 0 or not.\n    # 'width' indicates the width of the confidence interval,\n    # for assessment of precision\n    output <- tibble(group = c('small', 'medsmall', 'medium',\n                               'small', 'medsmall', 'medium'),\n                     interval = c(.99, .99, .99, .95, .95, .95),\n                     lower = c(ci99[[2,1]], ci99[[3,1]], ci99[[4,1]],\n                               ci95[[2,1]], ci95[[3,1]], ci95[[4,1]]),\n                     upper = c(ci99[[2,2]], ci99[[3,2]], ci99[[4,2]],\n                               ci95[[2,2]], ci95[[3,2]], ci95[[4,2]])) %>%\n  #DR: maybe this code should be made a little more flexible?; because if you wanted to consider additional-sized effects you would need to expand the entries above\n\n      mutate('nonzero' = case_when(lower > 0 ~ 1,\n                                   TRUE ~ 0),\n             'width' = abs(upper - lower),\n               'sim' = data[[1, 'sim']],\n             'cell.size' = nrow(data)/4)\n\n    return(output)\n  }\n\n  # run the regression function over the different sample sizes\n  output <- map_df(.x = data.cuts, .f = run.reg)\n\n  return(output)\n}\n\n\n\nOnce we have made and tested that our function works as intended and returns the values we want to make inferences from, we can run it over the many simulated data sets:\n\nCodet1 <- Sys.time()\nlinreg.output <- future_map_dfr(.x = sim.data,\n                                .f = linear.reg.maker,\n                                breaks = seq(from = 150, to = 1500, by = 150))\nt2 <- Sys.time()\nt2 - t1\n\nTime difference of 34.66015 secs\n\n\nThe object linreg.output is now a large dataframe, cataloguing whether or not certain inference thresholds were reached across the many simulations. In the third primary step, we can summarise and graphically display this information.\nStep 3: Summarise the output returned in Step 2 to determine the likelihood of achieving various inferential goals\nNow, we want to know how likely we are to achieve a range of inferential goals, depending on factors such as the sample size, the underlying effect sizes, or anything else we varied in simulating our data and running our models. For this example, this is as simple as generating a summary of the output from Step 2… 25\n\nCode# group the data according to group, confidence interval, and size per group\nfour.group.lin.summary <- linreg.output %>% group_by(group, interval, cell.size) %>%\n  # summarise the amount of times we get a CI greater than 0\n  summarise(.groups = 'keep',\n            'ci above 0 vs. control' = sum(nonzero)/5)  %>%\n  # change some factors for plotting\n  mutate(interval = factor(interval, levels = c('0.95', '0.99'),\n                           labels = c('95% CI', '99% CI')),\n         'Effect size' = factor(group, levels = c('small', 'medsmall', 'medium'),\n                                labels = c('Very small (.1)', 'Small (.2)', 'Medium (.4)')))\n\n\nA… and then plotting the resulting power curve:\n\nCode(\npower_curve_4_group_lin <- ggplot(data = four.group.lin.summary) +\n  scale_x_continuous(limits = c(100, 1550), breaks = seq(from = 150, to = 1500, by = 150)) +\n  scale_y_continuous(limits = c(0, 100), breaks = seq(from = 0, to = 100, by = 20)) +\n  geom_hline(aes(yintercept = 80), linetype = 'dashed', size = .33, alpha = .25) +\n  geom_hline(aes(yintercept = 90), linetype = 'dashed', size = .33, alpha = .25) +\n  geom_path(aes(x = cell.size, y = `ci above 0 vs. control`, color = `Effect size`,\n                group = `Effect size`), size = .66) +\n  geom_point(aes(x = cell.size, y = `ci above 0 vs. control`, color = `Effect size`),\n             size = 1.5) +\n  labs(y = 'Power to detect a non-zero effect',\n       x = 'Number of participants per condition (control group not included)') +\n  scale_color_manual(values = c('#c10d0d', '#7dc3c2', '#dcc55b')) +\n  facet_wrap(~interval) +\n  theme(\n    aspect.ratio = 1,\n    panel.grid.major = element_line(colour = \"white\", size = 0.33),\n    panel.grid.minor = element_line(colour = \"white\", size = 0.2),\n    panel.background = element_rect(fill = \"grey96\"),\n    axis.line = element_line(color = 'black', size = 0.375),\n    axis.ticks = element_line(color = 'black', size = 0.5),\n    text = element_text(color = 'black', family = 'Gill Sans MT', size = 9),\n    axis.text = element_text(color = 'black', family = 'Gill Sans MT', size = 7),\n    strip.background = element_blank()\n  )\n)\n\n\n\n\nStep 4. Assess the output and determine whether the proposed analyses and inference goals are realistic and likely to yield informative results.\nIn Step 4, we use the information we have generated above to make substantive conclusions about the projected power of our experiment to detect particular effects, given particular underlying parameters. From this we can make recommendations as to experimental design.\nBased on the power curve plotted above, we can conclude that we would have a very high likelihood of detecting effects of .4 versus a control group at even quite low sample sizes, and also a good possibility of detecting effect sizes of .2 at quite modest sample sizes. On the other hand, for the very small effect size, we would not be likely to detect a difference from a control group even with 1500 participants per group. If effect sizes of this sizes are of interest, then we might consider going back to Step 1 and reconsidering our experimental design to include more participants."
  },
  {
    "objectID": "chapters/basic_stats.html",
    "href": "chapters/basic_stats.html",
    "title": "10  Frameworks, ‘models’, inference, tests",
    "section": "",
    "text": "Aim to integrate content from\n\n\n\n\n\n\nWillem’s blog and vignettes\n\nReinstein notes and beyond\n\nThis should not include extensive discussion of causality (which is later), only an introduction to regression as a ‘fitted descriptive line/plane’]\n\n\nLakens on statistical inference\nMcElreath"
  },
  {
    "objectID": "chapters/basic_stats.html#what_models",
    "href": "chapters/basic_stats.html#what_models",
    "title": "10  Frameworks, ‘models’, inference, tests",
    "section": "\n10.1 Types of ‘statistical models’ goals",
    "text": "10.1 Types of ‘statistical models’ goals\nAs discussed in our posts and linked material, we broadly imagine three categories of modeling: (See discussion in the fold/footnote.)\n\nDescriptive modeling: Essentially, offering a “dimension reduction” of the data, presenting ‘which features relate to donation behavior’?\nPredictive modeling: Training a model to produce the best out-of-sample (or out-of-time) fit for current (or future) donations based on individual characteristics. This model should not include ‘leaks’, i.e., it should excluding individual characteristics and outcomes that occur after the time we would be ‘doing the prediction’.\nCausal: Consider identification of ‘causal paths’, i.e., ‘what actually determines amounts donated’.1"
  },
  {
    "objectID": "chapters/basic_stats.html#conceptual",
    "href": "chapters/basic_stats.html#conceptual",
    "title": "10  Frameworks, ‘models’, inference, tests",
    "section": "\n10.2 Discussion: statistics, probability and inference",
    "text": "10.2 Discussion: statistics, probability and inference\nFrequentist, Bayesian, ‘randomization inference’, ‘likelihood-ist’\n\nDR: I am not saying this should be a major focus. We probably don’t want to get too deep here. However, if we do end up discussing these issues, I propose we put it or link it here."
  },
  {
    "objectID": "chapters/basic_stats.html#hypothesis-testing-statistical-comparisons-and-inferences",
    "href": "chapters/basic_stats.html#hypothesis-testing-statistical-comparisons-and-inferences",
    "title": "10  Frameworks, ‘models’, inference, tests",
    "section": "\n10.3 Hypothesis testing, statistical comparisons and inferences",
    "text": "10.3 Hypothesis testing, statistical comparisons and inferences\n\n10.3.1 ‘Common statistical tests are linear models’\n\nMany of the ‘univariate’ tests presented below can be extended to multiple-variable models (e.g., regression coefficients).\nFurther discussion, examples, and tables comparing the statistics by Oska Fentem in his Notion here."
  },
  {
    "objectID": "chapters/basic_stats.html#randomization-and-permutation-based-tests-and-inference",
    "href": "chapters/basic_stats.html#randomization-and-permutation-based-tests-and-inference",
    "title": "10  Frameworks, ‘models’, inference, tests",
    "section": "\n10.4 Randomization and permutation-based tests and inference",
    "text": "10.4 Randomization and permutation-based tests and inference\n2\nBasic description: (still looking for the best source for this)\n\nDiscussion of the difference between randomization inference and bootstrapping here\n\n\n\nBootstrapped p-values are about uncertainty over the specific sample of the population you drew, while randomization inference p-values are about uncertainty over which units within your sample are assigned to the treatment.\n\nThe infer package vignette gives a good walk-through; this package is useful for doing these tests (some also recommend the coin package).3\n\nWe use this, and give some explanation, in the 2021 EAS donations post - see bookdown (see folds within)\n\nWe use permutation tests for testing whether the median (and mean) of planned donations exceeded/fell short of the mean for actual donations, but using the data from different years’ surveys (without connected individuals). …\n\nThe code we use for these tests is permalinked here\n\nWhy use these techniques?\n\nTractable for testing differences in medians\nFairly easy to explain, fairly intuitive\nDo not depend on strong assumptions about underlying distributions or ‘large sample asymptotics’\nSome statisticians and Econometricians (e.g., Athey and Imbens) argue for their value and robustness; it also seems close to what a lot of ‘data science’ people do (they love simulation)\n\n\nDR concerns about ‘population vs super-population’, possibly misinformed:\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n… we nearly always want to make inferences about the population that the treatment and control groups are taken from (even thinking about a hypothetical super-population), not about the impact on the sampled groups themselves. So, with this in mind, when would I still want to use randomization inference."
  },
  {
    "objectID": "chapters/basic_stats.html#particular-experimetrics-issues",
    "href": "chapters/basic_stats.html#particular-experimetrics-issues",
    "title": "10  Frameworks, ‘models’, inference, tests",
    "section": "\n10.5 Particular ‘experimetrics’ issues",
    "text": "10.5 Particular ‘experimetrics’ issues\nShould we include controls (covariates) in analyzing ‘treatment effects’ from randomized experiments?\n\n“in the conventional sampling paradigm… Controlling for observable heterogeneity using a regression model” is required for the assumptions to be justified with this approach. With the randomisation approach it makes more sense to put data into strata by covariates, analyse within-group experiments and average results.” - (?) Athey and Imbens\n\n:::"
  },
  {
    "objectID": "chapters/basic_stats.html#equivalence-tests-and-simple-frequentist-approaches",
    "href": "chapters/basic_stats.html#equivalence-tests-and-simple-frequentist-approaches",
    "title": "10  Frameworks, ‘models’, inference, tests",
    "section": "\n11.1 Equivalence tests and simple frequentist approaches",
    "text": "11.1 Equivalence tests and simple frequentist approaches\nFrom Wikipedia (accessed 12 Apr 2022)\n\nIn equivalence tests, the null hypothesis is defined as an effect large enough to be deemed interesting, specified by an equivalence bound. The alternative hypothesis is any effect that is less extreme than said equivalence bound. The observed data is statistically compared against the equivalence bounds. If the statistical test indicates the observed data is surprising, assuming that true effects are at least as extreme as the equivalence bounds, a Neyman-Pearson approach to statistical inferences can be used to reject effect sizes larger than the equivalence bounds with a pre-specified Type 1 error rate.\n\n\nSeveral tests exist for equivalence analyses; however, more recently the Two-one-sided t-tests (TOST) procedure has been garnering considerable attention. As outlined below, this approach is an adaptation of the widely known t-test.\n\n\n“A very simple equivalence testing approach is the ‘two-one-sided t-tests’ (TOST) procedure. [Schuirmann, 1987] In the TOST procedure an upper (\\(\\Delta U\\)) and lower (-\\(\\Delta L\\)) equivalence bound is specified based on the smallest effect size of interest (e.g., a positive or negative difference of \\(d = 0.3\\)). Two composite null hypotheses are tested: \\(H_{01}: \\Delta \\leq –\\Delta L\\) and \\(H_{02}: \\Delta \\geq \\Delta U\\) When both these one-sided tests can be statistically rejected, we can conclude that \\(–\\Delta L < \\Delta < \\Delta U\\), or that the observed effect falls within the equivalence bounds … .[Searman et al, 1998]” [Lakens 2017]\n\nLakens offers pointers, and TOSTER package, for this approach.\nBayes factors\nSemi-aside: Unresolved discussion of ‘Bayes factors’ (Nik and Reinstein)\nClear DataColada argument against the use of ‘Bayes Factors’ for ‘accepting the null’"
  },
  {
    "objectID": "chapters/basic_stats.html#simple-bayes",
    "href": "chapters/basic_stats.html#simple-bayes",
    "title": "10  Frameworks, ‘models’, inference, tests",
    "section": "\n11.2 Some simple Bayesian testing examples",
    "text": "11.2 Some simple Bayesian testing examples\nWe return to consider a very simple and common case of interest:\n\nWe have a population divided into two treatment conditions (e.g., ‘Impact’ vs ‘Emotion’ language in an email donation appeal)\n\nWe wish to focus on a binary outcome (e.g., ‘whether clicked to donate’), which may be rare in each treatment.\n\nWe want to understand the impact of the treatment condition on the binary outcome, putting probability bounds on our belief about this\n\n\n\n\nThis is a classic example for Bayesian inference. Some good discussions and vignettes:\nDiscussions and walk-throughs\n\nI have a memory that McElreath goes through this, but I cannot find it\nJamie Elsey’s code example: bayes_odds_ratio.R, which uses the brms interface to Stan\nReinstein and Dickerson’s work using bayesian_test_me on ‘dual process’ donation data\n\n\nReady packages\n\n\nbayesAB is easy to apply, allows flexible specification of priors (and has tools to help you visualize these), and generates very clear output and graphs. See vignette\n\nIt (seems to) save the simulation data for you to summarize as you like\nLimitations: Requires raw data for each treatment (not just summarized data), cannot address more than two groups, does not (?) share the ‘raw code doing the work’ for tweaking\n\n\n\nBayesianFirstAid::bayes.prop.test is also easy to use (see vignette), and it works with either vectors or counts.\n\nIt uses a uniform prior, but you can get it to spit out the rjags code and then adjust this\n… or adjust to multiple groups\nThe plots it give you by default are pretty messy, but it also preserves the simulation data, so you could make your own plots"
  },
  {
    "objectID": "chapters/basic_stats.html#simulation_to_bayes",
    "href": "chapters/basic_stats.html#simulation_to_bayes",
    "title": "10  Frameworks, ‘models’, inference, tests",
    "section": "\n11.3 A single framework? “Significance and equivalence testing” with randomization inference/simulation; building to Bayes",
    "text": "11.3 A single framework? “Significance and equivalence testing” with randomization inference/simulation; building to Bayes\n\n\n\nThis is an outline/discussion. I (DR) try to implement and demonstrate this in eamt_data_analysis/oftw_giving_season_upsell_2021_trial/oftw_upsell_input_first_analysis.Rmd\nNote: there may be good ways to hijack all sorts of existing tools, such as the bayesAB package or BayesianFirstAid\n\n\nSuppose we see a ‘small difference’ between treatment groups and it is ‘not significant in standard tests’ (tests not shown here yet).\nHow can we: - Put meaningful bounds on this? - Statistically ‘rule out large effects’? - Do this in the same context as our ‘null hypothesis tests’? - Do and communicate this in a way that is acceptable to a range of audiences?\n(This parallels the analysis done in HERE, which includes dome further explanation of the methods)\n\nI (David) propose taking the following approach:\n\nConstruct (by simulation or analytical formulae) the ‘probability of “some function of our data” given a range of true parameters, i.e., given a range of relevant ’true rates of (relative) incidence’ under each treatment (1/2, 3/4, equal, 4/3, 2, etc). This includes (and perhaps focuses on) the case where ‘the true rates of incidence are equal to one another and equal to the average empirical incidence in our sample.’\n\nWhat “function of our data”?\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nThe exact proportional difference in incidence rates that we see\nA proportional difference in incidence rates as large as we see (in absolute value) or larger (I believe this is the ‘two-tailed frequentist p-value’)\nA proportional difference in incidence rates as large as we see (in absolute value) or smaller (this gives us a sense of ‘how unlikely is a large true effect … can we probabilistically rule out a big difference’)\nA proportional difference in incidence rates as large or larger in favor of the Treatment\nA proportional difference in incidence rates as large as we see in favor of the Control\n… Perhaps similar measures for other statistics such as ‘difference in counts (not proportional), or ’average amount (donated)’ (for the latter, we’d need to consider distinct true distributions of contributions)\n\n\n\n\n\nPlot the above over the parameter range to get a visual picture of the maximum likelihood parameter, and (eyeballing) the relative probability of different ranges\nFor a few important elements of the above, consider the ‘relative likelihood of the data’ under different true rates of incidence (or distributions of donations), for important comparisons such as\n\n\nA relative incidence of 1.5 versus a relative incidence of 1\n\n… If the ratio is very small we might suspect that ’no difference is far more likely than a strong difference in favor of the treatment.\n\nImplicitly considering a ‘flat prior’, integrate (average) and compare important ranges of the above\n\nE.g….\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nthe probability mass of the parameter ‘the incidence is 1.25x or higher in favor of the Treatment’ , versus 1.25x or lower (closer to zero, or higher incidence under the Control)…\n… If the ratio is very small, then, given a fairly flat prior, our posterior should put a low probability on ‘a large difference in favor of the treatment’ …\nAnd similarly, reversing the control and treatment\nAnd similarly, for both extremes relative to a middle incidence range…\n… here, if the ratio is very small, we will say “we can probablistically rule out an effect of 1.5x or larger in either direction”\n\n\n\n\n\nNote:\n\n\n\n\n\n\nNote\n\n\n\n\n\nJamie:\n“To put simply, likelihood is”the likelihood of \\(\\theta\\) having generated D” and posterior is essentially “the likelihood of θ having generated D” further multiplied by the prior distribution of θ. If the prior distribution is flat (or non-informative), likelihood is exactly the same as posterior.”\nSo in essence, if you wanted, you could just put a prior distribution on the differences over which you have simulated differences, and multiply it with your likelihood. To get a proper posterior you would need a continuous distribution for the prior over the parameter space, and probably more simulations across the parameter space as well, to fill in the shape of the posterior. I guess the key difference between what you are suggesting and just running it as a Bayesian binomial regression is that your likelihood function is produced by simulation, whereas the one in the Bayesian regression would be generated through the model assumptions of the regression.\n\n\n\n\nOptionally, repeat the above with a Bayesian tool, considering more than one ‘explicit prior’.\n\nOngoing question: Do we need or want a separate prior for ‘incidence under treatment’ and ‘incidence under control’, or can our prior simply focus on the relative incidence rates?\n\nJamie suggests: Just do a Bayesian analysis and consider the posterior distribution. You can test the sensitivity to different priors."
  },
  {
    "objectID": "chapters/basic_stats.html#empirical-bayes",
    "href": "chapters/basic_stats.html#empirical-bayes",
    "title": "10  Frameworks, ‘models’, inference, tests",
    "section": "\n11.4 ‘Empirical Bayes’",
    "text": "11.4 ‘Empirical Bayes’\nReference: David Robinson “Introduction to empirical bayes examples from baseball statistics” downloaded 28 Mar 2022\n\nEmpirical Bayes estimation, where a beta distribution fit on all observations is then used to improve each individually. What’s great about this method is that as long as you have a lot of examples, you don’t need to bring in prior expectations.\n\nSome data cleaning\n\nsetup baseball-statsp_load(Lahman)\n# Filter out pitchers\ncareer <- Batting %>%\nfilter(AB > 0) %>%\nanti_join(Pitching, by = \"playerID\") %>%\ngroup_by(playerID) %>%\nsummarize(H = sum(H), AB = sum(AB)) %>%\nmutate(average = H / AB)\n# Include names along with the player IDs\ncareer <- Master %>%\ntbl_df() %>%\ndplyr::select(playerID, nameFirst, nameLast) %>%\nunite(name, nameFirst, nameLast, sep = \" \") %>%\ninner_join(career, by = \"playerID\") %>%\ndplyr::select(-playerID)\n\n\ncareer_filtered <- career %>%\nfilter(AB > 100) #he uses 500, I think 100 is enough"
  },
  {
    "objectID": "chapters/basic_stats.html#step-1-estimate-a-beta-prior-from-all-your-data",
    "href": "chapters/basic_stats.html#step-1-estimate-a-beta-prior-from-all-your-data",
    "title": "10  Frameworks, ‘models’, inference, tests",
    "section": "\n11.5 Step 1: Estimate a [Beta] prior from all your data",
    "text": "11.5 Step 1: Estimate a [Beta] prior from all your data\nWe set up a likelihood function… probability of the data given different parameters. It sums the log of ‘how likely is each row (batter) given that batting averages are drawn particular true beta distribution, and hits are drawn from this’. At bats are taken as a constant, I guess\n\nCodelibrary(stats4)\np_load(VGAM)\n\nll <- function(alpha, beta) {\nx <- career_filtered$H\ntotal <- career_filtered$AB\n\n-sum(VGAM::dbetabinom.ab(x, total, alpha, beta, log = TRUE)) #Note this is outputting the negative Log likelihood (not subtracting it from the last line)\n# 'how likely\n}\n\n\nNext we search for the ‘most likely parameters’ … the alpha and beta that maximize the above (using a particular algorithm).\n\nCode# maximum likelihood estimation\nm <- mle(ll, start = list(alpha = 1, beta = 10), method = \"L-BFGS-B\",\nlower = c(0.0001, .1))\nab <- coef(m)\nalpha0 <- ab[1]\nbeta0 <- ab[2]"
  },
  {
    "objectID": "chapters/basic_stats.html#step-2-use-that-distribution-as-a-prior-for-each-individual-estimate",
    "href": "chapters/basic_stats.html#step-2-use-that-distribution-as-a-prior-for-each-individual-estimate",
    "title": "10  Frameworks, ‘models’, inference, tests",
    "section": "\n11.6 Step 2: Use that distribution as a prior for each individual estimate",
    "text": "11.6 Step 2: Use that distribution as a prior for each individual estimate\nWith the Beta prior and Binomial distribution, it turns out there is a simple rule for finding the midpoint (MAP?) of the posterior for the mean. Adjust the ‘average’ formula by … taking the numerator (hits) and add \\(\\alpha_0\\) from the Beta prior. Take the denominator (at-bats) and add \\(\\alpha_0 + \\beta_0\\).\nSo if someone got 30 hits out of 100, our midpoint estimate for their true ‘data generating’ batting average is \\(\\frac{30+\\alpha_0}{100+\\alpha_0+\\beta_0}\\)\nWe can do this across all the data, and find the ‘best and worst’ batters:\n\nCodecareer_eb <- career %>%\nmutate(eb_estimate = (H + alpha0) / (AB + alpha0 + beta0))\n\n\ncareer_eb %>% arrange(eb_estimate) %>% head(5)\n\n\n\n\n\n\n\n\n\nname\nH\nAB\naverage\neb_estimate\n\n\nBill Bergen\n516\n3028\n0.17 \n0.178\n\n\nRay Oyler\n221\n1265\n0.175\n0.191\n\n\nJohn Vukovich\n90\n559\n0.161\n0.195\n\n\nJohn Humphries\n52\n364\n0.143\n0.195\n\n\nGeorge Baker\n74\n474\n0.156\n0.196\n\n\n\nCodecareer_eb %>% arrange(-eb_estimate) %>% head(5)\n\n\n\n\n\n\n\n\n\nname\nH\nAB\naverage\neb_estimate\n\n\nRogers Hornsby\n2930\n8173\n0.358\n0.355\n\n\nShoeless Joe Jackson\n1772\n4981\n0.356\n0.35 \n\n\nEd Delahanty\n2597\n7510\n0.346\n0.342\n\n\nBilly Hamilton\n2164\n6283\n0.344\n0.34 \n\n\nWillie Keeler\n2932\n8591\n0.341\n0.338\n\n\n\n\nAs noted (and plotted) in the book, this method tends to ‘shrink’ the averages towards the overall midpoint. The lowest averages are raised a bit, and the highest averages are reduced a bit … with more ‘shrinkage’ where there are fewer observations (at-bats)."
  },
  {
    "objectID": "chapters/factor_analysis.html",
    "href": "chapters/factor_analysis.html",
    "title": "11  Factor analss: EFA/CFA, PCA",
    "section": "",
    "text": "Dimension reduction, and ‘carving reality at its joints’, uncovering structural parameters?"
  },
  {
    "objectID": "chapters/factor_analysis.html#where-have-we-used-thiswhere-do-we-anticipate-using-it",
    "href": "chapters/factor_analysis.html#where-have-we-used-thiswhere-do-we-anticipate-using-it",
    "title": "11  Factor analss: EFA/CFA, PCA",
    "section": "\n11.1 Where have we used this/where do we anticipate using it?",
    "text": "11.1 Where have we used this/where do we anticipate using it?"
  },
  {
    "objectID": "chapters/factor_analysis.html#the-idea",
    "href": "chapters/factor_analysis.html#the-idea",
    "title": "11  Factor analss: EFA/CFA, PCA",
    "section": "\n11.2 The idea",
    "text": "11.2 The idea"
  },
  {
    "objectID": "chapters/factor_analysis.html#pca",
    "href": "chapters/factor_analysis.html#pca",
    "title": "11  Factor analss: EFA/CFA, PCA",
    "section": "\n11.3 PCA",
    "text": "11.3 PCA"
  },
  {
    "objectID": "chapters/factor_analysis.html#factor-analysis",
    "href": "chapters/factor_analysis.html#factor-analysis",
    "title": "11  Factor analss: EFA/CFA, PCA",
    "section": "\n11.4 Factor analysis",
    "text": "11.4 Factor analysis\n\n11.4.1 Conceptual question 1: How can we have correlated traits but no overlap in factor loadings?\nFactors represent ‘real but latent ways people differ’ (in personality psych)\n\n“We don’t want cross-loadings” (in the case where we use sum scores). We don’t want the ‘factor loadings’ to be nonzero (or far from 0) on the same items for multiple factors.\n\nDoes this make sense?\n… We don’t want our indicators (survey measures) to be measuring more than one true factor, I guess. We don’t want questions we think are about happiness to really be picking up sociability. But can such questions be designed? Suppose the questions do pick up multiple personality characteristics. If we redefine the ‘factors’ so that they only load on separate sets of question, I don’t see how these redefined factors will be truly reflecting the personality characteristics.\nWe know\n\nthe latent personality traits are correlated (e.g., happy people are more sociable)\nThus, people who are happy will tend to be more sociable, and thus tend to respond more positively to responses to questions about happiness and questions about sociability\nSo if we restrict ‘only questions about sociability can load onto factor’, I claim that the value of this factor will tend to be higher for people who are happy (I know we are not restricting it, but that is the stated goal, so we should consider what it implies)\nThus, ‘does this imply that the factor is measuring happiness and not just sociability?’\n\nPerhaps the answer is something like\n\nYes, each sociabity question will tend to be higher for people who are happier (higher on the true ‘happiness factor’). However, the estimated ‘sociability factor level’ for an individual (summed factor loadings times individual values of items) need not be reflected in the loadings on the happiness items. If we are ‘already adequately picking up sociability’ through the loadings on the sociability questions, what remains (residual?) need not be related to the happiness items.\n\n\n\n\n\n\n\nFurther thoughts and insight into ‘how do PCA and FA differ?’\n\n\n\n\n\nI know what principal component analysis does: it reduces dimensionality. It projects your outcomes into a series of vectors (components) so that each ‘individual’ can be described by a value for each component. This score ‘predicts’ her value for each of the outcomes, and the algorithm does this to minimize the sum of squared differences (SSE) between the prediction and actual. A natural result of ‘minimizing the SSE’ is that each component vector will be set to be orthogonal to each other vector. How many vectors (componenets)? That’s another issue, and there are various rules of thumb and metrics.\nI know what factor analysis intends to do, and why this is conceptually different from PCA.\nExplanation here about ‘latent factors’, with genes as an example, before they were able to be observed … We only observe and measure physical and behavioral outcomes, like height, weight, math performance, and verbal performance on specific tests. We want to predict ‘how many genes determine these’ and ‘how to best predict the value of these genes for each individual’. Note that the genes themselves may be correlated. If there is a ‘growth’ gene and an ‘intelligence’ gene, they may tend to be positively related to one another, because of other factors (assortative mating, health and nutrition in early childhood, etc.)\n\nTrue ‘factors’ (genes): intelligence, growth\n\n(However, we do not know how many true factors/genes there are… we just know there are some nmber).\n\nMeasured ‘items’: height, weight, hand_size, verbal_score, math_score, spatial_score\n\nWhat I’ve been stuck on is ’mechanically, what is it that FA does to “better recover these genes (factors)” that PCA does not do. It is clearly not merely trying to maximize prediction… because that would be PCA. Maybe it maximizes predictiveness (minimizes some error term) subject to some constraints … where imposing these constraints makes it more likely that we are identifying genes, i.e., makes the factors a better estimate of these gene values.\nOne possible ‘constraint’ that might be meaningful: Maybe we know that specific genes cause (or ‘have a (causal) impact on’) certain behaviors and traits, although we do not know which ones and by how much. (A stronger assumption would be that ‘only a single gene causes each measured behavior or trait’ … but that’s probably too strong and unrealistic.) This (weaker or stronger assumption) will imply that certain traits, the traits ‘caused’ by any gene, must indeed be correlated in the population. If the ‘growth’ gene causes measured height, weight, and hand size, then people will tend to have high, medium, or low values of all of these, depending (at least in part) on the value of their growth gene. If the ‘intelligence’ gene causes measured verbal, math, and spatial performance … these will be similarly related.\nIn the other hand, the axes of variation (‘principal component vectors’) that most succinctly predict all of the measured items above might involve ‘substantial mixing’ of the physical measures and scores, even measures that do not themselves tend to be strongly correlated to one another. In other words, multiple items given positive weights in each vector might actually be negatively correlated in the population. Why?\nIs this a helpful example?: The data may mix people from two cities, one of which pushes fatty foods but has great math education. This may then lead to the most predictive vectors (here I state them as sums, crossing them with the values) being something like (wait, I need to make these orthogonal … ugh) \\(V1 = -1 height + 7 weight + 8 math - 1 verbal -1 spatial\\) and \\(V2 = 2 height -3 weight + 2 math + 1 verbal + 8 spatial\\). But in fact, (do we know this?) height and weight are correlated in the population, as are all of the ‘intelligence’ scores. The psychometrician doesn’t know this (because this is ‘exploratory’), but they do know that whatever the latent factors are, they will imply items all caused by the factor, and thus given positive weights within a factor must (?) be correlated to one another. (At least if multiple, negatively correlated genes/factors are not each having important impacts on these items).\nSo the psychometrician says (perhaps driven by some theoretical model of genes, stated in explicit equations):\n\nI am going to require that, whatever my factors are, the items given substantial positive weights within them must be positively correlated in the population\n\nBut what are ‘substantial positive weights’? Perhaps the metric itself is weighted; the more positive the weights in each factor, the more correlated the items must be.\nThis then may lead them to estimate a sort of ‘constrained PCA’ model … maximizing the predictiveness under the constraint like \\(R_{12} := \\rho(\\gamma_1 x_1, \\gamma_2 x_2) > 0.6 \\forall x_1, x_2\\), where \\(\\rho\\) is a Pearson’s correlation coefficient, and the \\(\\gamma\\)’s represent the factor loadings on any two items \\(x_1\\) and \\(x_2\\).\nOr perhaps, rather than a constaint, the FA may maximize a value function that considers both these \\(R_{ij}\\) measures and the predictive power (the latter as in PCA).\nIf the ‘theory is correct’ and ‘factors tend to contain correlated items’, then imposing the constraint (or ‘reward in the value function’) that items should be correlated, will likely yeild better estimates of the true factors.\nBut what does this say about ‘using FA to choose which items to include?’\n\n\n\n\n11.4.2 Suggested readings\n\n“Best practices in EFA” (Jason W. Osborne)\nDynamic Fit Index Cutoffs for Confirmatory Factor Analysis Models\n\n11.4.3 Integrate…\nWillem’s work link: ‘How to Science’\nhttps://willemsleegers.github.io/how-to-science/content/statistics/factor-analysis/\n\nCodeknitr::include_url(\"https://willemsleegers.github.io/how-to-science/content/statistics/factor-analysis/\")"
  },
  {
    "objectID": "chapters/mixed_multilevel_random.html",
    "href": "chapters/mixed_multilevel_random.html",
    "title": "12  Random/mixed fx, mlm",
    "section": "",
    "text": "(Related to 3, I think)↩︎"
  },
  {
    "objectID": "chapters/ml_modeling.html",
    "href": "chapters/ml_modeling.html",
    "title": "\n13  Prediction, & ML\n",
    "section": "",
    "text": "Codelibrary(rethinkpriorities)\nsource(here(\"code\", \"methods_setup.R\"))\n\nError in here(\"code\", \"methods_setup.R\"): could not find function \"here\""
  },
  {
    "objectID": "chapters/ml_modeling.html#consult-modeling-vignettes",
    "href": "chapters/ml_modeling.html#consult-modeling-vignettes",
    "title": "\n13  Prediction, & ML\n",
    "section": "\n13.1 Consult modeling vignette(s)",
    "text": "13.1 Consult modeling vignette(s)\nSee Vignette: Modeling donations in EA Survey data with Tidymodels and workflow"
  },
  {
    "objectID": "chapters/ml_modeling.html#pred-insights",
    "href": "chapters/ml_modeling.html#pred-insights",
    "title": "\n13  Prediction, & ML\n",
    "section": "\n13.2 Prediction models for insights?",
    "text": "13.2 Prediction models for insights?\n\n\n\n\n\n\nDiscussion: why build ‘predictive models of donations’, for example?\n\n\n\n\n\nWe focus on predicting the individual’s donation in a year, focusing on the same set of outcomes used in the previous section. For this model to be useful for an actual prediction problem going forward, it would need to rely on ‘ex-ante’ characteristic that were already observable at the time of a career/EtG/pledge decision.1 These might include immutable demographics, career plans, and pledges previously taken, and consider year and trend effects.\nAlthough we have these models in mind, this is not what we are doing here. We are not posing a specific ‘prediction problem’ per se. Instead we are using machine learning tools built for prediction problems to generate ‘data-driven insights’ about factors related to EA donation behavior. Here, we do not than directly specifying all of the included components of the model (features, interaction terms, etc.). Instead we provide a large set of possible ‘inputs’ and use ML techniques to train models that should predict well outside of the data they are trained on. These models should do a good job of accomplishing the task: ‘if you gave me a set of features of an EA, I should have a fairly accurate guess at what they will donate.’\nThe insights from these models should also be treated with caution. Again, they may not be deriving causal relationships. Furthermore, the parameters derived from model-fitting ML procedures are not in general unbiased or consistent, and it is difficult to derive proper confidence intervals for these parameters.\nStill, the benefit of this exercise may be considered ‘the derivation of robust and predictive relationships in the data that are mainly driven by the data itself, rather than our preconcieved ideas.’ These models may also be useful building blocks towards future predictive work."
  },
  {
    "objectID": "chapters/ml_modeling.html#penalized-reg",
    "href": "chapters/ml_modeling.html#penalized-reg",
    "title": "\n13  Prediction, & ML\n",
    "section": "\n13.3 Penalized regression models",
    "text": "13.3 Penalized regression models\n\n\n\n\n\n\nDiscussion: ‘elastic net’ models:\n\n\n\n\n\nIn brief, the elastic net models involve linear models (log-linear in our case), i.e., ‘regressions’, that carefully ‘penalize’ the (squared) magnitude of coefficients, in effect shrinking these towards zero. The penalties are specifically ‘tuned’ and ‘validated’ to maximize the predictive power of the model. As these are essentially regression approaches, we can report the sign and magnitude of the coefficients used in the ‘optimally tuned’ predictive model. (However, we should be careful about interpreting these parameters, and statistical inference is challenging. See e.g., (Mullainathan2017?) for a detailed discussion.)\nThe GLMnet approach combines ‘ridge (L2 norm) and lasso (L2 norm)’, tuning the mix of each, as well as tuning the penalization parameters within each.\nBoth the sum of absolute value coefficients (L1 norm) and the summed square of these (L2 norm) ma y be penalized. Thus, these will be ‘shrunk towards zero’ relative to a comparable OLS (standard linear model) estimate. How much do we ‘charge’ for the absolute or squared sums? This is ‘tuned’, i.e., we see what combinations perform best in the cross-validation exercise.\nIn some cases (because of the L1 norm), coefficients may be dropped entirely, i.e., ‘shrunk all the way to zero’.2"
  },
  {
    "objectID": "chapters/ml_modeling.html#metrics-of-fit",
    "href": "chapters/ml_modeling.html#metrics-of-fit",
    "title": "\n13  Prediction, & ML\n",
    "section": "\n13.4 Metrics of fit",
    "text": "13.4 Metrics of fit\nWe may want to consider ‘how successful’ our predictive models are at making practically useful predictions. In other words, ‘how far off’ are the predictions and classifications on average, from the actual outcomes. This procedure considers the fit on randomly-drawn set-aside ‘testing data’, data that has not been used in ‘training’ (or ‘fitting’) the model.\nIn the vignette here we consider and discuss the root-mean-square-error (RMSE) and mean-absolute-error (MAE) metrics in the context of predicting donatino outcomes.\nIn order to assess the usefulness of each predictive regression model we consider both root-mean-square-error (RMSE) and mean-absolute-error (MAE). RMSE (aka RMSD) can be interpreted as the average ‘Euclidean distance’ between the actual values and the model’s prediction. For each observation (in the set-aside ‘testing sample’), to construct RMSE we:\n\nMeasure the differences between the actual and predicted outcome (e.g., donation)\nSquare these differences\nTake the average of these squared differences across all observations\nTake the square root of this\n\nTo construct mean-absolute-error (MAE) we simply\n\nMeasure the absolute-value differences between the actual and predicted outcome (e.g., donation) for each observation\nTake the average of these across all observations\n\nMAE has a much more straightforward interpretation: it simply asks ‘how far off are we, on average?’\nWhile the RMSE is used in the model fitting for various reasons, it is arguably less-interpretable and less-relevant than MAE in judging the model’s fit in cases like this one. RMSE error negatively assesses the model fit based on squared deviations, and is thus very sensitive to ‘large mistakes’. This may be relevant where ‘large errors are much much worse than small ones’ – here, this is not so clearly the case. In the presence of data with large outlying observations, prediction will tend to be poor by this measure.\nNote that when considering models where the outcome is transformed (e.g., log(donations)) we construct the RMSE and MAE by exponentiating to generate predictions for the level outcomes, and then measure the deviations on the level scale.3"
  },
  {
    "objectID": "chapters/ml_modeling.html#resources-and-use-cases",
    "href": "chapters/ml_modeling.html#resources-and-use-cases",
    "title": "\n13  Prediction, & ML\n",
    "section": "\n13.5 Resources and use-cases",
    "text": "13.5 Resources and use-cases\n\nOska ML notes (Notion)\n\nPredictive models of donations in EA Survey\n\nCode to fit models in this folder\n\n… uses tidymodels including workflow_set and ‘recipes’ and the parsnip package\n\n\nGeneral intuitive discussion: “Data Science for Business by Foster Provost and Tom Fawcett (2013)”; DR notes here and here"
  },
  {
    "objectID": "chapters/time_series_application.html",
    "href": "chapters/time_series_application.html",
    "title": "14  Time Series (applied)",
    "section": "",
    "text": "‘Predict the influence of discrete events’ (and more) on web traffic, signups, etc"
  },
  {
    "objectID": "chapters/time_series_application.html#petes-general-notes-on-time-series-in-a-predictionml-context",
    "href": "chapters/time_series_application.html#petes-general-notes-on-time-series-in-a-predictionml-context",
    "title": "14  Time Series (applied)",
    "section": "14.1 Pete’s General notes on time series (in a prediction/ML context)",
    "text": "14.1 Pete’s General notes on time series (in a prediction/ML context)\n7:::\nIn Machine Learning, Time series problems are problems that involve forecasting (extrapolating) the future based on the information of the past. Typically we have to make a chain of non-independent predictions rather than predict discrete, independent events.\n\nDR: this prediction problem should be distinguished from ‘time series econometrics’, see, e.g., Diebold’s text, which focuses on estimating fundamental structural parameters, and considers forms of ‘causality’.\n\nMost machine learning problems assume that the order of rows don’t matter and that each row is independent of each other. Since time series problems involve time-ordered rows where past events may influence future events, the rows are not independent and this independence assumption is violated.\nAnother key assumption of ML is that the training data is similar to the data being predicted (in this case, future data). This assumption must be true for time series as well.\n\n14.1.1 Decomposition\nYou can decompose a time series into four key parts:\n\nThe trend (T), where the mean is changing over time (e.g., sales generally keep increasing over time)\nSeasonality (S) (e.g., sales are higher in the holiday season)\nA non-seasonal cyclical (C) component (e.g., stock market follows “business cycles”). This is distinct from seasonality as seasonality has a fixed period (e.g., every November), whereas a cycle does not.\nA random component (e)\n\n\nDR: The ‘random component’ could be distinguised or described further; there are various types of ‘random’ terms … shifts, changes in trends, ‘moving average’ one-off terms, ‘autoregressive’ error terms\n\n\n\n14.1.2 Types of decomposition\nThere are two basic types of decomposition – addititive, where y = T + S + C + e and multiplicative, where y = T * S * C * e."
  },
  {
    "objectID": "chapters/time_series_application.html#stationary-vs.-non-stationarity",
    "href": "chapters/time_series_application.html#stationary-vs.-non-stationarity",
    "title": "14  Time Series (applied)",
    "section": "14.2 Stationary vs. non-stationarity",
    "text": "14.2 Stationary vs. non-stationarity\nStationary time series (a) do not have a trend and also (b) do not have variance that changes over time. Non-stationary time series’ do have (a) and/or (b). Typically (a) and (b) create problems for modeling, as models have trouble extrapolating these and they tend to violate the assumption that the training data is similar to the data being predicted.\n\nDR: There can be trend-stationary series, that are stationary after including a trend. These are pretty easy to deal with. Note also that nonstationary series can be described as following a ‘random walk’\n\n\nBut I’m also not convinced that this should necessarily be a problem in a prediction problem. If the series is a random walk/nonstationary … we can still use that knowledge to make a decent prediction of where the outcome will be at time T+t given it’s value at time T.\n\n\n14.2.1 Converting to stationary\nWe can resolve these issues by converting a non-stationary series to a stationary series. This is done by differencing, where we look at the differences in the target over time rather than the actual target (y[t]-> y[t] - y[t-1]).\n\nDR: Most economic time series are in fact stationary after first-differencing. However, this is not guaranteed. You may still want to test for stationarity after first-differencing. (But my memory is that the whole tests for stationarity thing is a huge can of worms).\n\nWe can also handle exponential trends using techniques like log transformations."
  },
  {
    "objectID": "chapters/time_series_application.html#handling-features",
    "href": "chapters/time_series_application.html#handling-features",
    "title": "14  Time Series (applied)",
    "section": "14.3 Handling Features",
    "text": "14.3 Handling Features\n\n14.3.1 Lags\nLagging is pretty key to time series. A lag is when you use the value from the previous series to forecast the next series. You can lag the target variable (e.g., use last month’s sales to predict next month’s sales) and/or you can lag independent variables. Lagged target variables often have strong explanatory power because the real world has delays (e.g., it takes a few weeks for marketing to transition to sales so marketing spend from three weeks ago may be more predictive than marketing spend of the same week) and causations that occur over time (e.g., sales from last year show that the store is more popular so there is more word of mouth and it is even more popular the next year).\nLagging always involves losing some data, as if you are using data from the previous month you won’t be able to use the first month in your training data (because there’s no previous month data for month -1). If you are using data from the three previous months, you won’t be able to train on the first three months. This may not be an issue though, because unlike with non-time series problems, more data in time series isn’t always better (since you might prefer to only be modeling the most recent trend).\nWe can mix lags of different lengths.\n\n\n14.3.2 Rolling Statistics\nLags aren’t the only thing we can do - we can also calculate rolling statistics, like the mean of a variable over the past 14 days. You can also do rolling stats on differences.\n\n\n\n14.3.3 Which Lags / Rolling Stats to Use?\n\nAssess with cross correlation function, which tests correlation of many different lags.\n\n\nooAssess with partial autocorrelation function (PACF), which gives the partial correlation of a stationary time series with its own lagged values, regressed the values of the time series at all shorter lags. Also can assess with the autocorrelation function (ACF), which does not control for other lags.\n\nchanging lag lengths based on data type, domain knowledge, and how quickly you think the series reacts to change (e.g., use shorter lags for stocks)\ncompare by backtesting\n\n\n\n14.3.4 Known in Advance vs. Not\nWhen we are trying to predict in the future, we run into an issue that the features we are using for prediction might also be unknown at prediction time. For example, our historical data might contain information about rainfall and how that connects to sales, but we can’t reliably know the rainfall three months in the future to predict future sales.\nHowever, some features are known in advance - like Christmas time may also have a big impact on sales and we always know exactly when Christmas will be.\nFor features that are not known in advance, we can still use them by either explicitly forecasting them, extrapolating them out using lags / rolling stats of sufficient size, or extrapolating from current values using differing forecast differences."
  },
  {
    "objectID": "chapters/time_series_application.html#validation",
    "href": "chapters/time_series_application.html#validation",
    "title": "14  Time Series (applied)",
    "section": "14.4 Validation",
    "text": "14.4 Validation\nNormally for ML problems we use cross validation, where we randomly partition the data and then predict one partition using data from all the other partitions. The problem with this for time series is that this will involve using future data to predict the past, which will make for unrealistically good predictions.\nInstead, we can use backtesting where we predict a future time using a window of past times (e.g., predict Year 6 using Years 3-5, predict Year 5 using Years 2-4, predict Year 4 using Years 1-3).\nFor metrics, it’s good to compare the evaluation metric to a naive baseline model, or an intentionally minimal extrapolation."
  },
  {
    "objectID": "chapters/time_series_application.html#types-of-models",
    "href": "chapters/time_series_application.html#types-of-models",
    "title": "14  Time Series (applied)",
    "section": "14.5 Types of Models",
    "text": "14.5 Types of Models\n\nIntegrated model - move model one step at a time, e.g., ARIMA, exponential smoothing - univariate\nForecast distance model - predict fixed distances, e.g., XGB\nTrend and decomposition model - predict a different model for each distance, e.g., FB Prophet\n\n\n\n14.5.1 ARIMA\nAutoregressive process: AR(p): fit coefficients to p lags\nMoving average process: MA(q): fit coefficients to q previous errors\nARMA model: X = f(AR, MA)\nI(d) = Differencing d times\nARIMA(p, d, q) = AR(p) + I(d) + MA(q)"
  },
  {
    "objectID": "chapters/time_series_application.html#multiseries",
    "href": "chapters/time_series_application.html#multiseries",
    "title": "14  Time Series (applied)",
    "section": "14.6 Multiseries",
    "text": "14.6 Multiseries\nPredict a different time series for each unit (e.g., sales by store). Can use features across series (cross series features)."
  },
  {
    "objectID": "chapters/classification_model_notes.html",
    "href": "chapters/classification_model_notes.html",
    "title": "15  Classification models",
    "section": "",
    "text": "Below, I try to explain “Classification problems”, “Precision and Recall”, and ROC and ROC-AUC (machine learning validation) by analogy …\nThis loosely connects to the classification models used in the ‘EA Survey Donations post/bookdown’, particularly the consideration of model performance\nSpy-detection We have been asked to come up with a method for classifying spies based on observable characteristics of a suspect population (‘population’ in the statistical sense). For example, some of them have mustaches, some wear pointed hats, some have sinister cackles and lapdogs to a greater or less or degree, and some appear to be more wealthy than others… Let us suppose that: We have previous similar suspect populations that we know to be the same in nature (‘drawn from an identical pool’) to the future suspect populations that we will need to make these predictions for We ultimately learned with certainty which of the previous suspect populations turned out to be spies (To keep intuition and calculation simple) suppose that in these populations know that half of all of these people are spies, and half are completely innocent. (This is called ‘balanced classes’).\nWe are asked to come up with a method to use the observable characteristics to classify each individual as more or less likely to be a spy. If we are given a group of individuals’ characteristics our method needs to assign a number to each one that might be interpreted as the “likelihood this person is a spy”. For example our ‘model’ we might say “anyone with a mustache is 75% likely to be a spy, and anyone without a mustache is 25% likely to be a spy.” (If half of the population wears mustaches, this would also give us an average prediction in line with the known overall 50% spy rate.) Or, let’s make the ‘model’ a bit more complicated (but to allow ‘ordering’ predictions) … we might say … we give the probability of being a spy as\n\\(0.75 - 0.25*wtdev\\)\nif they wear a moustache and\n\\(0.25 - 0.25*wtdev\\) otherwise.\nwhere \\(wtdev\\) represents some “normalized” measure of the person’s weight relative to the average weight in the sample, We predict skinnier people are slightly more likely to be spies.\nNote that, since everyone’s weight is slightly different, this will yield a slightly different projected probability for any observed individual. Thus, our model could also be seen as a “ranking” where, for any group of individuals we are given we can order them from least likely to be a spy (the fattest clean-shaven people) to most likely to be a spy (the skinniest moustache-wearer).\nHow should we evaluate a model, in absolute terms, or against other models? This depends on how the models predictions will be used and on the cost of getting it right or wrong in either direction.\nOur model might be applied by a very soft judge (think Mr. Van Driessen from B&B) who thinks it is much worse to convict an innocent person then to set free many spies. On the other hand it might be applied by a very tight-ass judge (think the gym coach) who puts almost no cost on convicting the innocent … he just wants to cath spies. How can we conceive of the ‘success’ of our model? (edited)\n\nWe could compare across different models to see if one does ‘universally better or worse’ in a sense to be defined. Suppose each model both gives a probability of being a spy for each person. As noted, this implies that each model “rank-orders” a population of individuals in terms of most to least likely being a spy.\n\n\nIn a given (test) set of data, we might see something like the number line dots thing above. The person our model claims is most likely to be a spy is on the right and the one predicted least likely is on the left. In application the liberal judge (“vDriessen”) might convict only the “top 1/3” of the distribution, while the tight-ass (“Coach”) might put 2/3 of these ‘dots’ in prison. Each have made different types of mistakes in practice. vDriessen let a bunch of spies free (and falsely convicted only a few), while the Coach put a lot of innocent people in prison (and, in practice, but not in the dots example above, also let a few spies free). (edited)\nWe might consider that our model has some probability of being applied by various types of judges. It might be that our model performs “better” than an alternative model when applied by the liberal vDriessen but “worse” when applied by the strict Coach. On the other hand, we can imagine that some models might perform better for nearly all types of judge! How could we measure this? The ROC curve can be helpful:\n\nAbove we see models that are strictly ordered from best to worse … the lines do not cross (this need not always be the case). The blue line to picks or model that is strictly better than the orange line, which is itself strictly better than a “random classifier”. A ‘random classifier’ give us no information. If we use this we can only choose some probability to convict everyone. Using this, if we increase the rate we can convict people by 2 per 100, then on average we are convicting one more spy and one more innocent person, in our example. So, by what measure is the blue classifier ‘better’ than the orange one (and both are better than the random classifier)? (edited)\nRemember, each classifier gives a ‘ranked ordering’ of the population. Suppose that we consider going down the list from the most guilty looking to the most innocent looking, and increasing the number of people that we choose to convict.\nAs we go down the ranking in general and decide to convict a larger share of the population we are adding some possibility of convicting and innocent person (the ‘cost’) to gain the ‘benefit’ of convicting some true spies. As depicted in the ROC curves above, as we do this the blue curve (classifier) is achieving a better tradeoff than the orange, green or red curve … (this is because it gives us a ‘better ordered list of who us more likely to be guilty’)…\nAs the judge using the blue curve decides to go further down the ‘blue curve list’ (paying the cost of convicting more innocent people) he also catches more spies than if he had used the ‘orange, green, or red curve list’ . The benefit to cost ratio is higher at all ‘rates of conviction’ here. (edited)\nThis is fine in a relative sense … we know blue is better here (better than all the others except for the purple ‘perfect classifier’ … which is equivalent to a tool that always ‘perfectly orders’ guilt so there is no tradeoff)\nBut:\n\nWhat if the curves cross? and\nHow do we consider the “overall” success of these classifier models? Maybe they are all doing a terrible job.\n\n(gotta run now, will try to get back to this … for question 2 the ‘area under the curve’ seems like a decent measure that somehow ‘averages’ the gain in insight over the random classifier for all possible types of judge, if I understand correctly)\nI have been told that the following is the case:\n\nGiven randomly chosen examples from class 1 and 2, the AUC is the probability that our classifier gives a larger probability of assigning the class 1 example to class 1 (true positive) than the class 2 example (false positive)\n\nI am reinterpreting this more simply as…\n\nCompare a randomly chosen spy (case \\(i\\) where \\(y_i=1\\)) to a randomly chosen innocent person (case \\(i\\) where \\(y_i=0\\)) … (in the testing data). Recall that a ‘classifier’ (at least typically, e.g., as in a logistic regression) assigns to each case \\(i\\) some probability \\(\\hat(p_i)\\) of being a spy, based on its features. The AUC tells me “the probability that my classifier assigns a larger probability of guilt \\(\\hat(p_i)\\) to the randomly-chosen spy than to the randomly-chosen innocent person.”\n\nAn analogy has also been made to the Wilcoxon Rank-sum measure."
  },
  {
    "objectID": "chapters/causal_inf.html",
    "href": "chapters/causal_inf.html",
    "title": "16  Causal Inference",
    "section": "",
    "text": "Basic ideas and frameworks (simple, potential outcomes, DAGs)\nPitfalls and mistakes (layman’s terms)\nThe experimental ideal\nNon-experimental approaches to causal inference\nDealing with attrition\nSome stuff to integrate from here perhaps."
  },
  {
    "objectID": "chapters/causal_inf.html#useful-resources",
    "href": "chapters/causal_inf.html#useful-resources",
    "title": "16  Causal Inference",
    "section": "Useful resources:",
    "text": "Useful resources:\nCausal Inference: What If. R and Stata code for Exercises\nThe effect – bookdown, looks great, love nick HK\nCausal inference: the Mixtape\nUseful R Packages for Causal Inference in the Social Sciences\nApplied causal analysis with R\nMostly Harmless Econometrics (R code – where was it?)"
  },
  {
    "objectID": "chapters/fermi.html",
    "href": "chapters/fermi.html",
    "title": "\n17  MonteCarlo ‘Fermi est.’\n",
    "section": "",
    "text": "Codelibrary(pacman)\np_load(rethinkpriorities)\nlibrary(rethinkpriorities)\nlibrary(dplyr)\nlibrary(tibble)\n‘BOTEC’: Back of the envelope calculations are central to RP’s work\n‘Fermi estimation’ is essentially a more formal approach to this, carefully defining and explaining each element of the ‘model’ equation.\nWhen we explicitly define (and justify) a probability distribution over each variable in the model, and compute (often through simulation) the overall uncertainty of the outputs (predictions, estimates, etc), we call this “Monte Carlo Fermi Estimation”.1"
  },
  {
    "objectID": "chapters/fermi.html#how-to-measure-anything-by-douglas-hubbard",
    "href": "chapters/fermi.html#how-to-measure-anything-by-douglas-hubbard",
    "title": "\n17  MonteCarlo ‘Fermi est.’\n",
    "section": "\n17.1 ‘How to measure anything’, By Douglas Hubbard",
    "text": "17.1 ‘How to measure anything’, By Douglas Hubbard\nSee\n\nbook website here\n\nRP book group notes and links (private access)\nLuke M’s summary at Lesswrong\n\nz\nChapter 3: The Urn of Mystery Simulation\n\n\n\n\n\n\nChapter 3: The Urn of Mystery Simulation\n\n\n\n\n\nThe point\n\nThe Single Sample Majority Rule (i.e., The Urn of Mystery Rule): Given maximum uncertainty about a population proportion – such that you believe the proportion could be anything between 0% and 100% with all values being equally likely – there is a 75% chance that a single randomly selected sample [i.e., one ball drawn from the urn] is from the majority of the population\n\nDiscussion of exercise\n\nThis is a simulation that represents the Urn of Mystery example mentioned in Chapter 3, pages 44-46 from the 3rd edition of the book. This is a more economical way of testing the Urn of Mystery example than having a warehouse full of thousands of urns filled with green and red marbles. Consider that there are 1000 urns in the simulated warehouse, each with 0% to 100% green marbles. The percentage of green marbles are generated separately for each urn using a uniform distribution (the maximum possible uncertainty in this case). A marble is drawn at random from each urn. The probability of drawing a green marble is, obviously, just the percentage of marbles that are green. So, for each urn, the color of the randomly drawn marble is determined with a binary distribution using the percentage of green marbles as the chance of drawing green. Otherwise, red is drawn. In this simulation, we pretend the drawing person does not see the real percentage of green marbles in the urn. The person only uses the drawn marble to determine whether to bet the majority is green or red. We then determine whether that single draw turned out to be the majority color. We can see that after 1000 urns the single draw is the same color as the majority about 75% of the time.\n\n\n\n\nThe above (folded) narrative is rather confusing, and the spreadsheet is rather bulky. We can explain it and simulate it much more simply by using code. The point is… [what was the point again?]\n\nRandomly draw a ‘share green’ for each urn, for each of 1000 (or ‘\\(K=1000\\)’) ‘urns’, and record the majority color, i.e., is it more than half green?\n\n\nWe set the value ‘K=1000’, which we could adjust later, indicating ‘how many urns’ we are using in our simulation. It makes it clearer to define things at the top and see how it drives the results.\n\nCodeK <- 1000\n\nurns <- runif(K, 0, 1)\n\n\nThe code above yields the object urns, a vector of 1000 probabilities. It assigns ‘urns’ to be equal to the function runif, i.e., ‘random uniform’.\n2\n3 We could view the whole thing in several ways, such as by typing view(urns) or View(urns) for a peek. We can also have any part of this printed to the screen. The point is that this object is there in the background (in our ‘environment’). We don’t need to see it in front of us, at all times, as with a spreadsheet.\nPrinting out a peek at this object:\n\nCodestr(urns)\n\n num [1:1000] 0.08596 0.70347 0.40853 0.00597 0.84137 ...\n\n\nThe object is a ‘numeric vector’ with 1000 elements, the first 5 or so are listed. Each of these represent\n\nFor each of (\\(K= 1000\\)) urns, randomly draw a single marble, and record whether this draw is the same as the urn’s majority color.4\n\n\nWe put this together into a ‘tibble’ data frame below5\n\nCodelibrary(tidyverse)\n\nK <- 1000\n\nurns <- tibble(\n    share_green = runif(K, 0, 1), #recreating the vector of uniform draws as the first column of a tibble\n    majority = if_else(share_green > .5, \"green\", \"red\"), #classifying whether the tibble is majority green or red\n    draw = if_else(runif(K) > share_green, \"red\", \"green\") #if another random uniform draw exceeds the 'share green' in that urn, it's a draw of a red ball. Note this is doing this for the entire vector runif(K) atonce\n  ) \n\n\n\nDisplay this ‘matrix’ of 1000 outcomes (or a peek at it)\n\n\nCodeurns\n\n# A tibble: 1,000 × 3\n   share_green majority draw \n         <dbl> <chr>    <chr>\n 1       0.687 green    red  \n 2       0.517 green    green\n 3       0.456 red      red  \n 4       0.350 red      green\n 5       0.280 red      red  \n 6       0.448 red      green\n 7       0.651 green    red  \n 8       0.947 green    green\n 9       0.553 green    red  \n10       0.497 red      red  \n# … with 990 more rows\n\n\nThe snip above shows how these ‘random draws’ are generated, as noted above.\n\n\nCount ‘which share of these agree with their urn’s majority color’\n\n\nCode(\n  share_agree <- summarize(urns, share_agree = sum(draw == majority) / K)\n  )\n\n# A tibble: 1 × 1\n  share_agree\n        <dbl>\n1       0.727\n\nCode#sum number of cases where the 'draw is the same color as the majority for the urn\n\n\nI.e., 72.7% of the draws are the same color as the majority of their urn.6\nThe above code is not as elegant as it should be, and we should clean it up. Still I think this is better than using the Excel spreadsheet. Why? It gives you more control, a better record of what you have done, the ability to do more powerful analysis, and you can do more with the results (such as embedding them into a dynamic document like this one).\nFor one example, suppose you wanted to test this with 1 million urns. That would be a huge pain to do in Excel… I dare you to try it. In R we simply change the code to specify \\(K =1,000,000\\) urns, and do the above again.7\n\nCodeK <- 1000*1000\n\n\nurns <- tibble(\n    share_green = runif(K, 0, 1),\n    majority = if_else(share_green > .5, \"green\", \"red\"), \n    draw = if_else(runif(K) > share_green, \"red\", \"green\") \n  ) \n\n(\n  share_agree <- sum(urns$draw == urns$majority)/K\n  )\n\n[1] 0.749833\n\n\nOn my computer, this ran almost immediately.\n\n17.1.1 Tools for Monte-Carlo Fermi, other than Excel\nSome links to simple vignettes in other tools\n\nCausal, buy vs rent\nGuesstimate: simplest trivial example\nGuesstimate, slightly more involved: deep work before death of your mind\nSquiggle notebook, GiveDirectly/Givewell mode\nR code (working on it)"
  },
  {
    "objectID": "chapters/fermi.html#monte-carlo-fermi-approaches-to-givewell-style-cost-effectiveness-analysis",
    "href": "chapters/fermi.html#monte-carlo-fermi-approaches-to-givewell-style-cost-effectiveness-analysis",
    "title": "\n17  MonteCarlo ‘Fermi est.’\n",
    "section": "\n17.2 Monte-Carlo Fermi approaches to GiveWell-style Cost-Effectiveness Analysis",
    "text": "17.2 Monte-Carlo Fermi approaches to GiveWell-style Cost-Effectiveness Analysis\nEmbedded below, David Reinstein and Sam Nolan explain this approach, advocating its use in GiveWell models and beyond, laying out some building blocks,\n… and embed some tools and work-in-progress on this HERE, also embedded below.\nOverview\n\nThe basic ideas\nCausal and Guesstimate\nCode-based tools\n\n\nCodeknitr::include_url(\"https://effective-giving-marketing.gitbook.io/innovations-in-givewell-esque-ceas/\")\n\n\n\n\nDR: We may want to look for ways to explicitly incorporate and integrate these approaches into our data analysis work in R, etc."
  },
  {
    "objectID": "from_ea_market_testing/binary_trial_computations_redacted_ed.html",
    "href": "from_ea_market_testing/binary_trial_computations_redacted_ed.html",
    "title": "18  Inference/equivalence tests, (binomial)",
    "section": "",
    "text": "This work has been moved from the EA market testing repo, with ‘identifying’ content redacted\nThe context for this trial is discussed here (private) and will be publicly shared here when we gain permission."
  },
  {
    "objectID": "from_ea_market_testing/binary_trial_computations_redacted_ed.html#input-from-gsheet-wip",
    "href": "from_ea_market_testing/binary_trial_computations_redacted_ed.html#input-from-gsheet-wip",
    "title": "18  Inference/equivalence tests, (binomial)",
    "section": "\n18.1 Input from Gsheet (WIP)",
    "text": "18.1 Input from Gsheet (WIP)\n\nCode#library(googlesheets4)\n#read_sheet(\"XXX\")\n#Need authorization here; how to do it? Note -- we need those cells to be anonymized \n\n\nNote: subject lines were different … the impact email got more opens\n‘Giving Season contributions - website’ for donations … because CtA is a donation button on the website"
  },
  {
    "objectID": "from_ea_market_testing/binary_trial_computations_redacted_ed.html#randomization-inference-approach",
    "href": "from_ea_market_testing/binary_trial_computations_redacted_ed.html#randomization-inference-approach",
    "title": "18  Inference/equivalence tests, (binomial)",
    "section": "\n18.2 Randomization inference approach",
    "text": "18.2 Randomization inference approach"
  },
  {
    "objectID": "from_ea_market_testing/binary_trial_computations_redacted_ed.html#how-likely-is-the-imbalance-of-unique-emails-due-to-chance",
    "href": "from_ea_market_testing/binary_trial_computations_redacted_ed.html#how-likely-is-the-imbalance-of-unique-emails-due-to-chance",
    "title": "18  Inference/equivalence tests, (binomial)",
    "section": "\n18.3 How likely is the ‘imbalance of unique emails’ due to chance?",
    "text": "18.3 How likely is the ‘imbalance of unique emails’ due to chance?\nWe’ve 1345 unique emails in the dataset for ‘control’ treatment 1 and 1190 unique emails for treatment 2, if I did the quick calculations correctly. (Todo– integrate the data here.)\nWe know some emails are repeated. To analyze this correctly I should bring in the actual distribution of ‘number of times an email is in the data’ and simulate assignment. (Or maybe this is a ‘Poisson’ thing?)\nAs a (perhaps incorrect) first pass I could consider ‘legitimate unique email shows up in the treatment group’ as a random binomial event. (I guess the event is something like ’email is (not) duplicated?) Then we can do the standard binomial (Chi-sq) and Fisher’s exact tests.\n\nCoden1 <- 1345\nn2 <- 1190\n\n(\n  binom_emails <- prop.test(n1, n1+n2, correct=FALSE)\n)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  n1 out of n1 + n2, null probability 0.5\nX-squared = 9.4773, df = 1, p-value = 0.00208\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5111129 0.5499385\nsample estimates:\n       p \n0.530572 \n\nCode(\n  fisher_emails <- fisher.test(matrix(c(n1, n2, n2, n1), ncol=2))\n)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  matrix(c(n1, n2, n2, n1), ncol = 2)\np-value = 1.512e-05\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 1.142247 1.428639\nsample estimates:\nodds ratio \n  1.277394 \n\n\nIf the above tests are reasonable, this imbalance is very unlikely to occur by chance.\nNote: if we think this is opens, this is a demonstration that one email is opened more."
  },
  {
    "objectID": "from_ea_market_testing/binary_trial_computations_redacted_ed.html#how-likely-are-proportions-this-similar-under-different-size-true-effect-sizes-i-think-this-is-relates-to-a-power-calculation-but-reporting-1-power.",
    "href": "from_ea_market_testing/binary_trial_computations_redacted_ed.html#how-likely-are-proportions-this-similar-under-different-size-true-effect-sizes-i-think-this-is-relates-to-a-power-calculation-but-reporting-1-power.",
    "title": "18  Inference/equivalence tests, (binomial)",
    "section": "\n18.4 How likely are ‘proportions this similar’ under different size ‘true effect sizes’?^[ I think this is relates to a ‘power calculation’, but reporting \\(1-power\\).",
    "text": "18.4 How likely are ‘proportions this similar’ under different size ‘true effect sizes’?^[ I think this is relates to a ‘power calculation’, but reporting \\(1-power\\).\n]\nIn the next section I started an off-the-cuff simulation approach, in folded code. But I believe that the analytical form here with code and a simulation here should be applicable.\n\n18.4.1 Difference between two binomial random variables.\nReprinting and discussing from Stackexchange post\n“derive the distribution of the difference between two binomial random variables.”\n\nI can give you an answer for the pmf of X-Y. From there |X - Y| is straightforward.\n\n\nSo we start with\n\\(X \\sim Bin(n_1, p_1)\\)\n\\(Y \\sim Bin(n_2, p_2)\\)\nWe are looking for the probability mass function of \\(Z=X-Y\\)\n\nNote: in our case we (arguably) care about the difference in ‘proportion of incidences’, but as we also have similar sample sizes (\\(n_1 \\approx n_2\\)) this is basically a normalization\n\nFirst note that the min and max of the support of Z must be \\((-n_2, n_1)\\) since that covers the most extreme cases (\\(X=0\\) and \\(Y=n_2\\)) and (\\(X=n_1\\) and \\(Y=0\\)).\nThen we need a modification of the binomial pmf so that it can cope with values outside of its support.\n\\(m(k, n, p) = \\binom {n} {k} p^k (1-p)^{n-k}\\) when \\(k \\leq n\\) and 0 otherwise.\n\nNote that this “modified binomial” is what the statistics::dbinom function in R returns.\n\nThen we need to define two cases\n\n\\(Z \\geq 0\\)\n\\(Z \\lt 0\\)\n\nIn the first case\n\\(p(z) = \\sum_{i=0}^{n_1} m(i+z, n_1, p_1) m(i, n_2, p_2)\\)\nsince this covers all the ways in which X-Y could equal z.\n\nBecause \\((i+z) - i = z\\), of course … ‘sum up’ the probability of each of these ‘co-occurances’\n\nFor example when z=1 this is reached when X=1 and Y=0 and X=2 and Y=1 and X=3 and Y=4 and so on. It also deals with cases that could not happen because of the values of \\(n_1\\) and \\(n_2\\). For example if \\(n_2 = 4\\) then we cannot get Z=1 as a combination of X=4 and Y=5. In this case thanks to our modified binomial pmf the probablity is zero.\nFor the second case we just reverse the roles. For example if z=-1 then this is reached when X=0 and Y=1, X=1 and Y=2 etc.\n\\(p(z) = \\sum_{i=0}^{n_2} m(i, n_1, p_1) m(i+z, n_2, p_2)\\)\nPut them together and that’s your pmf.\n\n\\(f(z)=\\)\n\\[\\begin{cases}\n    \\sum_{i=0}^{n_1} m(i+z, n_1, p_1) m(i, n_2, p_2),& \\text{if } z\\geq 0\\\\\n    \\sum_{i=0}^{n_2} m(i, n_1, p_1) m(i+z, n_2, p_2),              & \\text{otherwise}\n\\end{cases}\\]\n\nHere’s the function in R and a simulation to check it’s right (and it does work.) https://gist.github.com/ragscripts/9681819\n\nLet me try to apply it…\nDefining their code for this function:\n\nCodemodBin <-dbinom #DR: I just do this renaming here for consistency with the rest ... but the modBin they defined was redundant\n\ndiffBin<-function(z, n1, p1, n2, p2){\n\n  prob <- 0\n\n  if (z>=0){\n    for (i in 1:n1){\n      prob <- prob + modBin(i+z, n1, p1) * modBin(i, n2, p2)\n    }\n\n  }\n  else\n  {\n    for (i in 1:n2){\n      prob<-prob+modBin(i+z, n1, p1)*modBin(i, n2, p2)\n    }\n  }\n  return(prob)\n}\n\n\n\n18.4.2 Applying this to present data\nRather than using their example, I’ll dive right in to the present case.\nMy notes on the outcomes as of 15 Dec 2021\n\n\n\n\n\n\nNote\n\n\n\n\n\nTreatment 1 - Impact and Treatment 2 - Emotion Story tabs\nTreatment 1: We record - 8 unique emails donating, 26 donations in total, - worth 5200 USD in total - 1345 unique emails listed as getting ‘control’ treatment 1\nTreatment 2: - 6 unique emails, 28 donations so far — worth 7500 USD in total. - 1190 unique emails listed for treatment 2\nIf I believe my ‘unique emails count’, that implies an 0.59% ‘conversion’ rate for T1 - Control a 0.50% conversion rate for T2 - Emotion/Story\n\n\n\nPutting the observations into defined objects (later: do from data)\n\nCoden1 <- 1345\nn2 <- 1190\nd1 <- 8\nd2 <- 6\nz <- d1-d2\n\n\nComputation for a few ‘ad-hoc cases’ (later explore the space with vectors of values)\n\nSuppose truly equal incidence, at the mean level\n\n\nCodep1 <- (d1+d2)/(n1+n2)\n\np2 <- p1\n\n(\n  db_0 <- diffBin(z, n1, p1, n2, p2)\n)\n\n[1] 0.1024599\n\n\nThis implies there is a 10.2% chance of getting this exact difference of +2 incidences between the treatments (in one direction), if the true incidence rates were equal.\nLet’s plot this for a range of ‘incidence rate differences’ in this region. (Sorry, using the traditional plot, ggplot is better).\n\nCodes <- seq(-10*z, 10*z)\np<-sapply(s, function(z) diffBin(z, n1, p1, n2, p2))\nplot(s,p)\n\n\n\n\nWe see a large likelihood of values in the range of the +2 difference observed, and a low likelihood of a difference of 10 or more in either direction.\n\n18.4.3 Adaptation: ‘of this magnitude or smaller’\n\nCodeltmag_diffBin <- function(z, n1, p1, n2, p2){\n  prob <- 0\n  z_n <- -z #negative value\n\n  for (i in z_n:z){     #sum for all integer differences between observed value and its negative, inclusive\n    prob <- prob + diffBin(i, n1, p1, n2, p2)\n    }\n\n  return(prob)\n}\n\n\nNow, a similar computation as above, but for ‘this big or smaller in magnitude’:\n\nCode  (\n    mag_db_0 <- ltmag_diffBin(z, n1, p1, n2, p2)\n  )\n\n[1] 0.4908031\n\n\nThis implies there is a 49.1% chance of getting a difference no larger than this one in magnitude of +/-2 incidences between the treatments if the true incidence rates were equal.\n\nAnd finally, what we were looking for: the chance of ‘a difference this small or smaller’ as a function of the true difference…\nSet up an arbitrary vector of ‘true differences’ (to keep it simple, only change it in one direction for now …)\nBelow, I plot\nY-axis: ’how likely would a difference in donations ‘as small or smaller in magnitude’” than we see in the data against\nX-axis: if the “true difference in incidence rates” were of these magnitudes\n\nCodeoptions(scipen=999)\n\nB <- c(1, 1.5, 2, 2.5, 3, 4, 10)\n\np1 <- rep((d1+d2)/(n1+n2), length(B))\np2 <- p1*B\n\n\nas.list(ltmag_diffBin(z, n1, p1, n2, p2)*100) %>% format(digits=3, scientific=FALSE)\n\n[1] \"49.1\"             \"39.4\"             \"20.6\"             \"8.02\"            \n[5] \"2.51\"             \"0.153\"            \"0.00000000000855\"\n\nCodeprobmag <- ltmag_diffBin(z, n1, p1, n2, p2)\n\n\n#qplot(B, probmag, log  = \"x\", xlab = \"True relative incidence\", ylab =\"Prob. of difference this small\")\n\n(\n  probmag_plot <-\n    ggplot() +\n  aes(x=B, y=probmag) +\n  geom_point() +\n  scale_x_continuous(trans='log2') +\n    ylim(0,.51) +\n    xlab(\"True relative incidence rate\") +\n    ylab(\"Prob. diff. as small as obsd\")\n\n)\n\n\n\n\nHard-coded takeaways 15 Dec 2021 :\nOur data is consistent with ‘no difference’ (of course) … but its also consistent with ‘a fairly large difference in incidence’\nE.g., even if one treatment truly lead to ‘twice as many donations as the other’, we still have a 20% chance of seeing a differences as small as the one we see (of 8 versus 6)\nWe can reasonably ‘rule out’ differences of maybe 2.5x or greater\nMain point: given the rareness of donations in this context, our sample size doesn’t let us make very strong conclusions in either directions … at least not yet. I hope that combined with other evidence, we will be able to infer more"
  },
  {
    "objectID": "chapters/other_sections.html",
    "href": "chapters/other_sections.html",
    "title": "\n19  Other suggested sections\n",
    "section": "",
    "text": "Codelibrary(\"here\")\nsource(here(\"code\", \"methods_setup.R\"))\n\nInstalling remotes [2.4.2] ...\n    OK [linked cache]\nInstalling downloader [0.4] ...\n    OK [linked cache]"
  },
  {
    "objectID": "chapters/other_sections.html#conjoint-analysis",
    "href": "chapters/other_sections.html#conjoint-analysis",
    "title": "\n19  Other suggested sections\n",
    "section": "\n19.1 ‘Conjoint analysis’",
    "text": "19.1 ‘Conjoint analysis’\nSee end of this Slack thread"
  },
  {
    "objectID": "chapters/other_sections.html#opensci",
    "href": "chapters/other_sections.html#opensci",
    "title": "\n19  Other suggested sections\n",
    "section": "\n19.2 (Open and robust science: RP attitudes, discussions, resources)",
    "text": "19.2 (Open and robust science: RP attitudes, discussions, resources)\nIntegrate from:\nReinstein discussions here\nCode to do Robustness checks as a ‘specification chart’, and sensitivity analysis\nSource: https://pbs.twimg.com/media/ESyDHGjUYAYHfva?format=jpg&name=medium"
  },
  {
    "objectID": "chapters/other_sections.html#meta",
    "href": "chapters/other_sections.html#meta",
    "title": "\n19  Other suggested sections\n",
    "section": "\n19.3 (Meta-analysis)",
    "text": "19.3 (Meta-analysis)\nIncorporate and consolidate from Reinsteins meta notes and more"
  },
  {
    "objectID": "chapters/other_sections.html#related-aggregating-expert-judgment",
    "href": "chapters/other_sections.html#related-aggregating-expert-judgment",
    "title": "\n19  Other suggested sections\n",
    "section": "Related: ‘aggregating expert judgment’",
    "text": "Related: ‘aggregating expert judgment’\nTesting out the AggreCAT package on the ACX prediction contest\n\nInstall aggreCATinstall.packages(\"devtools\")\n\nInstalling devtools [2.4.5] ...\n    OK [linked cache]\n\nInstall aggreCAT#install.packages(\"sessioninfo\", dependencies = TRUE)\n\ndevtools::install_github(\"metamelb-repliCATS/aggreCAT\")\n#Note: this failed initially because dependent packages needed updating.  `update.packages()` nay solve it. Nope, it didn't but all sorts of errors in that update\n\nlibrary(aggreCAT)\n\n\n\ninput_acx_prediction_dataacx <- read_csv(here(\"sample_data\", \"acx_2023blindmode_predictions.csv\"))\n#Improve: source from a hosted file or API\n\n\n\nidentify my own predictionsacx  %<>%  mutate(\n  d_reinstein =  ifelse(\n    `@1.WillVladimirPutinbePresidentofRussia`==82 & `@2.WillUkrainecontrolthecityofSevastopol`==54, TRUE, FALSE)\n)\n\n\nWhat can aggreCAT do for us?\nFirst testing this package with their built-in data.\n\nBelow we demonstrate how to use the most simple commonly implemented aggregation method ArMean, which takes the arithmetic mean of participant Best Estimates. We first use a small subset of 5 participants for a single claim, 28\n\n\nCodedata(data_ratings)\nset.seed(1234)\n\nparticipant_subset <- data_ratings %>% #just 5 users with names\n  distinct(user_name) %>%\n  sample_n(5) %>%\n  mutate(participant_name = paste(\"participant\", rep(1:n()))) #numbers rather than id codenames\n\nsingle_claim <- data_ratings %>% \n  filter(paper_id == \"28\") %>% #all data on a single claim\n  right_join(participant_subset, by = \"user_name\") #join to the above 5 users (keeping only these 5)\n\n#note this stat only uses the rows with element == `three_point_best`\nsingle_claim %>% \n  filter(element == \"three_point_best\") %>% \nAverageWAgg(expert_judgements = ., \n            type = \"ArMean\")\n\n\n\n\n\n\n\n\nmethod\npaper_id\ncs\nn_experts\n\n\nArMean\n28\n70.8\n5\n\n\n\n\nCan we do similar across multiple claims with this package?\n\nCodeall_claims <- data_ratings %>% \n  right_join(participant_subset, by = \"user_name\")   #join to the above 5 users (keeping only these 5) \n\nall_claims %>% \n filter(element == \"three_point_best\") %>% \nAverageWAgg(expert_judgements = ., \n            type = \"ArMean\")\n\n\n\n\n\n\n\n\nmethod\npaper_id\ncs\nn_experts\n\n\nArMean\n100\n75  \n5\n\n\nArMean\n102\n34  \n5\n\n\nArMean\n103\n59.4\n5\n\n\nArMean\n104\n53.4\n5\n\n\nArMean\n106\n45  \n5\n\n\nArMean\n108\n74  \n5\n\n\nArMean\n109\n68  \n5\n\n\nArMean\n116\n61  \n5\n\n\nArMean\n118\n60.8\n5\n\n\nArMean\n133\n58.6\n5\n\n\nArMean\n137\n61  \n5\n\n\nArMean\n138\n68.6\n5\n\n\nArMean\n145\n67  \n5\n\n\nArMean\n168\n35  \n5\n\n\nArMean\n176\n46  \n5\n\n\nArMean\n186\n57.6\n5\n\n\nArMean\n20\n70  \n5\n\n\nArMean\n203\n41  \n5\n\n\nArMean\n21\n60  \n5\n\n\nArMean\n215\n45  \n5\n\n\nArMean\n24\n22.2\n5\n\n\nArMean\n26\n63  \n5\n\n\nArMean\n28\n70.8\n5\n\n\nArMean\n38\n33.4\n5\n\n\nArMean\n79\n31  \n5\n\n\n\n\nThis seems to work, at least with the data structured as they have it. Now what if we want multiple aggregation methods?\n\nCodethree_aggregations <- purrr::map_dfr(.x = list(AverageWAgg, IntervalWAgg, ShiftingWAgg),\n                                .f = ~ .x(data_ratings))\n\nthree_aggregations %>%  tabyl(method)\n\n\n\n\n\n\n\nmethod\nn\npercent\n\n\nArMean\n25\n0.333\n\n\nIntWAgg\n25\n0.333\n\n\nShiftWAgg\n25\n0.333\n\n\n\n\nNow let’s see what can be done with our ACX dataset. As ACX doesn’t ask for CIs (and doesn’t have multiple rounds of expert revisions), aggreCAT may be of limited use here.^[I’d have to estimate these bounds arbitrarily (e.g., make them larger for people with less experience). Only a subset of their measures may be informative. First I’ll check which ones might work, using their data for all aggregations (with baseline settings for each), but only keeping this ‘best prediction’ for each.\n\nCodeall_aggsOK <- list(AverageWAgg,  IntervalWAgg, ShiftingWAgg, ExtremisationWAgg)\n\nall_aggs <- list(LinearWAgg, AverageWAgg, BayesianWAgg, IntervalWAgg, ShiftingWAgg, ReasoningWAgg, DistributionWAgg, ExtremisationWAgg)\n\n\nthree_aggs <- list(LinearWAgg, AverageWAgg, BayesianWAgg)\n\n\ndata_ratings %>% \n filter(element == \"three_point_best\") %>% \n  purrr::map_dfr(.x = all_aggsOK,\n                                .f = ~ .x(data_ratings)) %>%\n  tabsums(cs, method)\n\n\n\n\n\n\n\n\nmethod\nnonmissing\nmean\nsd\n\n\nArMean\n25\n51.7\n17.3\n\n\nBetaArMean\n25\n1  \n0  \n\n\nIntWAgg\n25\n48.6\n23.4\n\n\nShiftWAgg\n25\n50.2\n17.1\n\n\n\n\nAll of the aggregators seemed computable even without having elements other than ‘three_point_best’ What’s going on here? E.g,. IntervalWAgg says ‘the weights are dependent on the lower and upper bounds of three-point elicitation’ … but I left those out. ‘ShiftingWAgg’ is meant to depend on discussion; but here I have no indicators of that. Yet still, these all seem to come up with different means. BetaARMean which is meant to do some extremisation wotks rather strangely; shifting everything to 1, apparently.\nFocusing in on some key interesting methods… I’ve been told ‘Geometric Means’ are good in these sorts of situations\n\nCodedata_ratings %>% \n filter(element == \"three_point_best\") %>% \nAverageWAgg(expert_judgements = ., \n            type = \"GeoMean\")\n\n\n\n\n\n\n\n\nmethod\npaper_id\ncs\nn_experts\n\n\nGeoMean\n100\n67.3\n25\n\n\nGeoMean\n102\n26.4\n25\n\n\nGeoMean\n103\n60.2\n25\n\n\nGeoMean\n104\n40.4\n25\n\n\nGeoMean\n106\n28.8\n25\n\n\nGeoMean\n108\n71.2\n25\n\n\nGeoMean\n109\n71.3\n25\n\n\nGeoMean\n116\n60  \n25\n\n\nGeoMean\n118\n52  \n25\n\n\nGeoMean\n133\n56.2\n25\n\n\nGeoMean\n137\n60.5\n25\n\n\nGeoMean\n138\n74.8\n25\n\n\nGeoMean\n145\n61.3\n25\n\n\nGeoMean\n168\n23.7\n25\n\n\nGeoMean\n176\n21.7\n25\n\n\nGeoMean\n186\n48.3\n25\n\n\nGeoMean\n20\n67.9\n25\n\n\nGeoMean\n203\n42.7\n25\n\n\nGeoMean\n21\n52.1\n25\n\n\nGeoMean\n215\n35.8\n25\n\n\nGeoMean\n24\n15.7\n25\n\n\nGeoMean\n26\n62.5\n25\n\n\nGeoMean\n28\n61.2\n25\n\n\nGeoMean\n38\n23.7\n25\n\n\nGeoMean\n79\n23.4\n25\n\n\n\n\nLinearWAgg seems interesting, either suppliying some sort of weights by Participant (e.g., downweight those who haven’t done forecasting), or with the supplied GranWAgg – “granularity of best estimates”\nLet’s move to trying to use these on the ACX data. First I adjust the data to make it more compatible with the aggreCAT package.\n\n\n\n\n\n\nNote – it cannot deal with NAs:\n\n\n\n\n\n\nCodedata_ratings2 <- data_ratings\ndata_ratings2[4,6] <- NA_real_\n\ndata_ratings2 %>% \nAverageWAgg(expert_judgements = ., \n            type = \"GeoMean\")\n\nError in `preprocess_judgements()`:\n! NAs Found in Values\n\n\n\n\n\nUpweighting and downweighting people – ad-hoc.\nNow, I have little time left, so I’ll come up with some ad-hoc weights.\n\n\n\n\n\n\nHow to use what we know/what ACX suggested in aggregation\n\n\n\n\n\nWe have some idea of how different characteristics predicted success in previous prediction contests. Suppose I had a model that was good at predict in the accuracy of an individual’s prediction, as a function of their demography, their other prediction, etc. How would I then use that (along with everyone’s predictions), to come up with a good prediction aggregation?\nE.g., how should we map those into the ‘weights’ suggested? I’m not sure, and it would take me some time to figure this out.\n\n\n\nI guess I will do something lame and ad-hoc. Everyone starts out with weight 1.\n\nAnyone who has no forecasting experience is downweighted fivefold (x0.2),\nSuperforecasters are upweighted 20x\nSAT math uprated if above average (unless already a superforecaster)\nPHd upweighted 1.5x\nDownweight x 0.1 if you put 98+ or lt 4% chance of Putin being in power\nDownweight by quantile of share of ‘exact 50% predictions’ (up to about 90% downweighting)\n\nAlso, replace all ‘50% predictions’ with the arithmetic average (done later)\n\nCode# get column names starting with \"@\"\npred_cols <- colnames(acx)[startsWith(colnames(acx), \"@\")]\nacx_for_ag_wts <- acx \nacx_for_ag_wts$share50 <- (rowSums(acx[pred_cols]==50, na.rm=TRUE))/50\n# subset data based on column names and calculate row sums\n\nacx_for_ag_wts %<>%\n   mutate(\n    user_name = row_number()\n    ) %>% \n mutate(\n   SATscoremath =  case_when(\n     (SATscoremath>800|SATscoremath<100) ~ NA_real_, \n     TRUE ~ SATscoremath),\n   SAT_math_rank = percent_rank(SATscoremath),\n    SAT_math_rank=\n     case_when(\n    is.na(SAT_math_rank) ~ mean(0.5, na.rm=TRUE),\n    TRUE ~ SAT_math_rank),\n   share50_qtl = percent_rank(share50) \n    ) %>% \n      mutate(across(c(ForecastingExperience, Superforecaster, Degree), ~replace(., is.na(.), \"No\"))) %>% \nmutate(\n  weight = \n    (ifelse(ForecastingExperience==\"No\",0.2,1))*\n    (ifelse(Superforecaster==\"Yes\",20,1))*\n    (1+((SAT_math_rank>0.5)*(Superforecaster!=\"Yes\")*(SAT_math_rank-0.5))*4)*\n    (1+(Degree==\"Ph D.\")*.5)*\n    (ifelse(`@1.WillVladimirPutinbePresidentofRussia`>=99,0.1,1))*\n    (ifelse(`@1.WillVladimirPutinbePresidentofRussia`< 5, 0.1,1))*\n  (1-(share50_qtl)/1.2) \n)\n\n#the code above is kind of crappy; better to convert it to logs to allow simple addition?\n\nacx_for_ag_wts_only <- acx_for_ag_wts %>% \n  select(user_name, weight)\n\n\n\nCodeacx_for_ag_wts_long <- acx_for_ag_wts %>%  \n   select(where(~n_distinct(.) > 1)) %>%  #delete columns that are identical everywhere\n  pivot_longer( #each 'question' needs it's own row for each 'user'\n    contains(\"@\"),\n    names_to = \"paper_id\",\n    values_to = \"value\"\n    ) \n  \nacx_for_ag_wts_long %<>%  select(user_name, paper_id, value, everything()) %>%  #make all predictions (for an individual) into a row \n  mutate(element = \"three_point_best\",\n    round = \"round_2\") %>% #aggreCAT needs these things \n  group_by(paper_id) %>% \n  mutate(\n    value = \n    case_when(\n      is.na(value) ~ mean(value, na.rm=TRUE), #Sadly the package can't deal with NAs so I cheaply set them to the arithmetic average\n      value == 0.5 ~ mean(value, na.rm=TRUE),\n      TRUE ~ value\n  )\n  ) %>% ungroup  \n\n\nLet’s see if it works\n\nCodeacx_for_ag_wts_long %>%  \n  ungroup() %>% \n filter(paper_id==\"@1.WillVladimirPutinbePresidentofRussia\") %>%\n  filter(user_name<=154)  %>% \n  AverageWAgg(expert_judgements = ., \n            type = \"GeoMean\")\n\n\n\n\n\n\n\n\nmethod\npaper_id\ncs\nn_experts\n\n\nGeoMean\n@1.WillVladimirPutinbePresidentofRussia\n99\n154\n\n\n\nCodeacx_for_ag_wts_long %>%  \n  ungroup() %>% \nfilter(paper_id==\"@1.WillVladimirPutinbePresidentofRussia\") %>%\n  filter(user_name<=155)  %>% \n  AverageWAgg(expert_judgements = ., \n            type = \"GeoMean\")\n\n\n\n\n\n\n\n\nmethod\npaper_id\ncs\nn_experts\n\n\nGeoMean\n@1.WillVladimirPutinbePresidentofRussia\nInf\n155\n\n\n\n\nStrangely, this seems to only work here with 155 or less rows!\nNow to do some funny calculations and blend them with my own predictions\nacx_for_ag_wts_results: The weighted arithmetic means\n\nCodeacx_for_ag_wts_results <- acx_for_ag_wts_long %>%\n LinearWAgg(expert_judgements = ., \n            type = \"Participant\", \n   weights = acx_for_ag_wts_only\n    ) %>%  \n    mutate(agg = \"linear_wt\", \n      value = cs)\n\n\nNow (weighted?) geometric means\n\nCodeweighted.geomean <- function(x, w, ...)\n{\n  return(prod(x^w, ...)^(1/sum(w, na.rm = TRUE)))\n}\n\nweighted.geomean <- function(x, w, ...) exp(weighted.mean(log(x), w, ...))\n\n\nsumwt <- sum(acx_for_ag_wts_long$weight[acx_for_ag_wts_long$weight>1], na.rm=TRUE)\n\nreg.geomean <- function(x, ...)\n{\n  return(prod(x, ...))\n}\n\nacx_for_ag_geom_wt <- acx_for_ag_wts_long %>%\n  filter(!is.na(weight)  & !is.na(value))  %>%\n  group_by(paper_id) %>% \n  mutate(\n    sumwt = sum(weight, na.rm=TRUE)\n   ) %>% \n    summarize(\n      weighted_geomean = weighted.geomean(value, weight),\n    ) %>% \n  mutate(agg = \"geom_wt\",\n    value = weighted_geomean)\n\n\nOK, looks like I forgot about the prediction market data in metaculus HERE on the same thing.\nI’ll make a tibble of the weighted linear mean, weighted geometric mean, metaculus data, my own predictions)\n\nCode#p_load(DataEditR)\n#metaculus_data <- acx_for_ag_wts_results %>% \n # select(paper_id)  %>% \n  #mutate(value=50)\n  \n\n#data_edit(metaculus_data)\n\n\nmetaculus_data <- read_csv(here(\"sample_data\", \"metaculus.csv\")) %>% \n  mutate(agg = \"metaculus\")\n\n\n#and me\n\nreinstein_pred <- acx %>%  \n  filter(d_reinstein) %>% \n  select(starts_with(\"@\")) %>% \n  pivot_longer(col=everything()) %>% \n  rename(paper_id = name) %>% \n  mutate(agg = 'reinstein_pred')\n\n\naggregates <- bind_rows(reinstein_pred, metaculus_data, acx_for_ag_geom_wt, acx_for_ag_wts_results) %>%  \n  select(-method, -cs, -n_experts, -weighted_geomean) %>%  \n  pivot_wider(names_from=agg, id_cols = paper_id, values_from=value) %>% \n  mutate(guided_weight = 0.2*reinstein_pred + 0.4*metaculus + 0.2*linear_wt +  0.2*geom_wt,\n    final_impulsive_choice = guided_weight)\n\nwrite_csv(aggregates, here(\"sample_data\", \"aggregates.csv\"))\n\n\n\n#data_edit(aggregates)\n\n\nFixed some stuff at end and cleaned it and made some ad-hoc extremizing around a few correlated events\n\nCodefinal_ad_hoc <- read_csv(here(\"sample_data\", \"aggregates.csv\")) \n\nfinal_ad_hoc %>%  DT::datatable()\n\n\n\n\n\nCode#final_ad_hoc %<>%  mutate(dif_meta = reinstein_pred/metaculus)"
  },
  {
    "objectID": "chapters/rethinking_bayes_recoding.html",
    "href": "chapters/rethinking_bayes_recoding.html",
    "title": "20  Overview (McE/Bayes)",
    "section": "",
    "text": "Reinstein, others may contribute↩︎\nMcElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman & Hall/CRC Press.↩︎\nThis is not an actual fork, although I may move the content it over to my actual fork of that bookdown here … when I do, I’ll put a link back↩︎"
  },
  {
    "objectID": "chapters/1_golem.html",
    "href": "chapters/1_golem.html",
    "title": "21  Ch 1. Golem of Prague",
    "section": "",
    "text": "Modern econometrics mainly focuses on the (causal) identification and estimation of a single parameter or ‘treatment effect’.1 Economists often make choices to make the identification of this parameter more robust (e.g., identified under weaker assumptions) or more efficient — even if this tends to make the overall model worse at predicting the outcomes. E.g., they may leave out a possibly variable that is possibly ‘endogenous’ in a way that could bias the estimate of the parameter of interest, even if it is clear that in increases the predictive power of the model.\nEconometrics also focuses a lot on the unbiasedness of parameter estimates.2 One justification for focusing on unbiasedness might be in the context of a judge in an adversarial proceeding; even if it may be a bad predictor, we know this estimate does not understate or overstate an effect on average; thus it favors neither side of the debate.\nPerhaps for these reasons (focus on a single parameter, concern for unbiasedness), econometricians seem reluctant t use random effects/mixed models and ‘regularization’. Where they use these, I think it is often only for ‘incidental parameters’, not for the parameters of interest. I recall that the classic ‘Stein shrinkage estimator’ is a better predictor (in terms of mean squared error) but it is not unbiased. Similarly, if we ‘regularize’ an estimate of a ‘necessary control’3 the estimated parameter on our regressor of interest may be biased.\nMy potential concern: I’m not sure McElreath is always clear on how he sees as the goal of statistical analysis, and how it informs scientific knowledge. (Of course, at RP we have practical goals, not just the expansion of knowledge for its own sake.) When a model ‘fits the outside data better’ i.e., it ‘predicts better’, does it necessarily ‘represent the truth about the question of interest better’? Perhaps it is focusing too little on the relationship of greatest interest, in favor of finding a model that fits better overall.\nRelatedly, he seems to be somewhat interested in ‘falsification done right’. But it’s not clear to me how his take on a Bayesian framework does this. In particular, in some of the causal chapters, it seems like he is suggesting a DAG can be falsified by something that seems to me like an ‘absence of evidence of a relationship’, rather than, e.g., an equivalence test or a posterior tightly bounded around 0."
  },
  {
    "objectID": "chapters/1_golem.html#statistical-golems",
    "href": "chapters/1_golem.html#statistical-golems",
    "title": "21  Ch 1. Golem of Prague",
    "section": "\n21.2 Statistical golems",
    "text": "21.2 Statistical golems\n\n\n\n\n\n\n” Scientists also make golems… scientific models.”\n\n\n\n\n\n\nBut these golems have real effects on the world, through the predictions they make and the intuitions they challenge or inspire. A concern with truth enlivens these models, but just like a golem or a modern robot, scientific models are neither true nor false, neither prophets nor charlatans. Rather they are constructs engineered for some purpose. These constructs are incredibly powerful, dutifully conducting their programmed calculations.\n\n\n\n\n\nKurz: one of the most powerful themes interlaced throughout the pages is how we should be skeptical of our models. McElreath presentation?"
  },
  {
    "objectID": "chapters/1_golem.html#hostility-to-cookbooks-and-flowcharts",
    "href": "chapters/1_golem.html#hostility-to-cookbooks-and-flowcharts",
    "title": "21  Ch 1. Golem of Prague",
    "section": "\n21.3 Hostility to cookbooks and flowcharts",
    "text": "21.3 Hostility to cookbooks and flowcharts\n\nAdvanced courses in statistics do emphasize engineering principles, but most scientists never get that far. Teaching statistics this way is somewhat like teaching engineering backwards, starting with bridge building and ending with basic physics\n\n\nWhy aren’t the tests enough for research? The classical procedures of introductory statistics tend to be inflexible and fragile. By inflexible, I mean that they have very limited ways to adapt to unique research contexts. By fragile, I mean that they fail in unpredictable ways when applied to new contexts\n\n\nFisher’s exact test, which applies (exactly) to an extremely narrow empirical context, but is regularly used whenever cell counts are small\n\n\n… ethinking statistical inference as a set of strategies, instead of a set of pre-made tools."
  },
  {
    "objectID": "chapters/1_golem.html#null-hypothesis-significance-testing-and-falsification",
    "href": "chapters/1_golem.html#null-hypothesis-significance-testing-and-falsification",
    "title": "21  Ch 1. Golem of Prague",
    "section": "\n21.4 Null hypothesis significance testing and falsification",
    "text": "21.4 Null hypothesis significance testing and falsification\n\nParaphrasing from McElreath’s lecture video:\nScientists have turned things upside down; originally the idea was that you had substantive of hypotheses that you would want to falsify and now we try to falsify silly null hypotheses that “nothing is going on”. You should try to really build a hypothesis and test it not just reject that nothing is going on.\n\ngreatest obstacle that I encounter among students and colleagues is the tacit belief that the proper objective of statistical inference is to test null hypotheses\n\n\ndeductive falsification is impossible in nearly every scientific context\n\n\n\n\n\n\n\nIntricate argument for this, non-unique predictions of models\n\n\n\n\n\n\nMany models correspond to the same hypothesis, and many hypotheses correspond to a single model. This makes strict falsification impossible. (2) Measurement matters. Even when we think the data falsify a model, another observer will debate our methods and measures. They don-t trust the data. Sometimes they are right.\n\n\nAll models are false, so what does it mean to falsify a model? One consequence of the requirement to work with models is that it’s no longer possible to deduce that a hypothesis is false, just because we reject a model derived from it.\n\n\nAnd so this fact yields a statistical model, MII, that predicts a power law in the data. In contrast the constant selection process model P1A predicts something quite different, MIII. Unfortunately, other selection models (P1B) imply the same statistical model, MII, as the neutral model. They also produce power laws\n\n\nIf we reject the null, we can’t really conclude that selection matters, because there are other neutral models that predict different distributions of alleles. And if we fail to reject the null, we can’t really conclude that evolution is neutral, because some selection models expect the same frequency distribution\n\nWhat about ‘black swan’ falsification?:\n\nwe most often face two simultaneous problems that make the swan fable misrepresentative. First, observations are prone to error, especially at the boundaries of scientific knowledge. Second, most hypotheses are quantitative, concerning degrees of existence, rather than discrete, concerning total presence or absence\n\n\nH0 “Black swans are rare”\nNeutrinos case:\n\nthe “measurement” in this case is really an estimate from a statistical model, all false and true positives are possible..? usually measurement error is an issue. so black swan falsification is not so simple\n\n\n\n\n\n\n\n\n\n\nRethinking: Is NHST falsificationist?\n\n\n\n\n\nNull hypothesis significance testing, NHST, is often identified with the falsificationist, or Popperian, philosophy of science. However, usually NHST is used to falsify a null hypothesis, not the actual research hypothesis. So the falsification is being done to something other than the explanatory model.\n\n\n\n\n\n\n\n\n\nDR question: What would a ‘correct’ falsificationist/hypothesis testing approach involve?\n\n\n\n\n\nI’m not sure I’ve actually ever seen the NHST or other ‘falsificationist’ framework defended or fully expounded. What are some good references for this?\nE.g., if we fail to reject a hypothesis, do we change our beliefs at all? What do we do with the evidence and data going forward?"
  },
  {
    "objectID": "chapters/1_golem.html#we-model",
    "href": "chapters/1_golem.html#we-model",
    "title": "21  Ch 1. Golem of Prague",
    "section": "\n21.5 We model…",
    "text": "21.5 We model…\n\nattempting to mimic falsification is not a generally useful approach to statistical methods, what are we to do? We are to model.\n\n\nTools for golem engineering… We want to use our models for several distinct purposes: designing inquiry, extracting information from data, and making predictions. In this book I’ve chosen to focus on tools to help with each purpose. These tools are: (1) Bayesian data analysis (2) Model comparison (3) Multilevel models (4) Graphical causal models"
  },
  {
    "objectID": "chapters/1_golem.html#bayesian-data-analysis",
    "href": "chapters/1_golem.html#bayesian-data-analysis",
    "title": "21  Ch 1. Golem of Prague",
    "section": "\n21.6 Bayesian data analysis",
    "text": "21.6 Bayesian data analysis\n\nSupposing you have some data, how should you use it to learn about the world? There is no uniquely correct answer to this question. Lots of approaches, both formal and heuristic, can be effective. But one of the most effective and general answers is to use Bayesian data analysis. Bayesian data analysis takes a question in the form of a model and uses logic to produce an answer in the form of probability distributions. In modest terms, Bayesian data analysis is no more than counting the numbers of ways the data could happen, according to our assumptions. Things that can happen more ways are more plausible.\n\n\n21.6.1 Bayes vs frequentist probability\n\nBayesian probability … includes as a special case another important approach, the frequentist approach. The frequentist approach requires that all probabilities be defined by connection to the frequencies of events in very large samples\n\n\n\n\n\n\n\nFrequentist: (DR question)\n\n\n\n\n\n\nThis leads to frequentist uncertainty being premised on imaginary resampling of data—if we were to repeat the measurement many many times, we would end up collecting a list of values that will have some pattern to it. It means also that parameters and models cannot have probability distributions, only measurements can. The distribution of these measurements is called a sampling distribution. This resampling is never done, and in general it doesn’t even make sense—it is absurd to consider repeat sampling of the diversification of song birds in the Andes.\n\nDR_question: But Bayesian estimation also uses resampling. It also considers a draw from an imaginary distribution of ‘true parameters’. This also seems absurd, so why is it better?\n\nBayesian golems treat “randomness” as a property of information, not of the world. We just use randomness to describe our uncertainty in the face of incomplete knowledge.\n\nAgain, why is this better?\n\n\n\n\n\n\n\n\n\nAdvantage of Bayesian approach? (DR question)\n\n\n\n\n\n\nNote that the preceding description doesn’t invoke anyone’s “beliefs” or subjective opinions. Bayesian data analysis is just a logical procedure for processing information\n\n\nBayesian framework presents a distinct pedagogical advantage: many people find it more intuitive. Perhaps the best evidence for this is that very many scientists interpret non-Bayesian results in Bayesian terms, for example interpreting ordinary p-values as Bayesian posterior probabilities and non-Bayesian confidence intervals as Bayesian ones\n\nDR_question: how do we define a ‘Bayesian CI’?\n\n\n\n\n21.6.2 Model comparison and prediction.\n\nBayesian data analysis provides a way for models to learn from data. But when there is more than one plausible model-and in most mature fields there should be-how should we choose among them? One answer is to prefer models that make good predictions. This answer creates a lot of new questions, since knowing which model will make the best predictions seems to require knowing the future. We’ll look at two related tools, neither of which knows the future: cross-validation and information criteria\n\n\n21.6.3 Multilevel models\n\nMultilevel models-also known as hierarchical, random effects, varying effects, or mixed effects models\n\n\nCross-validation and information criteria measure overfitting risk and help us to recognize it. Multilevel models actually do something about it. What they do is exploit an amazing trick known as partial pooling\n\ne.g., to… >…adjust estimates for repeat sampling > … for imbalance in sampling\n… model variation explicitly (heterogeneity)\n\n\n\n\n\n\nDR question: Is multi-level modeling mainly about reducing overfitting?\n\n\n\n\n\nI had thought it was more about modeling the structure of the data in a more efficient way by reflecting something closer to the trye nature of randomness in the data. The ‘regularization’ (reducing overfit) seems to be a ‘side benefit’ of this. But is this a lucky coincidence and ‘suspicious convergence’?\n\n\n\n\nMultilevel models preserve the uncertainty in the original, pre-averaged values, while still using the average to make predictions\n\n\nSuddenly single level models end up looking like mere components of multilevel models; multilevel regression deserves to be the default form of regression\n\n\neven well-controlled treatments interact with unmeasured aspects of the individuals, groups, or populations studied\n\n\n21.6.4 Graphical causal models\n\nall we see is a statistical association. From the data alone, it could also be that the branches swaying makes the wind.\n\n\n\n\n\n\n\nA statistical model is an amazing association engine.\n\n\n\n\n\n\nIt makes it possible to detect associations between causes and their effects. But a statistical model is never sufficient for inferring cause, because the statistical model makes no distinction between the wind causing the branches to sway and the branches causing the wind to blow\n\n\n\n\n\n… a complete scientific model contains more information than a statistical model derived from it. And this additional information contains causal implications. These implications make it possible to test alternative causal models.\n\n\n\n\n\n\n\nModels that are causally incorrect can make better predictions than those that are causally correct\n\n\n\n\n\n\ntools like cross-validation are very useful. But these tools will happily recommend models that contain confounding variables and suggest incorrect causal relationships. Why? Confounded relationships are real associations, and they can improve prediction. After all, if you look outside and see branches swaying, it really does predict wind.\n\n\n\n\n\nDAGs\n\nDAGs are heuristic—they are not detailed statistical models. But they allow us to deduce which statistical models can provide valid causal inferences, assuming the DAG is true. But where does a DAG itself come from? The terrible truth about statistical inference is that its validity relies upon information outside the data.\n\n\nCausal Salad\n\nthe approach which dominates in many parts of biology and the social sciences is instead causal salad. Causal salad means tossing various ‘control’ variables into a statistical model, observing changes in estimates, and then telling a story about causation. Causal salad seems founded on the notion that only omitted variables can mislead us about causation. But included variables can just as easily confound us\n\n4\n\nInstead of choosing among various black-box tools for testing null hypotheses, we should learn to build and analyze multiple non-null models of natural phenomena. To support this goal, the chapter introduced Bayesian inference, model comparison, multilevel models, and graphical causal models."
  },
  {
    "objectID": "chapters/1_golem.html#session-info",
    "href": "chapters/1_golem.html#session-info",
    "title": "21  Ch 1. Golem of Prague",
    "section": "Session info",
    "text": "Session info\n\nCodesessionInfo()\n\nR version 4.1.2 (2021-11-01)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nloaded via a namespace (and not attached):\n [1] digest_0.6.29     jsonlite_1.8.2    magrittr_2.0.3    evaluate_0.17    \n [5] rlang_1.0.6       stringi_1.7.8     cli_3.4.1         renv_0.15.2      \n [9] rstudioapi_0.14   rmarkdown_2.17    tools_4.1.2       stringr_1.4.1    \n[13] htmlwidgets_1.5.4 xfun_0.33         fastmap_1.1.0     compiler_4.1.2   \n[17] htmltools_0.5.3   knitr_1.40"
  },
  {
    "objectID": "chapters/2_small_large_worlds.html",
    "href": "chapters/2_small_large_worlds.html",
    "title": "22  Ch 2. Small/Large Worlds",
    "section": "",
    "text": "Codelibrary(pacman)\np_load(dplyr, magrittr, ggplot2, stringr, tidyr, install = FALSE)"
  },
  {
    "objectID": "chapters/2_small_large_worlds.html#garden-of-forking-data",
    "href": "chapters/2_small_large_worlds.html#garden-of-forking-data",
    "title": "22  Ch 2. Small/Large Worlds",
    "section": "\n22.1 2.1. Garden of Forking data",
    "text": "22.1 2.1. Garden of Forking data\n\nThe small world is the self-contained logical world of the model.\n\n\nThe way that Bayesian models learn from evidence is arguably optimal in the small world. When their assumptions approximate reality, they also perform well in the large world. But large world performance has to be demonstrated rather than logically deduced.\n\n\nThis demonstrates that there are three (out of 64) ways for a bag containing [a combo of blue and white marbles] to produce the data. The inferential power comes from comparing this count to the numbers of ways each of the other conjectures of the bag’s contents could produce the same data.\n\n\n…. can be computed just by multiplying the new count by the old count. This updating approach amounts to nothing more than asserting that (1) when we have previous information suggesting there are \\(W_prior\\) ways for a conjecture to produce a previous observation \\(D_{prior}\\) and (2) we acquire new observations \\(D_{new}\\) that the same conjecture can produce in \\(W_{new}\\) ways, then (3) the number of ways the conjecture can account for both \\(D_{prior}\\) as well as \\(D_{new}\\) is just the product \\(W_{prior} \\times W_{new}\\).\n\n\nThis is sometimes known as the principle of indifference: When there is no reason to say that one conjecture is more plausible than another, weigh all of the conjectures equally. This book does not use nor endorse “ignorance” priors. As we’ll see in later chapters, the structure of the model and the scientific context always provide information that allows us to do better than ignorance.\n\n\n\n\n\n\n\n\nNote: I’m skipping the construction of the ‘forking paths’ plot pasted above\n\n\n\n\n\nKurz laboriously calculates the values and constructs it using tibbles and ggplot. Not sure what the latter teaches us.\n\n\n\n2.1.2 Using prior information\nSome functions and data for tabulating ‘how likely is the data we drew’ (marbles we saw) given different bag compositions. We ‘count the (equally likely) ways’ below. This is the product of ‘ways of drawing the first (blue) marble’, the second ‘white’, and the third ‘blue’ marble.\n\nWays of producing data# if we make two custom functions, here, it will simplify the code within `mutate()`, below\nn_blue <- function(x) {\n  rowSums(x == \"b\")\n}\n\nn_white <- function(x) {\n  rowSums(x == \"w\")\n}\n\nt <-\n  # for the first four columns, `p_` indexes position\n  tibble(p_1 = rep(c(\"w\", \"b\"), times = c(1, 4)),\n         p_2 = rep(c(\"w\", \"b\"), times = c(2, 3)),\n         p_3 = rep(c(\"w\", \"b\"), times = c(3, 2)),\n         p_4 = rep(c(\"w\", \"b\"), times = c(4, 1))) %>%\n  mutate(`draw 1: blue`  = n_blue(.),\n         `draw 2: white` = n_white(.),\n         `draw 3: blue`  = n_blue(.)) %>%\n  mutate(`ways to produce` = `draw 1: blue` * `draw 2: white` * `draw 3: blue`)\n\nt %>%  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\np_1\np_2\np_3\np_4\ndraw 1: blue\ndraw 2: white\ndraw 3: blue\nways to produce\n\n\n\nw\nw\nw\nw\n0\n4\n0\n0\n\n\nb\nw\nw\nw\n1\n3\n1\n3\n\n\nb\nb\nw\nw\n2\n2\n2\n8\n\n\nb\nb\nb\nw\n3\n1\n3\n9\n\n\nb\nb\nb\nb\n4\n0\n4\n0\n\n\n\n\n\nNext we get more data: another blue marble (already shown above), and there is a ‘shortcut’:\n\nYou could start all over again, making a garden with four layers to trace out the paths compatible with the data sequence. Or you could take the previous counts—the prior counts—over conjectures (0, 3, 8, 9, 0) and just update them in light of the new observation. It turns out that these two methods are mathematically identical, as long as the new observation is logically independent of the previous observations\n\n\nWays; add a marblet <-\n  t %>%\n  rename(`previous counts` = `ways to produce`,\n         `ways to produce` = `draw 1: blue`) %>%\n  select(p_1:p_4, `ways to produce`, `previous counts`) %>%\n  mutate(`new count` = `ways to produce` * `previous counts`)\n\n\nWe can also incorporate ‘different data’ – e,g., certain amounts of ‘prior counts of each bag in the factory’ … we simply multiply these in. ‘You first draw a factory bag with a particular count … which can occur XXX ways, and then you draw a blue marble from that bag, which can occur YYY ways, etc’.\n\nadd factory countst <- t %>%\nselect(p_1:p_4, `new count`) %>%\nrename(`prior count` = `new count`) %>%\n  mutate(`factory count` = c(0, 3:0)) %>%\nmutate(`new count` = `prior count` * `factory count`)\n\nt %>%\nknitr::kable()\n\n\n\np_1\np_2\np_3\np_4\nprior count\nfactory count\nnew count\n\n\n\nw\nw\nw\nw\n0\n0\n0\n\n\nb\nw\nw\nw\n3\n3\n9\n\n\nb\nb\nw\nw\n16\n2\n32\n\n\nb\nb\nb\nw\n27\n1\n27\n\n\nb\nb\nb\nb\n0\n0\n0\n\n\n\n\n\n2.1.3. From counts to probability\n\nPutting this all together to compute the plausibilities\n\n\n\n\n\n\nDR question: Is this with a flat prior\n\n\n\nIs this with a flat prior… starting with an equal probability of each bag type, or is ‘plausibility’ something different than the posterior?\n\n\n\nplausibilitiest %>%\n  select(p_1:p_4) %>%\n  mutate(p                      = seq(from = 0, to = 1, by = .25),\n         `ways to produce data` = c(0, 3, 8, 9, 0)) %>%\n  mutate(plausibility = `ways to produce data` / sum(`ways to produce data`))\n\n# A tibble: 5 × 7\n  p_1   p_2   p_3   p_4       p `ways to produce data` plausibility\n  <chr> <chr> <chr> <chr> <dbl>                  <dbl>        <dbl>\n1 w     w     w     w      0                         0         0   \n2 b     w     w     w      0.25                      3         0.15\n3 b     b     w     w      0.5                       8         0.4 \n4 b     b     b     w      0.75                      9         0.45\n5 b     b     b     b      1                         0         0"
  },
  {
    "objectID": "chapters/2_small_large_worlds.html#building-a-model",
    "href": "chapters/2_small_large_worlds.html#building-a-model",
    "title": "22  Ch 2. Small/Large Worlds",
    "section": "\n22.2 2.2. Building a model",
    "text": "22.2 2.2. Building a model\n\nDesigning a simple Bayesian model benefits from a design loop with three steps. (1) Data story: Motivate the model by narrating how the data might arise. (2) Update: Educate your model by feeding it the data. (3) Evaluate: All statistical models require supervision, leading to model revision\n\n\n\n\n\n\n\nDR question\n\n\n\nHow can we ‘revise the model’ without overfitting or otherwise cheating in some way that overstates the confidence we should have in our results?\n\n\n\nThe maximum height of the curve increases with each sample, meaning that fewer values of \\(p\\) amass more plausibility as the amount of evidence increases\n\nPower of Bayesian inference in small-sample contexts\n\nWhy? In non-Bayesian statistical inference, procedures are often justified by the method’s behavior at very large sample sizes, so-called asymptotic behavior. As a result, performance at small samples sizes is questionable. In contrast, Bayesian estimates are valid for any sample size. This does not mean that more data isn’t helpful—it certainly is. Rather, the estimates have a clear and valid interpretation, no matter the sample size. But the price for this power is dependency upon the initial plausibilities, the prior. If the prior is a bad one, then the resulting inference will be misleading.\n\nDR note: There are some frequentist/non-Bayesian procedures and tests that don’t rely on large sample approximations; e.g., Fisher’s exact test\n2.2.1 - the ‘globe tossing’ data story\n2.2.2 Bayesian updating\nStart with a particular sequence of data, accumulate trials and successes\n\nCode(d <-\n    tibble(\n      toss = c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\"),\n      n_trials = 1:9,\nn_success = cumsum(toss == \"w\")\n  )\n)\n\n# A tibble: 9 × 3\n  toss  n_trials n_success\n  <chr>    <int>     <int>\n1 w            1         1\n2 l            2         1\n3 w            3         2\n4 w            4         3\n5 w            5         4\n6 l            6         4\n7 w            7         5\n8 l            8         5\n9 w            9         6\n\n\nNext, we compute and plot the plausibility of every ‘true share of water p’ after observing the draws from the globe-tossing. We update this after each toss.\nOK I need to construct some data on this globe tossing first; I skipped this earlier\nNext we build the tibble plausibility/updating tibble.\nGoing through the coding steps for my own benefit… First we expand the tibble to consider each of 50 possible p_water values after each trial (toss).\n\nCodesequence_length <- 50\n\n(\nplaus_globe_updates <-\nd %>%\n  expand(nesting(n_trials, toss, n_success),\n         p_water = seq(from = 0, to = 1, length.out = sequence_length))\n)\n\n# A tibble: 450 × 4\n   n_trials toss  n_success p_water\n      <int> <chr>     <int>   <dbl>\n 1        1 w             1  0     \n 2        1 w             1  0.0204\n 3        1 w             1  0.0408\n 4        1 w             1  0.0612\n 5        1 w             1  0.0816\n 6        1 w             1  0.102 \n 7        1 w             1  0.122 \n 8        1 w             1  0.143 \n 9        1 w             1  0.163 \n10        1 w             1  0.184 \n# … with 440 more rows\n\n\nNext we create the ‘lagged’ columns (for ease of plotting the updating):\n\nupdating globe tossing - lagsplaus_globe_updates %<>%\n  group_by(p_water) %>%\n # you can learn more about lagging here: https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/lag or here: https://dplyr.tidyverse.org/reference/lead-lag.html\n  mutate(lagged_n_trials  = lag(n_trials,  k = 1),\n         lagged_n_success = lag(n_success, k = 1)) %>%\n  ungroup()\n\nplaus_globe_updates[95:105,]\n\n# A tibble: 11 × 6\n   n_trials toss  n_success p_water lagged_n_trials lagged_n_success\n      <int> <chr>     <int>   <dbl>           <int>            <int>\n 1        2 l             1  0.898                1                1\n 2        2 l             1  0.918                1                1\n 3        2 l             1  0.939                1                1\n 4        2 l             1  0.959                1                1\n 5        2 l             1  0.980                1                1\n 6        2 l             1  1                    1                1\n 7        3 w             2  0                    2                1\n 8        3 w             2  0.0204               2                1\n 9        3 w             2  0.0408               2                1\n10        3 w             2  0.0612               2                1\n11        3 w             2  0.0816               2                1\n\n\nNext we start with a flat prior and, for each ‘trial’ …\n\ncompute the likelihood of the data (sucesses and trials) given each probability of water, according to the binomial probability function.\nboth with the previous ‘lagged’ data and adding the new data point\nnormalize each of these by dividing by the likelihood of the data that has arisen,\nthis yields the ‘plausbility’ of each probability of water . (I.e., the posterior?)\n\n\nCodeplaus_globe_updates %<>%\n  mutate(prior  = ifelse(n_trials == 1, 1, #DR: I adjusted this to =1 to avoid confusing it with a certainty that p=.5\n          dbinom(\n            x= lagged_n_success,\n            size = lagged_n_trials,\n            prob = p_water)),\n         likelihood = dbinom(x  = n_success,\n                             size = n_trials,\n                             prob = p_water),\n         strip      = str_c(\"n = \", n_trials)) %>%\n  # the next three lines allow us to normalize the prior and the likelihood,\n  # putting them both in a probability metric\n  group_by(n_trials) %>%\n  mutate(prior      = prior      / sum(prior),\n         likelihood = likelihood / sum(likelihood))\n\n\nPlotting this:\n\nCodeplaus_globe_updates %>%\n  # filter(n_trials==5) %>%\n  # plot!\n  ggplot(aes(x = p_water)) +\n  geom_line(aes(y = prior), linetype = 2) +\n  geom_line(aes(y = likelihood)) +\n  scale_x_continuous(\"proportion water\", breaks = c(0, .5, 1)) +\n  scale_y_continuous(\"plausibility\", breaks = NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~strip, scales = \"free_y\")\n\n\n\n\n1\n\n\n\n\n\n\nOther love letter notes on the above\n\n\n\n\n\n\nIf it wasn’t clear in the code, the dashed curves are normalized prior densities. The solid ones are normalized likelihoods. If you don’t normalize (i.e., divide the density by the sum of the density), their respective heights don’t match up with those in the text. Furthermore, it’s the normalization that makes them directly comparable.\n\n\nTo learn more about dplyr::group_by() and its opposite dplyr::ungroup(), check out R4DS, Chapter 5. To learn about tidyr::expand(), go here."
  },
  {
    "objectID": "chapters/2_small_large_worlds.html#components-of-the-model",
    "href": "chapters/2_small_large_worlds.html#components-of-the-model",
    "title": "22  Ch 2. Small/Large Worlds",
    "section": "\n22.3 2.3. Components of the model",
    "text": "22.3 2.3. Components of the model\n\n\na likelihood function: “the number of ways each conjecture could produce an observation”\n\n\n\n\none or more parameters: “the accumulated number of ways each conjecture could produce the entire data”\n\n\n\n\na prior: “the initial plausibility of each conjectured cause of the data”\n\n\n\n\n\n\n\n\nCoding tip: dbinom, pbinom, rbinom, etc\n\n\n\n\n\n\n“d” in dbinom stands for density. Functions named in this way almost always have corresponding partners that begin with “r” for random samples and that begin with “-p” for cumulative probabilities\n\n\n\n\n\nThe distributions we assign to the observed variables typically have their own variables.\n\n\n\n\n\n\n\n(Comparing frameworks) Rethinking: A central role for likelihood.\n\n\n\n\n\n\nA great deal of ink has been spilled focusing on how Bayesian and non-Bayesian data analyses differ. Focusing on differences is useful, but sometimes it distracts us from fundamental similarities. Notably, the most influential assumptions in both Bayesian and many non-Bayesian models are the distributions assigned to data, the likelihood functions. The likelihoods influence inference for every piece of data, and as sample size increases, the likelihood matters more and more. This helps to explain why Bayesian and non-Bayesian inferences are often so similar. If we had to explain Bayesian inference using only one aspect of it, we should describe likelihood, not priors.\n\n(DR: move this to our discussion of compring statistical frameworks?)\n\n\n\nWhat prior?\n\nSo where do priors come from? They are both engineering assumptions, chosen to help the machine learn, and scientific assumptions, chosen to reflect what we know about a phenomenon. The flat prior in Figure 2.5 is very common, but it is hardly ever the best prior.\n\n\nThere is a school of Bayesian inference that emphasizes choosing priors based upon the personal beliefs of the analyst. While this subjective Bayesian approach thrives in some statistics and philosophy and economics programs, it is rare in the sciences.\n\n\nIf your goal is to lie with statistics, you’d be a fool to do it with priors, because such a lie would be easily uncovered. Better to use the more opaque machinery of the likelihood. Or better yet-don’t actually take this advice!—massage the data, drop some “outliers,” and otherwise engage in motivated data transformation\n\n\nbecause non-Bayesian procedures need to make choices that Bayesian ones do not, such as choice of estimator or likelihood penalty.\n\nDR: I skip the construction of the ‘multiply prior by likelihood to get posterior’ graphs for now …\nBelow: flat, stepped, Laplace priors"
  },
  {
    "objectID": "chapters/2_small_large_worlds.html#making-the-model-go",
    "href": "chapters/2_small_large_worlds.html#making-the-model-go",
    "title": "22  Ch 2. Small/Large Worlds",
    "section": "\n22.4 2.4 ‘Making the model go’",
    "text": "22.4 2.4 ‘Making the model go’\n\n\n\n\n\n\nImportant\n\n\n\nDR: I think this section is particularly important. It’s our first pass on ‘how to actually do this stuff’. Reading group: I suggest we put some focus on it, maybe in the next session.\n\n\n2.4.1 Bayes theorem\nIn word form:\n\nPosterior (probability of any given value of \\(p\\)) =\n\n(Probability of the data [given p] \\(\\times\\) [Prior probability of p]) divided by the ‘Average probability of the data’\nI.e., (in my own words) ‘how likely is this data and the particular parameter p’ divided by ‘the probability of this data overall’ (given any p, with the probability of each p following the prior)\n\n‘average probability of the data’\n\n\nAveraged over what? Averaged over the prior. It’s job is just to standardize the posterior, to ensure it sums (integrates) to one. In mathematical form: Pr(W, L) = E\n\nProbability of one Water followed by one Land:\n\\[Pr(W, L|p)\\] \\[=E\\Big( Pr(W, L|p) Big)\\]\n\\[= \\int  Pr(W,L|p) Pr(p)dp \\]\n\nThe key lesson is that the posterior is proportional to the product of the prior and the probability of the data [given the prior]. Why? Because for each specific value of p, the number of paths through the garden of forking data is the product of the prior number of paths and the new number of paths. A flat prior constructs a posterior that is simply proportional to the likelihood\n\n\n\n\n\n\n\n“Bayesian data analysis isn’t about Bayes’ theorem”\n\n\n\n\n\nDissing the ‘HIV test false positive’ thing\n\nInference under any probability concept will eventually make use of Bayes’ theorem. Common introductory examples of ‘Bayesian’ analysis using HIV and DNA testing are not uniquely Bayesian\n\n\n\n\n\nNumerical techniques for computing posterior distributions: (1) Grid approximation (2) Quadratic approximation (3) Markov chain Monte Carlo (MCMC)\n\n\nGrid approximation: Basically mechanical Bayesian updating of the probability of a parameter value being in a range, dividing up the space of possible parameters into different ranges. (And then smoothing?)\n\n\n… achieve an excellent approximation of the continuous posterior distribution by considering only a finite grid of parameter values\n\n\nin most of your real modeling, grid approximation isn’t practical. The reason is that it scales very poorly, as the number of parameters increases\n\n\n22.4.1 Grid approximation\nThe code below makes a data frame with\n\n20 Probabilities between 0 and 1\nA ‘flat’ density (=1 everywhere) for these\nThe likelihood of “6 waters in 9” (the data) given each of the 20 probabilities (binomial distribution)\nThis likelihood \\(\\times\\) the prior for each p_water.\n\n\nRem: this is likelihood of a particular p_water and the observed data given that p_water\n\nGiven the flat prior it is simply the latter\n\n\nStandardizing this by dividing by the probability of the data observed\n\n\nGrid, computing posteriors(\n  d <-\n    tibble(p_grid = seq(from = 0, to = 1, length.out = 20),      # define grid\n           prior  = 1) %>%                                       # define prior\n    mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %>%  # compute likelihood at each value in grid\n    mutate(unstd_posterior = likelihood * prior) %>%             # compute product of likelihood and prior\n    mutate(posterior = unstd_posterior / sum(unstd_posterior))   # standardize the posterior, so it sums to 1\n)\n\n# A tibble: 20 × 5\n   p_grid prior likelihood unstd_posterior   posterior\n    <dbl> <dbl>      <dbl>           <dbl>       <dbl>\n 1 0          1 0               0          0          \n 2 0.0526     1 0.00000152      0.00000152 0.000000799\n 3 0.105      1 0.0000819       0.0000819  0.0000431  \n 4 0.158      1 0.000777        0.000777   0.000409   \n 5 0.211      1 0.00360         0.00360    0.00189    \n 6 0.263      1 0.0112          0.0112     0.00587    \n 7 0.316      1 0.0267          0.0267     0.0140     \n 8 0.368      1 0.0529          0.0529     0.0279     \n 9 0.421      1 0.0908          0.0908     0.0478     \n10 0.474      1 0.138           0.138      0.0728     \n11 0.526      1 0.190           0.190      0.0999     \n12 0.579      1 0.236           0.236      0.124      \n13 0.632      1 0.267           0.267      0.140      \n14 0.684      1 0.271           0.271      0.143      \n15 0.737      1 0.245           0.245      0.129      \n16 0.789      1 0.190           0.190      0.0999     \n17 0.842      1 0.118           0.118      0.0621     \n18 0.895      1 0.0503          0.0503     0.0265     \n19 0.947      1 0.00885         0.00885    0.00466    \n20 1          1 0               0          0          \n\n\nWe then plot the posterior probabilities of each p_water:\n\nCode(\n  p1 <-\n  d %>% \n  ggplot(aes(x = p_grid, y = posterior)) +\n  geom_point() +\n  geom_line() +\n  labs(subtitle = \"20 points\",\n       x = \"probability of water\",\n       y = \"posterior probability\") +\n  theme(panel.grid = element_blank())\n)\n\n\n\n\nQuadratic approximation (summary)\n\nUnder quite general conditions, the region near the peak of the posterior distribution will be nearly Gaussian-or “normal”—in shape. This means the posterior distribution can be usefully approximated by a Gaussian distribution. A Gaussian distribution is convenient, because it can be completely described by only two numbers: the location of its center (mean) and its spread (variance)\n\n\nlogarithm of a Gaussian distribution forms a parabola. And a parabola is a quadratic function\n\n\nFor many of the most common procedures in applied statistics-linear regression, for example—the approximation works very well\n\nStepping through this\n\n\nFind the posterior mode\n\n\nSome optimization algorithm, a procedure that virtually “climbs” the posterior distribution\n\n\nestimate the curvature near the peak. This curvature is sufficient to compute a quadratic approximation of the entire posterior distribution. In some cases, these calculations can be done analytically, but…\n\n\n\n\n\n\n\n\nDR question\n\n\n\nBut how does it do this estimate of the curvature? Does it come out of many simulations, or the optimizing hill climbing thing, or??\n\n\n\n\n\n\n\n\nThe Hession and quadratic approximation\n\n\n\n\n\n\nHessian is a square matrix of second derivatives. It is used for many purposes in mathematics, but in the quadratic approximation it is second derivatives of the log of posterior probability with respect to the parameters. It turns out that these derivatives are sufficient to describe a Gaussian distribution, because the logarithm of a Gaussian distribution is just a parabola. Parabolas have no derivatives beyond the second\n\n\n\n\nApplying the quadratic approximation to the globe tossing data with rethinking::map().\n\n\n\n\n\n\nrethinking::map() coding notes\n\n\n\n\n\nHere the ‘love letter’ uses McElreath’s rethinking package instead of brms … I guess it’s because quadratic approximation is more for learning than for real use?\nquap and map:\n\n“Find mode of posterior distribution for arbitrary fixed effect models”\n“and then produce an approximation of the full posterior using the quadratic curvature at the mode.”\n\n(This has nothing to do with purrr::map iteration package thing.)\n\n\n\n\nquap with rethinking::maplibrary(rethinking)\n\nglobe_qa <-\n  rethinking::map(\n    alist(\n      w ~ dbinom(9, p),  # binomial likelihood\n      p ~ dunif(0, 1)    # uniform prior\n    ), \n    data = list(w = 6))\n\n# display summary of quadratic approximation\nprecis(globe_qa)\n\n       mean        sd      5.5%     94.5%\np 0.6666664 0.1571338 0.4155362 0.9177967\n\n\n\n\n\n\n\n\nSyntax of above code\n\n\n\n\n\nalist is a way of specifying a list (iirc) that preserves it as a ‘name of the math we want to compute on’ rather than it actually expanding/evaluating it.\nprecis is just a handy tool for seeing data .\nThe output globe_qa contains a lot of content (try str(globe_qa)).\n\n\n\nAbove, the precis gives us some summary statistics on the estimated posterior.\nDR: I will skip the code that produces the figure below showing how quap updates with more data, for now.\n\nMarkov chain Monte Carlo (incomplete explanation)\nDR:MCMC is not fully explained here. Somehow you ’sample from the posterior (over the parameter values)” and do some magic. Maybe weighting these by how consistent they are with the prior and the data?\n\nGrid approximation routinely fails here, because it just takes too long—the Sun will go dark before your computer finishes the grid. Special forms of quadratic approximation might work, if everything is just right. But commonly, something is not just right. Furthermore, multilevel models do not always allow us to write down a single, unified function for the posterior distribution. This means that the function to maximize (when finding the MAP) is not known, but must be computed in pieces\n\n\nis fair to say that MCMC is largely responsible for the insurgence of Bayesian data analysis that began in the 1990s.\n\n\nhighly non-obvious strategy. Instead of attempting to compute or approximate the posterior distribution directly, MCMC techniques merely draw samples from the posterior. You end up with a collection of parameter values, and the frequencies of these values correspond to the posterior plausibilities. You can then build a picture of the posterior from the histogram of these samples. We nearly always work directly with these samples, rather than first constructing some mathematical estimate from them\n\n\n, a working Markov chain for the globe tossing model does not require much code\n\nHe starts with \\(p=.5\\), considers the likelihood of the data under this probability, adjusts p.\n\n\n\n\n\n\n\nDR question\n\n\n\nI don’t see how this MCMC works; anyone have more insight? And where does the prior come in here?)\n\n\n\nKurz: Markov chain Monte Carlo\nBringing in brms\n\nCodelibrary(brms)\n\n\n\nHere re-fit the last model from above, the one for which \\(w = 24\\) and \\(n = 36\\).\n\n\nCodeb2.1 <-\n  brm(data = list(w = 24), \n      family = binomial(link = \"identity\"),\n      w | trials(36) ~ 1,\n      prior(beta(1, 1), class = Intercept),\n      iter = 4000, warmup = 1000,\n      control = list(adapt_delta = .95),\n      seed = 2,\n      file = \"b02.01\")\n\n\nDR: Not sure why we save a file here\n\nThe model output from brms looks like so.\n\n\nCodeprint(b2.1)\n\n Family: binomial \n  Links: mu = identity \nFormula: w | trials(36) ~ 1 \n   Data: list(w = 24) (Number of observations: 1) \n  Draws: 4 chains, each with iter = 4000; warmup = 1000; thin = 1;\n         total post-warmup draws = 12000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.66      0.07     0.50     0.79 1.00     3965     4326\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nThere’s a lot going on in that output, which we’ll start to clarify in Chapter 4. For now, focus on the ‘Intercept’ line. As we’ll also learn in Chapter 4, the intercept of a regression model with no predictors is the same as its mean. In the special case of a model using the binomial likelihood, the mean is the probability of a 1 in a given trial, \\(\\theta\\).\n\nLet’s plot the results of our model and compare them with those from rethinking::map(), above.\n\nCodeposterior_samples(b2.1) %>% \n  mutate(n = \"n = 36\") %>%\n\n  ggplot(aes(x = b_Intercept)) +\n  geom_density(fill = \"black\") +\n  scale_x_continuous(\"proportion water\", limits = c(0, 1)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~n)\n\n\n\n\n\nIf you’re still confused. Cool. This is just a preview. We’ll start walking through fitting models in brms in [Chapter 4][A Gaussian model of height] and we’ll learn a lot about regression with the binomial likelihood in [Chapter 10][Counting and Classification].\n\nPractice questions\n\nThe target of inference in Bayesian inference is a posterior probability distribution. Posterior probabilities state the relative numbers of ways each conjectured cause of the data could have produced the data"
  },
  {
    "objectID": "chapters/3_sampling_the_imaginary.html",
    "href": "chapters/3_sampling_the_imaginary.html",
    "title": "23  Ch 3. Sampling the Imaginary (posterior)",
    "section": "",
    "text": "packageslibrary(pacman)\np_load(dplyr, magrittr, ggplot2, stringr, tidyr, install = FALSE)\n1"
  },
  {
    "objectID": "chapters/3_sampling_the_imaginary.html#sampling-from-a-grid-approximate-posterior",
    "href": "chapters/3_sampling_the_imaginary.html#sampling-from-a-grid-approximate-posterior",
    "title": "23  Ch 3. Sampling the Imaginary (posterior)",
    "section": "\n23.1 3.1. Sampling from a grid-approximate posterior",
    "text": "23.1 3.1. Sampling from a grid-approximate posterior\nGlobe-tossing model. Analytically, we started with a prior, updated it on the data, and got a posterior; see extensive plots of this in the last chapter. We also did this (for demonstration) with the grid-approximation approach.\n\n\n\n\n\n\nGrid-approximation … generating 1000 probabilities & likelihoods\n\n\n\n\n\nThis is almost as in the last chapter, with slightly different naming of things, so I collapse it here\n\nCode# how many grid points would you like?\nn <- 1001\nn_success <- 6\nn_trials  <- 9\n\n(\n  d <-\n  tibble(p_grid = seq(from = 0, to = 1, length.out = n),\n         # note we're still using a flat uniform prior\n         prior  = 1) %>% \n  mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %>%\n  mutate(posterior = (likelihood * prior) / sum(likelihood * prior))\n)\n\n# A tibble: 1,001 × 4\n   p_grid prior likelihood posterior\n    <dbl> <dbl>      <dbl>     <dbl>\n 1  0         1   0         0       \n 2  0.001     1   8.37e-17  8.37e-19\n 3  0.002     1   5.34e-15  5.34e-17\n 4  0.003     1   6.07e-14  6.07e-16\n 5  0.004     1   3.40e-13  3.40e-15\n 6  0.005     1   1.29e-12  1.29e-14\n 7  0.006     1   3.85e-12  3.85e-14\n 8  0.007     1   9.68e-12  9.68e-14\n 9  0.008     1   2.15e-11  2.15e-13\n10  0.009     1   4.34e-11  4.34e-13\n# … with 991 more rows\n\n\n\n\n\nNext we sample from the 1000 probabilities in the grid d with weights equal to the posteriors (d$posterior), generating a tibble of 10,000 samples (sample).\n\n10k samples from the grid posterior# how many samples would you like?\nn_samples <- 1e4\n\n# make it reproducible\nset.seed(3)\n\nsamples <-\n  d %>% \n  sample_n(size = n_samples, weight = posterior, replace = T)\n\nglimpse(samples)\n\nRows: 10,000\nColumns: 4\n$ p_grid     <dbl> 0.564, 0.651, 0.487, 0.592, 0.596, 0.787, 0.727, 0.490, 0.7…\n$ prior      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ likelihood <dbl> 0.22408531, 0.27179502, 0.15128823, 0.24557832, 0.24825668,…\n$ posterior  <dbl> 0.0022408531, 0.0027179502, 0.0015128823, 0.0024557832, 0.0…\n\n\nNext we plot the samples.\n\nDR: I played around with this. Rather than getting the arbitrary zigzag, I first sort by probability of water (p_grid). This yields a smooth curve, where a shallower line and greater horizontal distance means ‘more samples in this area’.\n\n\nNumbering and plotting samplessamples %>% \n  arrange(p_grid) %>% \n  mutate(sample_number = 1:n()) %>% \n  ggplot(aes(x = sample_number, y = p_grid)) +\n  geom_line(size = 1/10) +\n  scale_y_continuous(\"proportion of water (p)\", limits = c(0, 1)) +\n  xlab(\"sample number\")\n\n\n\n\nWe’ll make the density in the right panel with geom_density().\n\nCodesamples %>% \n  ggplot(aes(x = p_grid)) +\n  geom_density(fill = \"black\") +\n  scale_x_continuous(\"proportion of water (p)\", limits = c(0, 1))\n\n\n\n\nIt’s a bit lumpy because 10k samples isn’t enough. But I won’t bother with the larger number of samples here."
  },
  {
    "objectID": "chapters/3_sampling_the_imaginary.html#sampling-to-summarize",
    "href": "chapters/3_sampling_the_imaginary.html#sampling-to-summarize",
    "title": "23  Ch 3. Sampling the Imaginary (posterior)",
    "section": "\n23.2 3.2. Sampling to summarize",
    "text": "23.2 3.2. Sampling to summarize\n\n\n\n\n\n\nTerminology: ‘compatibility interval’\n\n\n\n\n\n\ninterval of posterior probability, such as the ones we are working with [below],\n\n\nWe’re going to call it a compatibility interval instead, in order to avoid the unwarranted implications of “confidence” and “credibility.”\n\n\na range of parameter values compatible with the model and data\n\n\n\n\n\nAll you’ve done so far is crudely replicate the posterior density you had already computed. … next it is time to use these samples to describe and understand the posterior.\n\nAt least with 1 dimensional data, it’s easy to ask and answer very specific questions of the posterior:\nIntervals of defined boundaries\n\nSuppose I ask you for the posterior probability that the proportion of water is less than 0.5.\n\nHere, you could do this simply from the grid approximation calculation\n2\n\nCoded %>% \n  filter(p_grid < .5) %>% \n  summarise(sum = sum(posterior))\n\n# A tibble: 1 × 1\n    sum\n  <dbl>\n1 0.171\n\n\nHowever: grid approximation isn’t used much.\n\nsince grid approximation isn’t practical in general, it won’t always be so easy. Once there is more than one parameter in the posterior distribution\n\n… so instead we will practice taking samples from the posterior.\n\nsimilarly add up all of the samples below 0.5, but also divide the resulting count by the total number of samples.\n\nKurz offers three tidyverse ways of coding this calculation, which may be useful in different circumstances:\n\nfilter and summarisesamples %>%\n  filter(p_grid < .5) %>% \n  summarise(sum = n() / n_samples)\n\n# A tibble: 1 × 1\n    sum\n  <dbl>\n1 0.162\n\nfilter and summarise#note 'n_samples' was previously defined here\n\n\n\ncount and mutate with dividesamples %>% \n  count(p_grid < .5) %>% \n  mutate(probability = n / sum(n))\n\n# A tibble: 2 × 3\n  `p_grid < 0.5`     n probability\n  <lgl>          <int>       <dbl>\n1 FALSE           8377       0.838\n2 TRUE            1623       0.162\n\n\n\nSummarize a logical condition#And an even trickier approach for the same is to insert the logical statement `p_grid < .5` within the `mean()` function.\n\nsamples %>%\n  summarise(sum = mean(p_grid < .5))\n\n# A tibble: 1 × 1\n    sum\n  <dbl>\n1 0.162\n\n\nWe can do similar for intervals (skipped here)\n\nTo determine the posterior probability between 0.5 and 0.75, you can use & within filter().\n\nIntervals of defined mass\nWe update to consider a base of ‘3 successes in 3 trials’, to contrast HPDI from the ‘percentile intervals’ (and similar for point estimates).\nDR: This is a pretty interesting case. I often am presented with ‘a small amount of data all going in one direction’ (e.g., ‘of the 4 people who clicked on the web site, all chose the medium-level donation pledge’) and want to consider what inferences I can make about the population – for decision-relevant purposes.\n\nCode# here we update the `dbinom()` parameters\nn_success <- 3\nn_trials  <- 3\n\n# update `d`\nd <-\n  d %>% \n  mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %>% \n  mutate(posterior  = (likelihood * prior) / sum(posterior))\n\n# make the next part reproducible\nset.seed(3)\n\n# here's our new samples tibble\n  samples <-\n    d %>% \n    sample_n(size = n_samples, weight = posterior, replace = T)\n\n\nPercentile intervals (PI) “assign equal probability mass to each tail …”\n\nBut in this example, it ends up excluding the most probable parameter values, near p = 1. So in terms of describing the shape of the posterior distribution-which is really all these intervals are asked to do—the percentile interval can be misleading.\n\nHighest posterior density interval (HPDI): “the narrowest interval containing the specified probability mass”\n\nright-hand plot in Figure 3.3 displays the 50% HPDI is\n\n\n\nWhich to use?\n\nHPDI is more computationally intensive than PI and suffers from greater simulation variance, which is a fancy way of saying that it is sensitive to how many samples you draw from the posterior. It is also harder to understand and many scientific audiences will not appreciate its features, hile they will immediately understand a percentile interval, as ordinary non-Bayesian intervals are typically interpreted (incorrectly) as percentile intervals\n\n\n\n\n\n\n\nSmall world caveats\n\n\n\n\n\n\nBut whether you use a Bayesian interpretation or not, a 95% interval does not contain the true value 95% of the time. The history of science teaches us that confidence intervals exhibit chronic overconfidence.\n\n\nThe 95% is a small world number\n\n\n\n\nKurz plots these intervals for our example, but the code he uses first is simply taking an arbitrary part of the computed distribution, not calculating anything. I’ll skip it.\n\n\n\n\n\n\nDR question:\n\n\n\nHow is HPDI computed? … I assume it iterates over intervals, draws observations from within this interval from the posterior sample and sums the probability mass until it settles on the ‘best one’?\n\n\n\n\n\n\n\n\nBringing in the tidybayes package\n\n\n\n\n\n\nMatthew Kay’s tidybayes package, … offers an array of convenience functions for summarizing Bayesian models\n\nDR: these functions seem useful even outside of Bayesian work.\n\nCodelibrary(tidybayes)\n\nmedian_qi(samples$p_grid, .width = .5)\n\n      y  ymin  ymax .width .point .interval\n1 0.843 0.709 0.935    0.5 median        qi\n\n\n\nWith .width = .5, we indicated we wanted a quantile-based 50% interval, which was returned in the ymin and ymax columns. The tidybayes framework makes it easy to request multiple types of intervals. E.g., here we’ll request 50%, 80%, and 99% intervals.\n\n\nCodemedian_qi(samples$p_grid, .width = c(.5, .8, .99))\n\n      y     ymin  ymax .width .point .interval\n1 0.843 0.709000 0.935   0.50 median        qi\n2 0.843 0.570000 0.975   0.80 median        qi\n3 0.843 0.260985 0.999   0.99 median        qi\n\n\nThe .width column in the output indexed which line presented which interval. The value in the y column remained constant across rows. That’s because that column listed the measure of central tendency, the median in this case.\n\n\n\n3.2.3. Point estimates\n\nWe’ve been calling point estimates measures of central tendency. If we arrange() our d tibble in descending order by posterior, we’ll see the corresponding p_grid value for its MAP estimate.\n\n\nCoded %>%\n  arrange(desc(posterior))\n\n# A tibble: 1,001 × 4\n   p_grid prior likelihood posterior\n    <dbl> <dbl>      <dbl>     <dbl>\n 1  1         1      1         1    \n 2  0.999     1      0.997     0.997\n 3  0.998     1      0.994     0.994\n 4  0.997     1      0.991     0.991\n 5  0.996     1      0.988     0.988\n 6  0.995     1      0.985     0.985\n 7  0.994     1      0.982     0.982\n 8  0.993     1      0.979     0.979\n 9  0.992     1      0.976     0.976\n10  0.991     1      0.973     0.973\n# … with 991 more rows\n\n\nWe can get the mode with mode_hdi() or mode_qi().\n\nCodesamples %>% mode_hdi(p_grid)\n\n# A tibble: 1 × 6\n  p_grid .lower .upper .width .point .interval\n   <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1  0.955  0.477      1   0.95 mode   hdi      \n\nCodesamples %>% mode_qi(p_grid)\n\n# A tibble: 1 × 6\n  p_grid .lower .upper .width .point .interval\n   <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1  0.955  0.401  0.994   0.95 mode   qi       \n\n\n*DR: I’m confused. What is the ‘mode of the HDI” and why is it not place with the ’maximum posterior likelihood’?\n\nGiven the entire posterior distribution, what value should you report? ,… The Bayesian parameter estimate is precisely the entire posterior distribution … it is very common for scientists to report the parameter value with highest posterior probability, a maximum a posteriori (MAP) estimate Why not report the posterior mean or median? …\n\nLoss functions: very cool!\n\nOne principled way to go beyond using the entire posterior as the estimate is to choose a loss function. A loss function is a rule that tells you the cost associated with using any particular point estimate. While statisticians and game theorists have long been interested in loss functions, and how Bayesian inference supports them, scientists hardly ever use them explicitly. The key insight is that different loss functions imply different point estimates.\n\n\n[something something computing the value after each decision a policymaker coudl make] … repeating this calculation for every possible decision, using\n\nThree types of point estimates (Kurz code)\n\nCode(\n  point_estimates <-\n  bind_rows(samples %>% mean_qi(p_grid),\n            samples %>% median_qi(p_grid),\n            samples %>% mode_qi(p_grid)) %>% \n  select(p_grid, .point) %>% \n  # these last two columns will help us annotate  \n  mutate(x = p_grid + c(-.03, .03, -.03),\n         y = c(.1, .25, .4))\n)\n\n# A tibble: 3 × 4\n  p_grid .point     x     y\n   <dbl> <chr>  <dbl> <dbl>\n1  0.803 mean   0.773  0.1 \n2  0.843 median 0.873  0.25\n3  0.955 mode   0.925  0.4 \n\n\n\nKurz code: plotting 3 point estimates against posteriord %>% \n  ggplot(aes(x = p_grid)) +\n  geom_ribbon(aes(ymin = 0, ymax = posterior),\n              fill = \"grey75\") +\n  geom_vline(xintercept = point_estimates$p_grid) +\n  geom_text(data = point_estimates,\n            aes(x = x, y = y, label = .point),\n            angle = 90) +\n  labs(x = \"proportion of water (p)\",\n       y = \"density\") +\n  theme(panel.grid = element_blank())\n\n\n\n\nMcElreath:\n\nDifferent loss functions nominate different point estimates. The two most common examples are the absolute loss as above, which leads to the median as the point estimate, and the quadratic loss (d - p) 2 , which leads to the posterior mean (mean(samples)) as the point estimate\n\nKurz:\n\nLet \\(p\\) be the proportion of the Earth covered by water and \\(d\\) be our guess. If McElreath pays us $100 if we guess exactly right but subtracts money from the prize proportional to how far off we are, then our loss is proportional to \\(p - d\\). If we decide \\(d = .5\\), then our expected loss will be:\n\n\nlos if you guess halfd %>% \n  mutate(loss = posterior * abs(0.5 - p_grid)) %>% \n  summarise(`expected loss` = sum(loss))\n\n# A tibble: 1 × 1\n  `expected loss`\n            <dbl>\n1            78.4\n\n\nThis is a simple expected value calculation over the grid of values. The code above computes the ‘loss at each true value’ multiplied by the ‘posterior probability of that value’ and sums these for all possible values.\nNext we map (using purrr::map) and plot the loss function over essentially the full range of possible guesses.3\n\ncode: XXX#a function of a 'bayes output data frame' that does the above computation for a *particular decision* `our_d`\nmake_loss <- function(our_d){\n  d %>% \n  mutate(loss = posterior * abs(our_d - p_grid)) %>% \n  summarise(weighted_average_loss = sum(loss))\n}\n\n\n#mapping the above function over every element of the 'grid of possible choices' (which need to be in that data set of a particular format)\n\n(\n  l <-\n  d %>% \n  select(p_grid) %>% \n  rename(decision = p_grid) %>% #extract probability grid, label it as 'decision' to consider all possible decisions\n  mutate(weighted_average_loss = purrr::map(decision, make_loss)) %>%  \n    #wow, a map *inside* a mutate; not sure I understand the syntax of that. I guess `decision` on the previous line knows to refer to that particular column of the tibble we are 'currently working with', and creates a new column mapping each of these into the function `make_loss`\n    #for all \n  unnest() %>% #'purr' gives us a weird list, even within mutates? so you need to expand these lists I guess\n    arrange(weighted_average_loss)\n)\n\n# A tibble: 1,001 × 2\n   decision weighted_average_loss\n      <dbl>                 <dbl>\n 1    0.841                  31.9\n 2    0.842                  31.9\n 3    0.84                   31.9\n 4    0.843                  31.9\n 5    0.839                  31.9\n 6    0.844                  31.9\n 7    0.838                  31.9\n 8    0.845                  31.9\n 9    0.837                  31.9\n10    0.846                  31.9\n# … with 991 more rows\n\n\nNote I (DR) arranged the output tibble by the expected loss, from lowest to highest.\nPlotting this:\n\nCode# this will help us find the x and y coordinates for the minimum value\n\n# First find the minimum loss point, keeping both the guess and the loss\nmin_loss <-\n  l %>% \n  filter(weighted_average_loss == min(weighted_average_loss)) %>% \n  as.numeric()\n\n# the plot\nl %>%   \n  ggplot(aes(x = decision, y = weighted_average_loss)) + \n  #Kurz left out that second argument. I guess ggplot 'knows' to use the remaining column as the y feature?\n  geom_ribbon(aes(ymin = 0, ymax = weighted_average_loss), \n              fill = \"grey75\") +\n  # Adding lines at that minimum loss point\n  geom_vline(xintercept = min_loss[1], color = \"white\", linetype = 3) +\n  geom_hline(yintercept = min_loss[2], color = \"white\", linetype = 3) +\n  ylab(\"expected proportional loss\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n‘Absolute value loss functions \\(\\rightarrow\\) the median is the best prediction’\nWe saved the exact minimum value as min_loss[1], which is 0.841. Within sampling error, this is the posterior median as depicted by our samples.\n\nCodesamples %>% \n  summarise(posterior_median = median(p_grid))\n\n# A tibble: 1 × 1\n  posterior_median\n             <dbl>\n1            0.843\n\n\n‘Minimize a quadratic loss function by guessing the mean’\n4\n\nCode# DR: new  loss function\nmake_loss_q <- function(our_d) {\n  d %>% \n    mutate(loss = posterior * (our_d - p_grid)^2) %>% \n    summarise(weighted_average_loss = sum(loss))\n}\n\n# remake our `l` data (DR: maybe better to add this on to the previous one?)\nl_q <-\n  d %>% \n  select(p_grid) %>% \n  rename(decision = p_grid) %>% \n  mutate(weighted_average_loss = purrr::map(decision, make_loss_q)) %>% \n  unnest(weighted_average_loss)\n\n# new minimum loss coordinates\nmin_loss_q <-\n  l_q %>% \n  filter(weighted_average_loss == min(weighted_average_loss)) %>% \n  as.numeric()\n\n# update the plot\nl_q %>%   \n  ggplot(aes(x = decision)) +\n  geom_ribbon(aes(ymin = 0, ymax = weighted_average_loss),\n              fill = \"grey75\") +\n  geom_vline(xintercept = min_loss_q[1], color = \"white\", linetype = 3) +\n  geom_hline(yintercept = min_loss_q[2], color = \"white\", linetype = 3) +\n  ylab(\"expected proportional loss\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\nBased on quadratic loss \\((d - p)^2\\), the exact minimum value is 0.841. Within sampling error, this is the posterior mean of our samples.\n\nCodesamples %>% \n  summarise(posterior_mean = mean(p_grid))\n\n# A tibble: 1 × 1\n  posterior_mean\n           <dbl>\n1          0.803\n\n\n\nMore interesting points from McElreath\n\nTherefore the implied loss function is highly asymmetric, rising sharply as true wind speed exceeds our guess, but rising only slowly as true wind speed falls below our guess. In this context, the optimal point estimate would tend to be larger than posterior mean or median. Moreover, the real issue is whether or not to order an evacuation. Producing a point estimate of wind speed may not be necessary at all.\n\n\nIn this context, the optimal point estimate would tend to be larger than posterior mean or median. Moreover, the real issue is whether or not to order an evacuation. Producing a point estimate of wind speed may not be necessary at all.\n\nDR: Reinforcement learning could be better still?\n\nYou might argue that the decision to make is whether or not to accept an hypothesis. But the challenge then is to say what the relevant costs and benefits would be, in terms of the knowledge gained or lost\n\nDR: Yeah, ‘accept a hypothesis’ seems weird framed this way. Maybe it needs steelmanning.\n\nUsually it’s better to communicate as much as you can about the posterior distribution, as well as the data and the model itself, so that others can build upon your work"
  },
  {
    "objectID": "chapters/3_sampling_the_imaginary.html#sampling-to-simulate-prediction",
    "href": "chapters/3_sampling_the_imaginary.html#sampling-to-simulate-prediction",
    "title": "23  Ch 3. Sampling the Imaginary (posterior)",
    "section": "3.3. Sampling to simulate prediction",
    "text": "3.3. Sampling to simulate prediction\n\ncombine sampling of simulated observations, as in the previous section, with sampling parameters from the posterior distribution.\n\n\nbasic model checks\n\n\n\n\nPosterior predictive distribution\n\nCombining simulated observation distributions for all parameter values (not just the ten shown), each weighted by its posterior probability, produces the posterior predictive distribution\n\n\nThis distribution propagates uncertainty about parameter to uncertainty about prediction\n\n\nout. If instead you were to use only a single parameter value to compute implied predictions, say the most probable value at the peak of posterior distribution, you-d produce an overconfident distribution of predictions,\n\n\nw <- rbinom( 1e4 , size=9 , prob=samples ) The symbol samples above is the same list of random samples from the posterior distribution that you-ve used in previous sections. For each sampled value, a random binomial observation is generated\n\n\nNow the simulated predictions appear less consistent with the data, as the majority of simulated observations have fewer switches than were observed in the actual sample. This is consistent with lack of independence between tosses of the globe, in which each toss is negatively correlated with the last. Does this mean that the model is bad? That depends\n\n\nIn the long run, even the wrong model we’ve used throughout the chapter converge on the correct proportion. But it will do so more slowly than the posterior distribution may lead us to believe.\n\n\nposterior predictive checks\n\n\n3.3.2.2. Is the model adequate?\n\nDR: I.e., Does the model and its estimates predict a reasonable (sampled) distribution\nMultilevel models\n\nmultilevel model. Multilevel models-also known as hierarchical, random effects, varying effects, or mixed effects models\n\n\nCross-validation and information criteria measure overfitting risk and help us to recognize it. Multilevel models actually do something about it. What they do is exploit an amazing trick known as partial pooling\n\nModel comparison and prediction.\n\nBayesian data analysis provides a way for models to learn from data. But when there is more than one plausible model-and in most mature fields there should be-how should we choose among them? One answer is to prefer models that make good predictions. This answer creates a lot of new questions, since knowing which model will make the best predictions seems to require knowing the future. We’ll look at two related tools, neither of which knows the future: cross-validation and information criteria"
  },
  {
    "objectID": "chapters/4_geocentric_linear_models.html",
    "href": "chapters/4_geocentric_linear_models.html",
    "title": "\n24  Ch 4. Geocentric (linear) Models\n",
    "section": "",
    "text": "introduces linear regression as a Bayesian procedure."
  },
  {
    "objectID": "chapters/4_geocentric_linear_models.html#why-normal-distributions-are-normal",
    "href": "chapters/4_geocentric_linear_models.html#why-normal-distributions-are-normal",
    "title": "\n24  Ch 4. Geocentric (linear) Models\n",
    "section": "\n24.1 Why normal distributions are normal",
    "text": "24.1 Why normal distributions are normal\n\nMany natural (and unnatural) processes have much heavier tails … A real and important example is financial time series\n\nAfter laying out his soccer field coin toss shuffle premise, McElreath wrote:\n\nIt’s hard to say where any individual person will end up, but you can say with great confidence what the collection of positions will be. The distances will be distributed in approximately normal, or Gaussian, fashion. This is true even though the underlying distribution is binomial. It does this because there are so many more possible ways to realize a sequence of left-right steps that sums to zero. There are slightly fewer ways to realize a sequence that ends up one step left or right of zero, and so on, with the number of possible sequences declining in the characteristic bell curve of the normal distribution. (p. 72)\n\nNormal by addition.\n\ncoding football steps# we set the seed to make the results of `runif()` reproducible.\nset.seed(4)\n\npos <-\n  # make data with 100 people, 16 steps each with a starting point of `step == 0` (i.e., 17 rows per person)\n  crossing(person = 1:100,\n           step   = 0:16) %>%\n  #DR: 'crossing is a great shortcut to 'create all combinations''\n\n  # for all steps above `step == 0` simulate a `deviation`\n  mutate(deviation = map_dbl( #DR: `map_dbl` to make it a 'vector of numbers' rather than a list\n    step, #for all 16 step entries\n\n    ~if_else(. == 0, 0, runif(1, -1, 1)))) %>% #defines a function with '~', 0 for step 0, otherwise a uniform distribution step length each time\n\n  # after grouping by `person`, compute the cumulative sum of the deviations, then `ungroup()`\n  group_by(person) %>%\n  mutate(position = cumsum(deviation)) %>% #cumsum is great\n  ungroup()\n\n\n\nCodeglimpse(pos)\n\nRows: 1,700\nColumns: 4\n$ person    <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ step      <int> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 0, 1, 2, 3, 4, 5, 6, 7…\n$ deviation <dbl> 0.00000000, -0.98210841, -0.41252078, -0.44525008, 0.62714843, -0.47914446, 0.44…\n$ position  <dbl> 0.0000000, -0.9821084, -1.3946292, -1.8398793, -1.2127308, -1.6918753, -1.243063…\n\nCodeprecis(pos)\n\n                 mean        sd       5.5%      94.5%     histogram\nperson    50.50000000 28.874564  6.0000000 95.0000000    ▇▇▇▇▇▇▇▇▇▇\nstep       8.00000000  4.900421  0.0000000 16.0000000      ▇▅▅▅▅▅▅▅\ndeviation -0.02345358  0.560043 -0.8949205  0.8721156    ▅▅▅▃▇▅▅▃▃▃\nposition  -0.16368313  1.609085 -2.6457540  2.3513754 ▁▁▁▁▂▃▇▅▃▁▁▁▁\n\n\n\nHere’s the actual plot code.\n\n\nCode(\n  p1 <- ggplot(data = pos,\n       aes(x = step, y = position, group = person)) + #'where am I (vertical) at each step (horizontal) ... not sure what 'group' does here\n  geom_vline(xintercept = c(4, 8, 16), linetype = 2) + #add vertical lines for x intercepts at steps 4 8 and 16\n  geom_line(aes(color = person < 4, alpha  = person < 4)) + #the main lines of interest\n    #focusing on 4 specific cases\n  scale_color_manual(values = c(\"skyblue4\", \"black\")) +\n  scale_alpha_manual(values = c(1/7, 1)) +\n  scale_x_continuous(\"step number\", breaks = c(0, 4, 8, 12, 16)) + # ticks and labels on the bottom, light gridlines\n  theme(legend.position = \"none\")\n)\n\n\n\n\n\n\n\n\n\n\nDR question about above code\n\n\n\n\n\n\nprecisely what does ‘group’ do in the above?\n\n\n\n\nCode for plotting all random walks on soccer field, steps, and densities\nPlots at 4 and 8 steps:\n\nCode# Figure 4.2.a.\n# Figure 4.2.a.\np1 <-\n  pos %>%\n  filter(step == 4) %>%\n  ggplot(aes(x = position)) +\n  geom_line(stat = \"density\", color = \"dodgerblue1\") +\n  coord_cartesian(xlim = c(-6, 6)) +\n  labs(title = \"4 steps\")\n\n# Figure 4.2.b.\np2 <-\n  pos %>%\n  filter(step == 8) %>%\n  ggplot(aes(x = position)) +\n  geom_density(color = \"dodgerblue2\") +\n  coord_cartesian(xlim = c(-6, 6)) +\n  labs(title = \"8 steps\")\n\n\nGet the SD at 16 steps for plotting the functional normal distribution to compare it to.\n\n\n\n\n\n\nDR question: Isn’t there an analytical formula we could use here for the SD instead?\n\n\n\n\n\n\n\n\n\n\nCode# this is an intermediary step to get an SD value\npos_sd <- pos %>%\n  filter(step == 16) %>%\n  summarise(sd = sd(position))\n\n\nPlot at 16 steps, overlay normal distribution (DR – I put in the SD as an object from above)\n\nCode# Figure 4.2.c.\n\n# Figure 4.2.c.\np3 <-\n  pos %>%\n  filter(step == 16) %>%\n  ggplot(aes(x = position)) +\n  stat_function(fun = dnorm,\n                args = list(mean = 0, sd = pos_sd[[1,1]]),\n                linetype = 2) +  # 2.180408 came from the previous code block\n  geom_density(color = \"transparent\", fill = \"dodgerblue3\", alpha = 1/2) +\n  coord_cartesian(xlim = c(-6, 6)) +\n  labs(title = \"16 steps\",\n       y = \"density\")\n\n\n\n# combine the ggplots\np1 | p2 | p3\n\n\n\n\n\n\n\n\n\n\nDR question (code): Better way to code ‘extract the sd as a number’ above?\n\n\n\n\n\n\n\n\n\n\nWhile we were at it, we explored a few ways to express densities. The main action was with the geom_line(), geom_density(), and stat_function() functions, respectively.\n\nDR: geom_line(stat = \"density\"... might be the same as geom_density, stat_function is mainly for analytical densities?\nBut why?\n\nAny process that ads together random values from the same distribution converges to a normal. But it’s not easy to grasp why addition should result in a bell curve of sums. Here’s a conceptual way to think of the process. Whatever the average value of the source distribution, each sample from it can be thought of as a fluctuation from the average value. When we begin to add these fluctuations together, they also begin to cancel one another out. A large positive fluctuation will cancel a large negative one. The more terms in the sum, the more chances for each fluctuation to be canceled by another, or by a series of smaller ones in the opposite direction. So eventually the most likely sum, in the sense that there are the most ways to realize it, will be a sum in which every fluctuation is canceled by another, a sum of zero (relative to the mean). (pp. 73–74)\n\nNormal by multiplication\n\nsmall effects that multiply together are approximately additive, and so they also tend to stabilize on Gaussian distributions\n\n\nLarge deviates that are multiplied together do not produce Gaussian distributions, but they do tend to produce Gaussian distributions on the log scale\n\nSkipped coding this for now\nUsing Gaussian distributions (“but why?”)\nOntological justification.\nThe Gaussian is\n\na widespread pattern, appearing again and again at different scales and in different domains. Measurement errors, variations in growth, and the velocities of molecules all tend towards Gaussian distributions. These processes do this because at their heart, these processes add together fluctuations. And repeatedly adding finite fluctuations results in a distribution of sums that have shed all information about the underlying process, aside from mean and spread. One consequence of this is that statistical models based on Gaussian distributions cannot reliably identify micro-process. (p. 75)\n\n~I.e., we cannot ‘prove what we assumed’.\n\nKurz: But they can still be useful.\n\n\n24.1.0.1 Epistemological justification.\n\nAnother route to justifying the Gaussian as our choice of skeleton, and a route that will help us appreciate later why it is often a poor choice, is that it represents a particular state of ignorance. When all we know or are willing to say about a distribution of measures (measures are continuous values on the real number line) is their mean and variance, then the Gaussian distribution arises as the most consistent with our assumptions.\nThat is to say that the Gaussian distribution is the most natural expression of our state of ignorance, because if all we are willing to assume is that a measure has finite variance, the Gaussian distribution is the shape that can be realized in the largest number of ways and does not introduce any new assumptions. It is the least surprising and least informative assumption to make. In this way, the Gaussian is the distribution most consistent with our assumptions… If you don’t think the distribution should be Gaussian, then that implies that you know something else that you should tell your golem about, something that would improve inference. (pp. 75–76)\n\n\n\n\n\n\n\nDR question – can we justify this in our own words\n\n\n\n\n\nE.g., to a skeptical audience or ‘client’?\n\n\n\nFrom McElreath:\n\nBy the ontological justification, the world is full of Gaussian distributions, approximately.\n\n\nBy the epistemological justification, the Gaussian represents a particular state of ignorance. When all we know or are willing to say about a distribution of measures (measures are continuous values on the real number line) is their mean and variance, then the Gaussian distribution arises as the most consistent with our assumptions maximum entropy?\n\n\nif all we are willing to assume is that a measure has finite variance, the Gaussian distribution is the shape that can be realized in the largest number of ways\n\n\n24.1.0.2 Overthinking: Gaussian distribution.\n(Kurz quotes below)\nLet \\(y\\) be the criterion (DR: why ‘criterion’?), \\(\\mu\\) be the mean, and \\(\\sigma\\) be the standard deviation. Then the probability density of some Gaussian value \\(y\\) is\n\\[p(y|\\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\Bigg (- \\frac{(y - \\mu)^2}{2 \\sigma^2} \\Bigg).\\]\nWhy not demystify that monster with a little R code? For simplicity, we’ll look at \\(p(y)\\) over a series of \\(y\\) values ranging from -4 to 4, holding \\(\\mu = 0\\) and \\(\\sigma = 1\\). Then we’ll plot.\n\nCode# define our input values\n\ntibble(y     = seq(from = -4, to = 4, by = .1),\n       mu    = 0,\n       sigma = 1) %>%\n  # compute p(y) using a hand-made gaussian likelihood\n  mutate(\n    p_y = (1 / sqrt(2 * pi * sigma^2)) * exp(-(y - mu)^2 / (2 * sigma^2))) %>%\n\n  # plot!\n  ggplot(aes(x = y, y = p_y)) +\n  geom_line() +\n  ylab(expression(italic(p)(italic(\"y|\")*mu==0*\",\"~sigma==1)))\n\n\n\n\n\nYou get the same results if you switch out that mutate line with mutate(p_y = dnorm(y)) %>%. To learn more, execute ?dnorm.\n\nDR: how does executing it demystify it?"
  },
  {
    "objectID": "chapters/4_geocentric_linear_models.html#a-language-for-describing-models-4.2",
    "href": "chapters/4_geocentric_linear_models.html#a-language-for-describing-models-4.2",
    "title": "\n24  Ch 4. Geocentric (linear) Models\n",
    "section": "\n24.2 A language for describing models (4.2)",
    "text": "24.2 A language for describing models (4.2)\nFor example:\n\\[\\begin{align*}\n\\text{criterion}_i & \\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i  & = \\beta \\times \\text{predictor}_i \\\\\n\\beta  & \\sim \\text{Normal}(0, 10) \\\\\n\\sigma & \\sim \\text{HalfCauchy}(0, 1).\n\\end{align*}\\]\n\nThe combination of variables and their probability distributions defines a joint generative model that can be used both to simulate hypothetical observations as well as analyze real ones.\n\nDR: No ‘error term’ as in standard econometrics statement\n\nWe no longer have to remember seemingly arbitrary lists of bizarre conditions like homoscedasticity (constant variance), because we can just read these conditions from the model definitions. We specify distributions rather than error terms and conditions\n\nDR: Note these specific distributions are ‘stronger assumptions’, which people may argue are harder to justify\nRe-describing the globe tossing model (in these terms)\nDR: We previously just described it in words iirc\nFor the globe tossing model, the probability \\(p\\) of a count of water \\(w\\) based on \\(n\\) trials was\n\\[\\begin{align*}\nw & \\sim \\text{Binomial}(n, p) \\\\\np & \\sim \\text{Uniform}(0, 1).\n\\end{align*}\\]\n\n‘probability p and data drawn’ / ‘probability data drawn’\nWell, it’s what we compute in the grid approximation, for each probability.\nRemember the denominator is the same for all values of p.\n\n\n\n\n\n\nFrom model description to Bayes theorem\n\n\n\n\n\n\nCode# how many `p_grid` points would you like?\nn_points <- 100\n\nd <-\n  tibble(p_grid = seq(from = 0, to = 1, length.out = n_points),\n         w      = 6,\n         n      = 9) %>%\n  mutate(prior      = dunif(p_grid, 0, 1),\n         likelihood = dbinom(w, n, p_grid)) %>%\n  mutate(posterior = likelihood * prior / sum(likelihood * prior))\n\nhead(d)\n\n# A tibble: 6 × 6\n  p_grid     w     n prior likelihood posterior\n   <dbl> <dbl> <dbl> <dbl>      <dbl>     <dbl>\n1 0          6     9     1   0         0       \n2 0.0101     6     9     1   8.65e-11  8.74e-12\n3 0.0202     6     9     1   5.37e- 9  5.43e-10\n4 0.0303     6     9     1   5.93e- 8  5.99e- 9\n5 0.0404     6     9     1   3.23e- 7  3.26e- 8\n6 0.0505     6     9     1   1.19e- 6  1.21e- 7\n\n\nIn case you were curious, here’s what they look like.\n\nCoded %>%\n  select(-w, -n) %>%\n  gather(key, value, -p_grid) %>%\n  # this line allows us to dictate the order the panels will appear in\n  mutate(key = factor(key, levels = c(\"prior\", \"likelihood\", \"posterior\"))) %>%\n\n  ggplot(aes(x = p_grid, ymin = 0, ymax = value, fill = key)) +\n  geom_ribbon() +\n  scale_fill_manual(values = c(\"blue\", \"red\", \"purple\")) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(legend.position = \"none\") +\n  facet_wrap(~key, scales = \"free\")\n\n\n\n\n\nThe posterior is a combination of the prior and the likelihood. When the prior is flat across the parameter space, the posterior is just the likelihood re-expressed as a probability. As we go along, you’ll see that we almost never use flat priors in practice."
  },
  {
    "objectID": "chapters/4_geocentric_linear_models.html#a-gaussian-model-of-height-4.3",
    "href": "chapters/4_geocentric_linear_models.html#a-gaussian-model-of-height-4.3",
    "title": "\n24  Ch 4. Geocentric (linear) Models\n",
    "section": "A Gaussian model of height (4.3)",
    "text": "A Gaussian model of height (4.3)\n\n\n\n\n\n\nWillem Sleeger’s “Figuring out Bayesian statistics” covers much of this same ground.\n\n\n\n\n\nHis blog entry is better formatted and talks through some parts of the intuition more. You may find it more useful than the present notes. I incorporate some of it below (with acknowledgement)\n\n\n\n\n… single measurement variable to model as a Gaussian distribution. There will be two parameters describing the distribution’s shape, the mean - and the standard deviation\n\n\nthe ‘estimate’ here will be the entire posterior distribution, not any point within it\n\n\nAnd as a result, the posterior distribution will be a distribution of Gaussian distributions, or of the parameters of these\n\n\nThere are an infinite number of possible Gaussian distributions. Some have small means. Others have large means. Some are wide, with a large \\(\\sigma\\). Others are narrow. We want our Bayesian machine to consider every possible distribution, each defined by a combination of \\(\\mu\\) and \\(\\sigma\\), and rank them by posterior plausibility. (p. 79)\n\nThe data (height)\n\nLet’s get the Howell (2000, 2010) data from McElreath’s rethinking package.\n\n\nCodelibrary(rethinking)\ndata(Howell1)\nd <- Howell1\n\n\n\n\n\n\n\n\nCode notes on brms\n\n\n\n\n\n\n(Kurz) Here we open our main statistical package, Bürkner’s brms. But before we do, we’ll want to detach the rethinking package. R will not allow users to use a function from one package that shares the same name as a different function from another package if both packages are open at the same time. The rethinking and brms packages are designed for similar purposes and, unsurprisingly, overlap in the names of their functions. To prevent problems, it is a good idea to make sure rethinking is detached before using brms. To learn more on the topic, see this R-bloggers post.\n\n\n\n\n\nCoderm(Howell1)\ndetach(package:rethinking, unload = T)\nlibrary(brms)\n\n\nGo ahead and investigate the data with str(), the tidyverse analogue for which is glimpse().\n\nCoded %>%\n  str()\n\n'data.frame':   544 obs. of  4 variables:\n $ height: num  152 140 137 157 145 ...\n $ weight: num  47.8 36.5 31.9 53 41.3 ...\n $ age   : num  63 63 65 41 51 35 32 27 19 54 ...\n $ male  : int  1 0 0 1 0 1 0 1 0 1 ...\n\n\nHere are the height values\n\nCoded %>%\n  select(height) %>%\n  head()\n\n   height\n1 151.765\n2 139.700\n3 136.525\n4 156.845\n5 145.415\n6 163.830\n\n\nWe can use filter() to make an adults-only data frame.\n\nCoded2 <-\n  d %>%\n  filter(age >= 18)\n\n\nThere are a lot of ways we can make sure our d2 has 352 rows. Here’s one.\n\nCoded2 %>%\n  count()\n\n    n\n1 352\n\n\nThe model\n\nas mentioned earlier in this chapter, the empirical distribution needn’t be actually Gaussian in order to justify using a Gaussian probability distribution.\n\n\n\n\n\n\n\nDR question: Why?\n\n\n\n\n\nI think this is because we are considering means?\n\n\n\n\n\n\n\n\n\nOn iid as an ‘epistemological assumption’\n\n\n\n\n\nThe i.i.d. assumption is about how the golem represents its uncertainty. It is an epistemological assumption. It is not a physical assumption about the world, an ontological one. E. T. Jaynes (1922-1998) called this the mind projection fallacy, the mistake of confusing epistemological claims with ontological claims.71 The point isnt that epistemology trumps reality, but that in ignorance of such correlations the best distribution may be i.i.d.72\n\n\n\nThe likelihood for our model is\n\\[h_i \\sim \\operatorname{Normal}(\\mu, \\sigma),\\]\nour \\(\\mu\\) prior will be\n\\[\\mu \\sim \\operatorname{Normal}(178, 20),\\]\nand our prior for \\(\\sigma\\) will be\n\\[\\sigma \\sim \\operatorname{Uniform}(0, 50).\\]\nHere’s the shape of the prior for \\(\\mu\\) in \\(N(178, 20)\\).\n\nCodeggplot(data = tibble(x = seq(from = 100, to = 250, by = .1)),\n       aes(x = x, y = dnorm(x, mean = 178, sd = 20))) +\n  geom_line() +\n  ylab(\"density\")\n\n\n\n\n\n\n\n\n\n\nWhy these particular values?\n\n\n\n\n\n\nThe prior for \\(\\mu\\) is a broad Gaussian prior, centered on 178 cm, with 95% of probability between 178 - 40 cm. Why 178 cm? Your author is 178 cm tall. And the range from 138 cm to 218 cm encompasses a huge range of plausible mean heights for human populations. So domain-specific information has gone into this prior.\n\n\n\n\n\n\n\n\n\n\nDR question: why is 0 in the distribution for the std. error?\n\n\n\n\n\nI guess this is fixed later?\n\n\n\nGelman on priors\n\n\n\n\n\n\nbrms’s get_prior\n\n\n\n\n\nAt this point Sleegers’ notes consider the brms function get_prior for the mean only model height ~ 1. get_prior peeks at the data to consider an (?appropriate) prior.\n\nCodebrms::get_prior(height ~ 1, data = d2)\n\n                    prior     class coef group resp dpar nlpar bound  source\n student_t(3, 154.3, 8.5) Intercept                                  default\n     student_t(3, 0, 8.5)     sigma                                  default\n\n\nIt’s not clear to me what get_prior is doing here, or what its logic is. It would seem to be using the data to suggest priors, which McElreath seems to be against (but the ‘empirical bayes’ people seem to like). What exactly is the justification for doing this? the people who designed this package must have had something in mind.\nAnyways, get, suggesting_prior suggests specific student=t distributions for the intercept (mean) and for sigma. These t-distributions have three parameters, one of which (the ‘degrees of freedom’) affects the skewness/fatness of tails relative to the normal distribution.\n\n\n\nWhy simulate the prior probability distribution?\n\nOnce you’ve chosen priors for \\(h\\), \\(\\mu\\) and \\(\\sigma\\), these imply a joint prior distribution of individual heights. By simulating from this distribution, you can see what your choices imply about observable height. This helps you diagnose bad choices.\n\n\n… it can be quite hard to anticipate how priors influence the observable variables\n\nThe prior doesn’t affect the results much if you have a reasonably diffuse prior and lots of data. However:\n\nThere are plenty of inference problems for which the data alone are not sufficient, no matter how numerous. Bayes lets us proceed in these cases. But only if we use our scientific knowledge to construct sensible priors. Using scientific knowledge to build priors is not cheating. The important thing is that your prior not be based on the values in the data, but only on what you know about the data before you see it.\n\n\nso to get the joint likelihood across all the data, we have to compute the probability for each \\(h_i\\) [observed height] and then multiply all these likelihoods together\n\n\nBelow: ggplot of the prior for \\(\\sigma\\), a uniform distribution with a minimum value of 0 and a maximum value of 50.1\n\nCodetibble(x = seq(from = -10, to = 60, by = .1)) %>% #just the grid of sigma  space to plot over\n  ggplot(aes(x = x, y = dunif(x, min = 0, max = 50))) + #I can 'create the y variable' within the ggplot\n  geom_line() +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n\n\n\n\nWe can simulate from both priors at once to get a prior probability distribution of heights.\n\nCoden <- 1e4\n\nset.seed(4)\n\ntibble(sample_mu    = rnorm(n, mean = 178,       sd  = 20), #10k draws from normal for mean height\n       sample_sigma = runif(n, min  = 0,         max = 50)) %>%  #10k draws from uniform for sd of height\n  mutate(x = rnorm(n, mean = sample_mu, sd  = sample_sigma)) %>%\n  #10k draws of height from normal with mean and sd  from above in each case?\n  ggplot(aes(x = x)) +\n  geom_density(fill = \"black\", size = 0) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(subtitle = expression(Prior~predictive~distribution~\"for\"~italic(h[i])),\n       x = NULL) +\n  theme(panel.grid = element_blank())\n\n\n\n\nAs McElreath wrote, we’ve made a “vaguely bell-shaped density with thick tails. It is the expected distribution of heights, averaged over the prior” (p. 83).\nGrid approximation of the posterior distribution\n\nAll mean and sd height values to considern <- 200\n\nd_grid <-\n  # we'll accomplish with `tidyr::crossing()` what McElreath did with base R `expand.grid()`\n  crossing(mu    = seq(from = 140, to = 160, length.out = n),\n           sigma = seq(from = 4,   to = 9,   length.out = n))\n\nglimpse(d_grid)\n\nRows: 40,000\nColumns: 2\n$ mu    <dbl> 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140,…\n$ sigma <dbl> 4.000000, 4.025126, 4.050251, 4.075377, 4.100503, 4.125628, 4.150754, 4.175879, 4.20…\n\n\nd_grid contains every combination of mu and sigma across their specified values. Instead of base R sapply(), we’ll do the computations by making a custom function which we’ll plug into purrr::map2().\n\nfunction computes & sums log of normal density for each value of d2$height, given a particular mu and sigmagrid_function <- function(mu, sigma) {\n  dnorm(d2$height, mean = mu, sd = sigma, log = T) %>% \n    sum()\n}\n\n\n\nmaps the log llhd of the data (and params) for each combination in d_grid, converts to a relative probabilityd_grid <-\n  d_grid %>% \n  mutate(log_likelihood = map2(mu, sigma, grid_function)) %>% #maps \n  unnest(log_likelihood) %>% \n  mutate(prior_mu    = dnorm(mu,    mean = 178, sd  = 20, log = T),\n         prior_sigma = dunif(sigma, min  = 0,   max = 50, log = T)) %>% \n  mutate(product = log_likelihood + prior_mu + prior_sigma,\n    max_product =max(product)) %>%\n  mutate(probability = exp(product - max(product)) # exponentiate the log likelihood to get the probability; but the individual probability densities are meaningless. For computational reasons (I think) we state these relative to the max value\n    )\n  \nd_grid %>% arrange(-probability) %>% \n  head() %>% kable(cap = \n  \"highest prob. rows\") %>% kable_styling()\n\n\n\nhighest prob. rows\n \n mu \n    sigma \n    log_likelihood \n    prior_mu \n    prior_sigma \n    product \n    max_product \n    probability \n  \n\n\n 154.5729 \n    7.743719 \n    -1219.408 \n    -4.600709 \n    -3.912023 \n    -1227.920 \n    -1227.92 \n    1.0000000 \n  \n\n 154.5729 \n    7.718593 \n    -1219.408 \n    -4.600709 \n    -3.912023 \n    -1227.921 \n    -1227.92 \n    0.9999332 \n  \n\n 154.5729 \n    7.768844 \n    -1219.415 \n    -4.600709 \n    -3.912023 \n    -1227.928 \n    -1227.92 \n    0.9927172 \n  \n\n 154.5729 \n    7.693467 \n    -1219.415 \n    -4.600709 \n    -3.912023 \n    -1227.928 \n    -1227.92 \n    0.9923983 \n  \n\n 154.6734 \n    7.743719 \n    -1219.423 \n    -4.594836 \n    -3.912023 \n    -1227.930 \n    -1227.92 \n    0.9905659 \n  \n\n 154.6734 \n    7.718593 \n    -1219.423 \n    -4.594836 \n    -3.912023 \n    -1227.930 \n    -1227.92 \n    0.9904006 \n  \n\n\n\nmaps the log llhd of the data (and params) for each combination in d_grid, converts to a relative probabilityd_grid %>% arrange(-probability) %>%  slice_sample(n = 10) %>% kable(cap = \n  \"random rows\") %>% kable_styling()\n\n\n\nrandom rows\n \n mu \n    sigma \n    log_likelihood \n    prior_mu \n    prior_sigma \n    product \n    max_product \n    probability \n  \n\n\n 141.2060 \n    7.793970 \n    -1738.975 \n    -5.606916 \n    -3.912023 \n    -1748.494 \n    -1227.92 \n    0.0000000 \n  \n\n 156.0804 \n    5.306533 \n    -1298.278 \n    -4.515257 \n    -3.912023 \n    -1306.705 \n    -1227.92 \n    0.0000000 \n  \n\n 145.3266 \n    4.653266 \n    -2049.097 \n    -5.249107 \n    -3.912023 \n    -2058.258 \n    -1227.92 \n    0.0000000 \n  \n\n 146.2312 \n    4.603015 \n    -1938.764 \n    -5.176245 \n    -3.912023 \n    -1947.852 \n    -1227.92 \n    0.0000000 \n  \n\n 160.0000 \n    4.804020 \n    -1554.369 \n    -4.319671 \n    -3.912023 \n    -1562.601 \n    -1227.92 \n    0.0000000 \n  \n\n 140.8040 \n    6.060301 \n    -2155.813 \n    -5.644097 \n    -3.912023 \n    -2165.369 \n    -1227.92 \n    0.0000000 \n  \n\n 155.7789 \n    8.547739 \n    -1226.091 \n    -4.531893 \n    -3.912023 \n    -1234.535 \n    -1227.92 \n    0.0013412 \n  \n\n 147.8392 \n    5.231156 \n    -1584.059 \n    -5.051763 \n    -3.912023 \n    -1593.023 \n    -1227.92 \n    0.0000000 \n  \n\n 159.0955 \n    6.512563 \n    -1315.027 \n    -4.361397 \n    -3.912023 \n    -1323.301 \n    -1227.92 \n    0.0000000 \n  \n\n 149.1457 \n    7.115578 \n    -1325.270 \n    -4.955382 \n    -3.912023 \n    -1334.138 \n    -1227.92 \n    0.0000000 \n  \n\n\n\n\nAbove, we compute the likelihood of each data point given each combination of parameters under consideration, and multiply these together (or ‘add the log probabilities’). We compute the log probability of these parameters, and add these to the probability of the data under these parameters to get the joint (log) likelihood. Erexponentiate the log likelihood to get the probability. However, with a continuous probabilityindividual probability density values are meaningless; only the relative values matter. For computational reasons (I think) we state each of these relative to the max value of the probabilities.\nAbove, we present the ‘highest probability’ values as well as some randomly chosen values.\nFollowing Kurz, we can plot ‘where the model thinks the most likely values of our parameters lie’, e.g., in a heatmap plot:\n\nHeatmap:d_grid %>% \n  ggplot(aes(x = mu, y = sigma)) + \n  geom_raster(aes(fill = probability),\n              interpolate = T) +\n  scale_fill_viridis_c(option = \"A\") +\n  labs(x = expression(mu),\n       y = expression(sigma)) +\n  coord_cartesian(xlim = range(d_grid$mu)*.7+50,\n                  ylim = range(d_grid$sigma)*.7 + 3.5) +\n  theme(panel.grid = element_blank())\n\n\n\n\nNote that the posterior distribution of parameters need not be a circle or even symmetric. They may be correlated. Certain parameters may be ‘jointly more’ or ‘jointly less’ likely, as the process that generated the data we see may (e.g.) only tend be likely to come from high mean values when the standard deviation tends to be large.\nSampling from the posterior (3.4)\nMcElreath:\n\nsince there are two parameters, and we want to sample combinations of them, we first randomly sample row numbers in post in proportion to the values in post$prob. Then we pull out the parameter values on those randomly sampled rows draw from grid in proportion to calculated likelihoods.\n\n\nThe jargon “marginal” here means “averaging over the other parameters.”\n\n\nAnd this is quite typical. As sample size increases, posterior densities approach the normal distribution.\n\nDR: All posterior densities? When and why?\nKurz:\n\nWe can use dplyr::sample_n() to sample rows, with replacement, from d_grid.\n\nNote the ‘weight=probability’ argument\n\nCodeset.seed(4)\nd_grid_samples <-\n  d_grid %>%\n  sample_n(size = 1e4, replace = T, weight = probability)\n\nd_grid_samples %>%\n  ggplot(aes(x = mu, y = sigma)) +\n  geom_point(size = .9, alpha = 1/15) +\n  scale_fill_viridis_c() +\n  labs(x = expression(mu[samples]),\n       y = expression(sigma[samples])) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\nWe can use gather() and then facet_warp() to plot the densities for both mu and sigma at once.\n\n\nCoded_grid_samples %>%\n  select(mu, sigma) %>%\n  gather() %>% #'gather' seems to make it longer, going from mu and sigma being the colun names to making one 'value'  row for each mu and one for each sigma\n  ggplot(aes(x = value)) +\n  geom_density(fill = \"grey33\", size = 0) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~key, scales = \"free\")\n\n\n\n\n\nWe’ll use the tidybayes package to compute their posterior modes and 95% HDIs.\n\n\nCodelibrary(tidybayes)\n\nd_grid_samples %>%\n  select(mu, sigma) %>%\n  gather() %>%\n  group_by(key) %>%\n  mode_hdi(value)\n\n# A tibble: 2 × 7\n  key    value .lower .upper .width .point .interval\n  <chr>  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1 mu    155.   154.   155.     0.95 mode   hdi      \n2 sigma   7.82   7.14   8.30   0.95 mode   hdi      \n\n\nDR: I’m not entirely sure what value.lower and upper refer to here. Is it a 95% HDI or something?\n\nLet’s say you wanted their posterior medians and 50% quantile-based intervals, instead. Just switch out the last line for median_qi(value, .width = .5):\n\n\nCoded_grid_samples %>%\n  select(mu, sigma) %>%\n  gather() %>%\n  group_by(key) %>%\n  median_qi(value, .width = .5)\n\n# A tibble: 2 × 7\n  key    value .lower .upper .width .point .interval\n  <chr>  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1 mu    155.   154.   155.      0.5 median qi       \n2 sigma   7.77   7.57   7.97    0.5 median qi       \n\n\nis the standard deviation - that causes problems. So if you care about –often people do not-you do need to be careful of abusing the quadratic approximation bc quap essentially plus a normal distribution\nFitting the model with brm() (Kurz)\n\nbut will jump straight to the primary brms modeling function, brm().\n\n2\n\nCodeb4.1 <-\n  brm(data = d2,\n      family = gaussian,\n      height ~ 1,\n      prior = c(prior(normal(178, 20), class = Intercept),\n                prior(uniform(0, 50), class = sigma)),\n      iter = 31000, warmup = 30000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.01\")\n\n\nMcElreath’s uniform prior for \\(\\sigma\\) was rough on brms. It took an unusually-large number of warmup iterations before the chains sampled properly. As McElreath covered in [Chapter 8][Estimation.], Hamiltonian Monte Carlo (HMC) tends to work better when you default to a half Cauchy for \\(\\sigma\\). We can do that like this.\n\nCode#create 'fits' folder to store these fits, for some reason\ndir.create(file.path(\"fits\"))\n\nptm <- proc.time()\nb4.1_hc <-\n  brm(data = d2, family = gaussian,\n      height ~ 1,\n      prior = c(prior(normal(178, 20), class = Intercept),\n                # the magic lives here\n                prior(cauchy(0, 1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.01_hc\")\n(timer_b41 <- proc.time() - ptm)\n\n   user  system elapsed \n  0.174   0.001   0.176 \n\n\nAbove: MCMC I think3\nKurz goes into a discussion of ‘inspecting the HMC chains here’. But so far we have no idea what this means. Takeaway from our discussion – they should not show a trend or a cyclical pattern by the end.\nHere’s how to get the model summary of our brm() object.\n\nCodeprint(b4.1_hc, prob=.89)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 \n   Data: d2 (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.61      0.41   153.96   155.27 1.00     3833     2967\n\nFamily Specific Parameters: \n      Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS\nsigma     7.74      0.29     7.29     8.23 1.00     3511     2853\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nDR_question: Above – is the CI for the ‘centered interval’ or the ‘highest density’ one?\n\n\n\n\n\n\nStan-like\n\n\n\n\n\nYou can also get a Stan-like summary like this.\n\nCodeb4.1_hc$fit\n\nInference for Stan model: d4b86a975924e0ede774b15040e8835e.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n                mean se_mean   sd     2.5%      25%      50%      75%    97.5% n_eff Rhat\nb_Intercept   154.61    0.01 0.41   153.81   154.34   154.61   154.89   155.42  3796    1\nsigma           7.74    0.00 0.29     7.20     7.54     7.73     7.93     8.35  3553    1\nlp__        -1227.52    0.02 1.00 -1230.16 -1227.91 -1227.22 -1226.79 -1226.54  1984    1\n\nSamples were drawn using NUTS(diag_e) at Fri Nov 25 08:58:33 2022.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\n\n\n\n\nCodesummary(b4.1_hc, prob = .89)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 \n   Data: d2 (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.61      0.41   153.96   155.27 1.00     3833     2967\n\nFamily Specific Parameters: \n      Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS\nsigma     7.74      0.29     7.29     8.23 1.00     3511     2853\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAbove: I adjusted to ask for 89% intervals, as in McElreath\nMisc notes from McElreath to integrate in\n\n\n\n\n\n\nFinding the posterior distribution with quap\n\n\n\n\n\n\nquadratic approximation\n\n\nposterior’s peak will lie at the maximum a posteriori estimate (MAP), and we can get a useful image of the posterior’s shape by using the quadratic approximation of the posterior distribution at this peak\n\n\nThe quap function works by using the model definition\n\n\nuses these definitions to define the posterior probability at each combination of parameter values. Then it can climb the posterior distribution and find the peak, its MAP. Finally, it estimates the quadratic curvature at the MAP to produce an approximation of the posterior distribution how quap works, approximately\n\nm4.1 <- quap( flist , data=d2 )\n\nThese numbers provide Gaussian approximations for each parameter’s marginal distribution. This means the plausibility of each value of \\(\\mu\\), after averaging over the plausibilities of each value of \\(\\sigma\\), is given by a Gaussian distribution with mean 154.6 and standard deviation 0.4\n\n\nUnless you tell it otherwise, quap starts at random values sampled from the prior. But it’s also possible to specify a starting value for any parameter in the model.\n\nstart <- list( mu=mean(d2$height), sigma=sd(d2$height) ) \nm4.1 <- quap( flist , data=d2 , start=start )\n\n\nwhen you define a list of formulas, you should use alist, so the code isn’t executed. But when you define a list of start values for parameters, you should use list…\n\n\n\n\n\nBut I don’t recommend 95% intervals, because readers will have a hard time not viewing them as significance tests\n\n\nOnce the golem is certain that the mean is near 178-as the prior insists-then the golem has to estimate \\(\\sigma\\) conditional on that fact. This results in a different posterior for \\(\\sigma\\), even though all we changed is prior information about the other parameter\n\n\n\n\n\n\n\nThe quadratic approximation\n\n\n\n\n\n\na quadratic approximation to a posterior distribution with more than one parameter dimension– mu and sigma each contribute one dimension-is just a multi-dimensional Gaussian distribution\n\n\nWhen R constructs a quadratic approximation, it calculates not only standard deviations for all parameters, but also the covariances among all pairs of parameters\n\n\na list of means and a matrix of variances and covariances are sufficient to describe a multi-dimensional Gaussian distribution.\n\n\nvariance-covariance matrix can be factored into two elements: (1) a vector of variances for the parameters and (2) a correlation matrix that tells us how changes in any parameter lead to correlated changes in the others\n\n\n\n\n[correlations]\n\nvery close to zero in this example. This indicates that learning mu tells us nothing about sigma and likewise that learning sigma tells us nothing about mu\n\n\nNow instead of sampling single values from a simple Gaussian distribution, we sample vectors of values from a multi-dimensional Gaussian distribution"
  },
  {
    "objectID": "chapters/4_geocentric_linear_models.html#linear-prediction-4.4",
    "href": "chapters/4_geocentric_linear_models.html#linear-prediction-4.4",
    "title": "\n24  Ch 4. Geocentric (linear) Models\n",
    "section": "Linear prediction (4.4)",
    "text": "Linear prediction (4.4)\n(Kurz)\n\nHere’s our scatter plot of our predictor weight and our criterion height.\n\n\nCodeggplot(data = d2,\n       aes(x = weight, y = height)) +\n  geom_point(shape = 1, size = 2) +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n(Kurz) linear model strategy instructs the golem to assume that the predictor variable has a perfect constant and additive relationship to the mean of the outcome. The golem then computes the posterior distribution of this constant relationship. (p. 92)\n\n\nOur new univariable model will follow the formula\n\n\\[\\begin{align*}\nh_i & \\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\alpha + \\beta x_i \\\\\n\\alpha & \\sim \\text{Normal}(178, 100) \\\\\n\\beta & \\sim \\text{Normal}(0, 10) \\\\\n\\sigma & \\sim \\text{Uniform}(0, 50).\n\\end{align*}\\]\nWe agreed that the prior distribution for sigma here could be improved.\nPriors\nOur univariable model has three priors:\n\\[\\begin{align*}\n\\alpha & \\sim \\text{Normal}(178, 100), \\\\\n\\beta & \\sim \\text{Normal}(0, 10), \\; \\text{and} \\\\\n\\sigma & \\sim \\text{Uniform}(0, 50).\n\\end{align*}\\]\n\n(L:) Unlike with the rethinking package, our brms::brm() syntax won’t perfectly mirror the formal statistical notation. But here are the analogues to the exposition at the bottom of page 95 (with the corrected \\(\\alpha\\) prior).\n\n\n\n\\(h_i \\sim \\text{Normal}(\\mu_i, \\sigma)\\): family = gaussian\n\n\n\\(\\mu_i = \\alpha + \\beta x_i\\): height ~ 1 + weight\n\n\n\\(\\alpha \\sim \\text{Normal}(178, 100)\\): prior(normal(178, 100), class = Intercept\n\n\n\\(\\beta \\sim \\text{Normal}(0, 10)\\): prior(normal(0, 10), class = b)\n\n\n\\(\\sigma \\sim \\text{Uniform}(0, 50)\\): prior(uniform(0, 50), class = sigma)\n\n\n\nThus, to add a predictor you just the + operator in the model formula.\n\n\nbrms linear model of height in weightb4.3 <-\n  brm(data = d2,\n      family = gaussian,\n      height ~ 1 + weight, #THIS is the new bit; we add the slope variable name to the model\n      prior = c(prior(normal(178, 100), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(uniform(0, 50), class = sigma)),\n      iter = 41000, warmup = 40000, chains = 4, cores = 4, #How do we know what values to set here?\n      seed = 4,\n      file = \"fits/b04.03\")\n\n\n\n(K:) This was another example of how using a uniform prior for \\(\\sigma\\) required we use an unusually large number of warmup iterations before the HMC chains converged on the posterior. Change the prior to cauchy(0, 1) and the chains converge with no problem, resulting in much better effective samples, too. Here are the trace plots.\n\n\nCodeplot(b4.3)\n\n\n\n\n\nwith Cauchyb4.3c <-\n  brm(data = d2,\n      family = gaussian,\n      height ~ 1 + weight, #THIS is the new bit; we add the slope variable name to the model\n      prior = c(prior(normal(178, 100), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(uniform(0, 50), class = sigma)),\n      iter = 41000, warmup = 40000, chains = 4, cores = 4, #How do we know what values to set here?\n      seed = 4,\n      file = \"fits/b04c.03\")\n\nplot(b4.3c)\n\n\n\n\nDR: I don’t see how the Cauchy one looks better\n\n24.2.1 More McElreath notes on the linear model\n\nmake the parameter for the mean of a Gaussian distribution, \\(\\mu\\), into a linear function of the predictor variable and other, new parameters that we invent… The linear model strategy instructs the golem to assume that the predictor variable has a constant and additive relationship to the mean of the outcome. The golem then computes the posterior distribution of this constant relationship.\n\n\nWe ask the golem: ‘Consider all the lines that relate one variable to the other. Rank all of these lines by plausibility, given these data.’ The golem answers with a posterior distribution.\n\n\ndefinition of \\(\\mu_i\\) is deterministic … once we know alpha and beta and \\(x_i\\), we know \\(\\mu_i\\) with certainty\n\n\n[\\(\\beta\\)] … is often called a ‘slope’… Better to think of it as a rate of change in expectation\n\n\nWhy have a Gaussian prior with mean zero? … To figure out what this prior implies, we have to simulate the prior predictive distribution\n\n\ngoal is to simulate heights from the model, using only the priors\n\n\nsimulate over. The range of observed weights\n\n\nWe know that average height increases with average weight, at least up to a point. Let’s try restricting it to positive values. The easiest way to do this is to define the prior as Log-Normal instead\n\n\nWe don’t pay any attention to p-values in this book. But the danger remains, if we choose our priors conditional on the observed sample, just to get some desired result. The procedure we’ve performed in this chapter is to choose priors conditional on pre-data knowledge of the variables: their constraints, ranges, and theoretical relationships. This is why the actual data are not shown in the earlier section. We are judging our priors against general facts, not the sample.\n\n\n\n\n\n\n\nDR question – Bayesian p hacking?\n\n\n\n\n\nthis needs elaboration*\nDR: This seems to be Jamie Elsey’s point about reluctance to use any of the data/knowledg from the data in setting the priors, even over hyperparameters. The ‘empirical Bayes’ guy seems to disagree with this.\n\n\n\n\nYou can usefully think of \\(y = log(x)\\) as assigning to \\(y\\) the order of magnitude of \\(x\\). The function \\(x = exp(y)\\) is the reverse, turning a magnitude into a value\n\nDR: This is only approximate\n\n\n\n\n\n\nDR question\n\n\n\n\n\nNote the exp(log_b) in the definition of mu. Thus what’s the benefit of this substitution?\n\n\n\nIntepreting the model fit\n\nThere are two broad categories of processing: (1) reading tables and (2) plotting simulations.\n\n\nemphasize plotting posterior distributions and posterior predictions, instead of attempting to understand a table\n\n\nPlotting the implications of your models will allow you to inquire about things that are hard to read from tables: (1) Whether or not the model fitting procedure worked correctly (2) The absolute magnitude, rather than merely relative magnitude, of a relationship between outcome and predictor (3) The uncertainty surrounding an average relationship (4) The uncertainty surrounding the implied predictions of the model, as these are distinct from mere parameter uncertainty\n\n\nPosterior probabilities of parameter values describe the relative compatibility of different states of the world with the data, according to the model\n\n\n“we inspect the marginal posterior distributions of the parameters”\n\n‘marginal’ bc we are integrating or summing across the other parameters when estimating these measures for each parameter\n\nIt is most certainly not evidence that the relationship between weight and height is linear, because the model only considered lines. It just says that, if you are committed to a line, then lines with a slope around 0.9 are plausible ones\n\n\nto describe the quadratic posterior completely … we also require the variance-covariance matrix\n\n\n\n\n\n\n\nDR question - estimating and interpreting the covariabce matrix\n\n\n\n\n\nWe somehow estimated the (MAP?) values of the parameters in a series of simulations (I forgot how).\nNow we also consider the variance and covariance of these. Is this simply the empirical variance and covariance across simulations? (And then IIRC quap uses these to generate a more complete posterior and sample from it or something). I guess this is helpful because in each simulation I only need to derive a few things, the sort of max confidence values of the parameters, rather than the posterior probability for all possible values.\nWe also can, in principle, use only a few simulations (shown later) and derive an estimate of that covariance matrix and then, assuming normality or something, draw from the joint posterior implied by that covariance matrix to plot things.\n\n\n\n\n[plotting posterior inference is ] an informal check on model assumptions. When the model’s predictions don’t come close to key observations or patterns in the plotted data, then you might suspect the model either did not fit correctly or is rather badly specified\n\nDR: we do something like this in our 80k work etc when we compare the results from a ‘model’ to mean differences across conditions\n\nBut for even slightly more complex models, especially those that include interaction effects (Chapter 8), interpreting posterior distributions is hard\n\n\nposterior distribution considers every possible regression line connecting height to weight. It assigns a relative plausibility to each. This means that each combination of \\(\\alpha\\) and \\(\\beta\\) has a posterior probability. It could be that there are many lines with nearly the same posterior probability as the average line. Or it could be instead that the posterior distribution is rather narrow near the average line\n\n\nwe could sample a bunch of lines from the posterior distribution. Then we could display those lines on the plot, to visualize the uncertainty in the regression relationship.\n\nDrawing randomly from the posterior distribution will of course draw more ‘likely’ lines more often\n\nEach row is a correlated random sample from the joint posterior of all three parameters, using the covariances provided by vcov(m4.3) …\n\n\nthe paired values of a and b on each row define a line. The average of very many of these lines is the posterior mean line\n\n\n\n\n\n\n\nDR question: what is this MAP line?\n\n\n\n\n\nI don’t understand … is he implying that the MAP line will also be the “average” of the intercept and slope coefficients or something? I thought these would be distingt\n\n\n\n\nThe cloud of regression lines displays greater uncertainty at extreme values for weight\n\n\nNotice that the cloud of regression lines grows more compact as the sample size increases. This is a result of the model growing more confident about the location of the mean.\n\n\n\n\n\n\n\nDR question: more confident\n\n\n\n\n\nMore confident about what mean? Does he mean ‘more confident about the slope’?\nI think he means “more confident about the slope of the mean height in weight, as well as about the intercept” … thus more confident about the mean height for each weight\n\n\n\n\n\n\n\n\n\nUsing the link function to compute the posterior distribution of mean weight height for each weight\n\n\n\n\n\n\nlink will … take your quap approximation, sample from the posterior distribution, and then compute $` for each case in the data and sample from the posterior distribution.\n\n\nWe actually want something slightly different: a distribution of \\(\\mu\\) for each unique weight value on the horizontal axis. It’s only slightly harder to compute that, by just passing link some new data:\n\n\nRead apply(mu,2,mean) as compute the mean of each column (dimension ‘2’) of the matrix mu. Now mu.mean contains the average \\(\\mu\\) at each weight value, and mu.PI contains 89% lower and upper bounds [of the mean] for each weight value\n\n\n\n\n\n\n\n\n\n\nSummary of using link to generate predictions and intervals from posterior\n\n\n\n\n\n\n\nUse link to generate distributions of posterior values for \\(\\mu\\). The default behavior of link is to use the original data, so you have to pass it a list of new horizontal axis values you want to plot posterior predictions across.\nUse summary functions like mean or PI to find averages and lower and upper bounds of \\(\\mu\\) for each value of the predictor variable.\nFinally, use plotting functions like lines and shade to draw the lines and intervals. Or you might plot the distributions of the predictions, or do further numerical calculations with them. It’s really up to you. This recipe works for every model we fit in the book.\n\n\nDR: Is there a comparable ‘universal recipe’ in the tidy “love letter” adaptation?\n\n\n\nKurz: Tables of estimates\n\n(K:) With a little [] subsetting we can exclude the log posterior from the posterior_summary() so we can fucus on the parameters.\n\n\nCodeposterior_summary(b4.3, probs=c(0.055, .895))[1:3, ]\n\n               Estimate  Est.Error        Q5.5       Q89.5\nb_Intercept 113.9597354 1.89446919 110.8736517 116.3182837\nb_weight      0.9033226 0.04170303   0.8370964   0.9558139\nsigma         5.1026646 0.19251415   4.8079355   5.3439246\n\n\n\nAgain, brms doesn’t have a convenient corr = TRUE argument for plot() or summary(). But you can get that information after putting the chains in a data frame.\n\n\nCodeposterior_samples(b4.3) %>% #deprecated, needs updating to `as_draws`\n  select(b_Intercept:sigma) %>%\n  cor() %>% #makes correlation matrix\n  round(digits = 2)\n\n            b_Intercept b_weight sigma\nb_Intercept        1.00    -0.99 -0.02\nb_weight          -0.99     1.00  0.02\nsigma             -0.02     0.02  1.00\n\n\n\nMuch like the results from McElreath’s rethinking package, two of the parameters from our model fit with brm() are highly correlated, too. With centering, we can reduce that correlation.\n\n\nCoded2 <-\n  d2 %>%\n  mutate(weight_c = weight - mean(weight))\n\n\n\nFit the weight_c model, b4.4.\n\nDR: let’s ‘Cauchy this’ too…\n\nCodeb4.4c <-\n  brm(data = d2,\n      family = gaussian,\n      height ~ 1 + weight_c, #centering weight, not height here\n      prior = c(prior(normal(178, 100), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(cauchy(0, 1), class = sigma)),\n      iter = 46000, warmup = 45000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.04\")\n\n\n\nCodeplot(b4.4c)\n\n\n\nCodeposterior_summary(b4.4c)[1:3, ]\n\n               Estimate  Est.Error        Q2.5      Q97.5\nb_Intercept 154.5912212 0.27635576 154.0494827 155.116240\nb_weight_c    0.9054614 0.04344806   0.8240883   0.991615\nsigma         5.0953053 0.19670684   4.7237494   5.493291\n\n\n\nLike before, the uniform prior required extensive warmup iterations to produce a good posterior.\n\nDR: where do I see this (in his page)?\n\nThis is easily fixed using a half Cauchy prior, instead.\n\nDR: which I did\n\nAnyways, the effective samples improved.\n\nDR Question: What’s this ‘effective samples’ thing?\nHere’s the parameter correlation info.\n\nCodeposterior_samples(b4.4c) %>%\n  select(b_Intercept:sigma) %>%\n  cor() %>%\n  round(digits = 2)\n\n            b_Intercept b_weight_c sigma\nb_Intercept        1.00      -0.02  0.02\nb_weight_c        -0.02       1.00 -0.01\nsigma              0.02      -0.01  1.00\n\n\n\nSee? Now all the correlations are quite low.\n\nThe ‘pairs’ plot function seems very useful!:\n\nCodepairs(b4.4c)\n\n\n\n\nPutting these into tables … “how you might convert the posterior_summary() output into a summary table roughly following APA style”\n\nCode(\n  sumtable_b4.4c <- \nposterior_summary(b4.4c,  probs=c(0.055, .895))[1:3, ] %>%\n  data.frame() %>% #not tibble, because that wold lose the row names\n  rownames_to_column(\"parameter\") %>% #makes row names their own column. helpful!\n  mutate_if(is.double, round, digits = 2) %>%\n  rename(mean = Estimate,\n         sd   = Est.Error) %>%\n  mutate(`89.5% CI` = str_c(\"[\", Q5.5, \", \", Q89.5, \"]\")) %>%\n  select(-starts_with(\"Q\")) %>%\n  knitr::kable() %>% \n  .kable_styling()\n)\n\n\n\n\n parameter \n    mean \n    sd \n    89.5% CI \n  \n\n\n b_Intercept \n    154.59 \n    0.28 \n    [154.15, 154.93] \n  \n\n b_weight_c \n    0.91 \n    0.04 \n    [0.84, 0.96] \n  \n\n sigma \n    5.10 \n    0.20 \n    [4.79, 5.34] \n  \n\n\n\n\nPlotting posterior inference against the data\n\nIn truth, tables of estimates are usually insufficient for understanding the information contained in the posterior distribution. It’s almost always much more useful to plot the posterior inference against the data. Not only does plotting help in interpreting the posterior, bit it also provides an informal check on model assumptions. (p. 100)\n\n\nCoded2 %>%\n  ggplot(aes(x = weight, y = height)) +\n  geom_abline(intercept = fixef(b4.3c)[1],\n              slope     = fixef(b4.3c)[2]) + #see docs -- could also select rows by name here \n  geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\n\nAbove fixef() basically ‘extracts the regression parameters’, (documentation: extracts the mean value of these by default)\n\n\n\n\n\n\nOn fixef() and population-level ‘fixed’ effects\n\n\n\n\n\nIn the brms reference manual, Bürkner described the job of thefixef() function as “extract[ing] the population-level (‘fixed’) effects from a brmsfit object”. If you’re new to multilevel models, it might not be clear what he meant by “population-level” or “fixed” effects. Don’t worry. That’ll all become clear starting around [Chapter 12][Multilevel Models]. In the meantime, just think of them as the typical regression parameters, minus \\(\\sigma\\).\n\n\n\nKurz: Adding uncertainty around the mean\n\nBy default, we extract all the posterior iterations with posterior_samples(). Because we had 4,000 posterior draws, our output will contain 4,000 rows.\n\n\nCodepost <- posterior_samples(b4.3c)\n\npost %>%\n  glimpse()\n\nRows: 4,000\nColumns: 4\n$ b_Intercept <dbl> 112.6110, 113.1607, 115.7462, 114.4355, 113.9836, 114.5581, 114.4498, 114.4880…\n$ b_weight    <dbl> 0.9285251, 0.9237377, 0.8655688, 0.8952351, 0.9104904, 0.8933690, 0.8884695, 0…\n$ sigma       <dbl> 4.990416, 5.091564, 4.920441, 5.196292, 4.906998, 5.104242, 5.395416, 5.241586…\n$ lp__        <dbl> -1082.665, -1082.280, -1082.968, -1082.369, -1083.407, -1082.283, -1083.561, -…\n\n\n\nEach row is a correlated random sample from the point posterior of all three parameters, using the covariances provided by [cov(posterior_samples(b4.4)]. The paired values of [b_Intercept] and [b_weight] on each row define the line. The average of very many of these lines is the MAP line. (p. 101)\n\n\n\n\n\n\n\nDR question: the ‘correlated … sample’\n\n\n\n\n\nI thought the estimation yielded a sample from the posterior, these 4000 draws. But here it sounds like it is estimating a mean and covariance matrix and drawing from this distribution?\n\n\n\nModeling and plotting, with 4 different sample sizes\n\nHere are the four models leading up to McElreath’s Figure 4.5.\n\n\n\n\n\n\n\nDR question – there must be a better way of automating the code below across the 4 sample sizes, with a ‘map’?\n\n\n\n\n\n\n\n\n\n\nCoden <- 10\n\nb4.3_010 <-\n  brm(data = d2 %>%\n        slice(1:n),  # note our tricky use of `n` and `slice()`\n      family = gaussian,\n      height ~ 1 + weight,\n      prior = c(prior(normal(178, 100), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(cauchy(0, 1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.03_010\")\n\nn <- 50\n\nb4.3_050 <-\n  brm(data = d2 %>%\n        slice(1:n),\n      family = gaussian,\n      height ~ 1 + weight,\n      prior = c(prior(normal(178, 100), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(cauchy(0, 1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.03_050\")\n\nn <- 150\n\nb4.3_150 <-\n  brm(data = d2 %>%\n        slice(1:n),\n      family = gaussian,\n      height ~ 1 + weight,\n      prior = c(prior(normal(178, 100), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(cauchy(0, 1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.03_150\")\n\nn <- 352\n\nb4.3_352 <-\n  brm(data = d2 %>%\n        slice(1:n),\n      family = gaussian,\n      height ~ 1 + weight,\n      prior = c(prior(normal(178, 100), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(cauchy(0, 1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.03_352\")\n\n\nDR: Note, this took surprisingly long even for N=10!\n\nWe’ll need to put the chains [?] of each model into data frames.\n\n\n\n\n\n\n\nDR question: why are these called ‘chains’ here?\n\n\n\n\n\nI thought the samples from the posterior were the result of the modeling ‘after the chains had converged’?\n\n\n\n\nCodepost010 <- posterior_samples(b4.3_010)\npost050 <- posterior_samples(b4.3_050)\npost150 <- posterior_samples(b4.3_150)\npost352 <- posterior_samples(b4.3_352)\n\n\n\nHere is the code for the four individual plots.\n\n\n\n\n\n\n\nDR question: isn’t there a more efficient code way to ‘map’ all these?\n\n\n\n\n\n\n\n\n\n\nCodep1 <-\n  ggplot(data =  d2[1:10 , ], #first 10 obs; the ones used in fitting the model\n         aes(x = weight, y = height)) +\n  geom_abline(intercept = post010[1:20, 1], #20 lines for each to reduce clutter\n              slope     = post010[1:20, 2],\n              size = 1/3, alpha = .3) +\n  geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  coord_cartesian(xlim = range(d2$weight),\n                  ylim = range(d2$height)) +\n  labs(subtitle = \"N = 10\")\n\np2 <-\n  ggplot(data =  d2[1:50 , ], #first 50 obs...\n         aes(x = weight, y = height)) +\n  geom_abline(intercept = post050[1:20, 1],\n              slope     = post050[1:20, 2],\n              size = 1/3, alpha = .3) +\n  geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  coord_cartesian(xlim = range(d2$weight),\n                  ylim = range(d2$height)) +\n  labs(subtitle = \"N = 50\")\n\np3 <-\n  ggplot(data =  d2[1:150 , ],\n         aes(x = weight, y = height)) +\n  geom_abline(intercept = post150[1:20, 1],\n              slope     = post150[1:20, 2],\n              size = 1/3, alpha = .3) +\n  geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  coord_cartesian(xlim = range(d2$weight),\n                  ylim = range(d2$height)) +\n  labs(subtitle = \"N = 150\")\n\np4 <-\n  ggplot(data =  d2[1:352 , ],\n         aes(x = weight, y = height)) +\n  geom_abline(intercept = post352[1:20, 1],\n              slope     = post352[1:20, 2],\n              size = 1/3, alpha = .3) +\n  geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  coord_cartesian(xlim = range(d2$weight),\n                  ylim = range(d2$height)) +\n  labs(subtitle = \"N = 352\")\n\n\n\nNote how we used the good old bracket syntax (e.g., d2[1:10 , ]) to index rows from our d2 data. With tidyverse-style syntax, we could have done slice(d2, 1:10) or d2 %>% slice(1:10) instead.\n\n\nNow we can combine the ggplots with patchwork syntax to make the full version of Figure 4.5.\n\nDR: I didn’t know this syntax, I previously used this some combine plots function thing\n\nCode(p1 + p2 + p3 + p4) &\n  theme_bw() &\n  theme(panel.grid = element_blank())\n\n\n\n\nPrediction intervals\n\ngenerating an 89% prediction interval for actual heights, not just the average height, \\(\\mu\\) . This means we’ll incorporate the standard deviation \\(\\sigma\\) and its uncertainty as well\n\n\nFor any unique weight value, you sample from a Gaussian distribution with the correct mean \\(\\mu\\) for that weight, using the correct value of \\(\\sigma\\) sampled from the same posterior distribution.\n\n\n\n\n\n\n\nDR question: what does he mean by ‘correct’ here?\n\n\n\n\n\n\n\n\n\n\ndo this for every sample from the posterior, for every weight value of interest, you end up with a collection of simulated heights that embody the uncertainty in the posterior as well as the uncertainty in the Gaussian distribution of heights. There is a tool called sim which does this:\n\n\nThis matrix is much like the earlier one, mu, but it contains simulated heights ///\n\nA vector of simulated heights for each element in the weight sequence\n\nYou could plot the boundary for other percents, such as 67% and 97% (also both primes), and add those to the plot. it would be nice to plot several of these together, perhaps a gradual distribution/elevation plot of confidence\n\n\n\n\n\n\n\nWhy simulate and not use analytic formulas here?\n\n\n\n\n\n\ntrue that it is possible to use analytical formulas to compute intervals like this … and there is some additional insight that comes from knowing the mathematics, [however] the pseudo-empirical approach presented here is very flexible and allows a much broader audience of scientists to pull insight from their statistical modeling. And again, when you start estimating models with MCMC (Chapter 9), this is really the only approach available\n\n\n\n\n\nFor every distribution like dnorm, there is a companion simulation function\n\ndnorm: specifies the density at any point (I guess) rnorm: randomly generates draws the normal distribution"
  },
  {
    "objectID": "chapters/4_geocentric_linear_models.html#curves-from-lines-4.5",
    "href": "chapters/4_geocentric_linear_models.html#curves-from-lines-4.5",
    "title": "\n24  Ch 4. Geocentric (linear) Models\n",
    "section": "Curves from lines (4.5)",
    "text": "Curves from lines (4.5)\n\n\n\n\n\n\nNote: I don’t think Kurz does the ‘splines’ coding\n\n\n\n\n\n\n\n\n\nPolynomial regression\nWays to ‘build curves’:\n\nThe first is polynomial regression. The second is b-splines\n\nDR: I need to know more about how to use the splines\nDefines the parabolic model…\n\n… Just modify the definition of mu so that it contains both the linear and quadratic terms. But in general it is better to pre-process any variable transformations-you don’t need the computer to recalculate the transformations on every iteration of the fitting procedure\n\n\nThe parameter \\(\\alpha\\) (a) is still the intercept, so it tells us the expected value of height when weight is at its mean value. But it is no longer equal to the mean height in the sample, since there is no guarantee it should in a polynomial regression\n\nWe aren’t learning any causal relationship between height and weight\n\nThe quadratic is probably the most commonly-used polynomial regression model. It follows the form\n\n\\[\\mu = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2.\\]\nStandardizing the rhs variable to aid intepretation, and to help brm fit the model:\n\nCoded <-\n  d %>%\n  mutate(weight_s = (weight - mean(weight)) / sd(weight))\n\n\nHere’s the quadratic model in brms.\n\nCodeb4.5 <-\n  brm(data = d,\n      family = gaussian,\n      height ~ 1 + weight_s + I(weight_s^2),\n      prior = c(prior(normal(178, 100), class = Intercept),\n                prior(normal(0, 10), class = b), #I guess this is applied to *both* slope coefficients? -- yes, noted in next chapter\n                prior(cauchy(0, 1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.05\")\n\n\n\nCodeplot(b4.5)\n\n\n\nCodeprint(b4.5)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 + weight_s + I(weight_s^2) \n   Data: d (Number of observations: 544) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     146.67      0.38   145.91   147.41 1.00     3691     3202\nweight_s       21.40      0.30    20.83    21.98 1.00     3621     2830\nIweight_sE2    -8.42      0.28    -8.98    -7.86 1.00     3450     3040\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.77      0.18     5.44     6.11 1.00     3664     2794\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nOur quadratic plot requires new fitted()- and predict()-oriented wrangling.\n\n\nCodeweight_seq <- tibble(weight_s = seq(from = -2.5, to = 2.5, length.out = 30)) #the normalized weights\n\nf <-\n  fitted(b4.5,\n         newdata = weight_seq) %>%\n  as_tibble() %>%\n  bind_cols(weight_seq)\n\np <-\n  predict(b4.5,\n          newdata = weight_seq) %>%\n  as_tibble() %>%\n  bind_cols(weight_seq)\n#DR: Was this randomly drawing  'error terms'? I forget what the difference was here\n\n\n\nBehold the code for our version of Figure 4.9.a.\n\n\nCodeggplot(data = d,\n       aes(x = weight_s)) +\n  geom_ribbon(data = p,\n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = f,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, size = 1/2) +\n  geom_point(aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 1/3) +\n  coord_cartesian(xlim = range(d$weight_s)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n\n\n\n\nSplines\n\nB-splines do not directly transform the predictor by squaring or cubing it. Instead they invent a series of entirely new, synthetic predictor variables. Each of these synthetic variables exists only to gradually turn a specific parameter on and off within a specific range of the real predictor variable. Each of the synthetic variables is called a basis function\n\n\nThe linear model ends up looking very familiar: -i = - + w1Bi,1 + w2Bi,2 + w3Bi,3 + … where Bi,n is the n-th basis function-s value on row i, and the w parameters are corresponding weights for each\n\n\ndivide the full range of the horizontal axis into four parts, using pivot points called knots. The\n\n\nThese synthetic variables are used to gently transition from one region of the horizontal axis to the next. Essentially, these variables tell you which knot you are close to. Beginning on the left of the top plot, basis function 1 has value 1 and all of the others are set to zero. As we move rightwards towards the second knot, basis 1 declines and basis 2 increases. At knot 2, basis 2 has value 1, and all of the others are set to zero\n\n\nthey make the influence of each parameter quite local. At any point on the horizontal axis in Figure 4.12, only two basis functions have non-zero values\n\n\nParameters called weights multiply the basis functions. The spline at any given point is the sum of these weighted basis functions\n\n\nthe knots are just values of year that serve as pivots for our spline. Where should the knots go?\n\n\nsimple example above, place the knots at different evenly spaced quantiles of the predictor variable. This gives you more knots where there are more observations. We\n\n\nnext choice is polynomial degree. This determines how basis functions combine, which determines how the parameters interact to produce the spline not fully explained\n\n\nthe w priors influence how wiggly the spline can be"
  },
  {
    "objectID": "chapters/5_spurious_waffles.html",
    "href": "chapters/5_spurious_waffles.html",
    "title": "25  Ch 5. The Many Variables & The Spurious Waffles",
    "section": "",
    "text": "Load the Waffle House data."
  },
  {
    "objectID": "chapters/5_spurious_waffles.html#spurious-association",
    "href": "chapters/5_spurious_waffles.html#spurious-association",
    "title": "25  Ch 5. The Many Variables & The Spurious Waffles",
    "section": "5.1. Spurious association",
    "text": "5.1. Spurious association\n\nBut there’s no reason high marriage rate must cause more divorce\n\n\n\n\n\n\n\nDR: what would this even mean?\n\n\n\n\n\n… the ‘causal’ part is ambiguous in this example, particularly when we are talking about aggregates like ‘rates for the whole population’. I’m familiar with the framework of the Rubin causal model, where we consider the counterfactual state of some real-world outcome, when another real-world outcome would be set at one level or another.\n\n\n\n\neasy to imagine high marriage rate indicating high cultural valuation of marriage and therefore being associated with low divorce rate\n\nDR:2\n\nSince the outcome and the predictor are both standardized, the intercept - should end up very close to zero\n\n\\(\\alpha ∼ Normal(0, 0.2)\\)\nDR:3\n\nSo when \\(\\beta_A = 1\\), a change of 1.2 years in median age at marriage is associated with a full standard deviation change in the outcome variable [which he thinks is absurdly large]\n\nDR:4\n\n[a model] that includes both age at marriage and marriage rate will help us.\n\nDR:5\n\ncausal impact\n\nDR:6\n\nDAG will tell you the consequences of intervening to change a variable. But only if the DAG is correct\n\n\nPerhaps a direct effect would arise because younger people change faster than older people and are therefore more likely to grow incompatible with a partner. Second, it has an indirect effect by influencing the marriage rate, which then influences divorce, A \\(\\rightarrow\\) M \\(\\rightarrow\\) D\n\n\nthese different arrows, we need more than one statistical model.\n\n\nModel m5.1, the regression of D on A, tells us only that the total influence of age at marriage is strongly negative with divorce rate\n\nDR:7\n\nThe ‘total’ here means we have to account for every path from A to D. There are two such paths in this graph: \\(A \\rightarrow D\\), a direct path, and \\(A \\rightarrow M \\rightarrow D\\), an indirect path. In general, it is possible that a variable like A has no direct effect at all on an outcome like D. It could still be associated with D entirely through the indirect path. That type of relationship is known as mediation, and we’ll have another example later.\n\n\nThis DAG is also consistent with the posterior distributions of models m5.1 and m5.2. Why? Because both M and D ‘listen’ to A\n\n\nconditional independencies … which variables become dis-associated when we condition on some other set of variables.\n\n… that every pair of variables is correlated. This is because there is a causal arrow between every pair. These arrows create correlations.\nDR:8\n\nThey share a cause, and this leads them to be correlated with one another through that cause. But suppose we condition on A. All of the information in M that is relevant to predicting D is in A. So once we’ve conditioned on A, M tells us nothing more about D. So in the second DAG, a testable implication is that D is independent of M, conditional on A. In other words, \\(D \\perp M|A\\).\n\n\nHere’s the code to define the second DAG and display the implied conditional independencies.\n\n\nCodelibrary(dagitty)\n\nDMA_dag2 <- dagitty::dagitty('dag{ D <- A -> M }')\nimpliedConditionalIndependencies( DMA_dag2 )\n\nD _||_ M | A\n\n\n\nSo for example once you fit a multiple regression to predict divorce using both marriage rate and age at marriage, the model addresses the questions: (1) After I already know marriage rate, what additional value is there in also knowing age at marriage? (2) After I already know age at marriage, what additional value is there in also knowing marriage rate?\n\nDR: how can we differentiate 1 from 2:9\nCoding data vis and univariate models (Kurz)\nKurz on themes and plots:10\n\nCode# install.packages(\"ggrepel\", dependencies = T)\nlibrary(ggrepel)\n\nd %>%\n  ggplot(aes(x = WaffleHouses/Population, y = Divorce)) + #note we can put transformations *within* the `aes`\n  \n  stat_smooth(method = \"lm\", fullrange = T, size = 1/2,\n              color = \"firebrick4\", fill = \"firebrick\", alpha = 1/5) + #the linear plot and the shaded 'some sort of bounds' thing (what bound?)\n  \n  geom_point(size = 1.5, color = \"firebrick4\", alpha = 1/2) +\n  geom_text_repel(data = d %>% filter(Loc %in% c(\"ME\", \"OK\", \"AR\", \"AL\", \"GA\", \"SC\", \"NJ\")),  #only for key US states of interest\n                  aes(label = Loc), \n                  size = 3, seed = 1042) +  # this makes it reproducible\n  \n  scale_x_continuous(\"Waffle Houses per million\", limits = c(0, 55)) +\n  ylab(\"Divorce rate\") +\n  coord_cartesian(xlim = c(0, 50), ylim = c(5, 15)) +\n  theme_bw() +\n  theme(panel.grid = element_blank())  #removes gridlines\n\n\n\n\nDR note: I’m skipping the map plotting, for now, even though it’s a cool vis.\n\nStandardize median age marriaged <-\n  d %>%\n  mutate(MedianAgeMarriage_s = (MedianAgeMarriage - mean(MedianAgeMarriage)) /\n           sd(MedianAgeMarriage))\n\n\n\nfit the first univariable model, print itb5.1 <- \n  brm(data = d, \n      family = gaussian,\n      Divorce ~ 1 + MedianAgeMarriage_s,\n      prior = c(prior(normal(10, 10), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(uniform(0, 10), class = sigma)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5,\n      file = \"fits/b05.01\")\n\nprint(b5.1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Divorce ~ 1 + MedianAgeMarriage_s \n   Data: d (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept               9.69      0.22     9.25    10.12 1.00     5173     4232\nMedianAgeMarriage_s    -1.04      0.21    -1.46    -0.63 1.00     4436     4274\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.51      0.16     1.24     1.86 1.00     5403     4424\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nBelow…\n\nCreate: nd – tibble of standardized predictor values to plot over\napply fitted to nd with the newdata argument to fit values over this range (rather than over the data the model was fit on)\n\n“to return model-implied expected values for Divorce”\n\nRange to plot over, predict for this range# define the range of `MedianAgeMarriage_s` values we'd like to feed into `fitted()`\nnd <- tibble(MedianAgeMarriage_s = seq(from = -3, to = 3.5, length.out = 30))\n\n# now use `fitted()` to get the model-implied trajectories\nf <- \n  fitted(b5.1, newdata = nd) %>%\n  as_tibble() %>%\n  # tack the `nd` data onto the `fitted()` results\n  bind_cols(nd)\n\n\n\nPlot predictions# plot\nggplot(data = f, \n       aes(x = MedianAgeMarriage_s, y = Estimate)) +\n  geom_smooth(aes(ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  geom_point(data = d, #overlaying the original data\n             aes(y = Divorce), \n             size = 2, color = \"firebrick4\") +\n  ylab(\"Divorce\") +\n  coord_cartesian(xlim = range(d$MedianAgeMarriage_s), \n                  ylim = range(d$Divorce)) +\n  theme_bw() +\n  theme(panel.grid = element_blank())                   \n\n\n\n\nNext … they do the same thing but with marriage rate as the predictor\n\nCoded <-\n  d %>%\n  mutate(Marriage_s = (Marriage - mean(Marriage)) / sd(Marriage))\n\n\n\nCodeb5.2 <- \n  brm(data = d, \n      family = gaussian,\n      Divorce ~ 1 + Marriage_s,\n      prior = c(prior(normal(10, 10), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(uniform(0, 10), class = sigma)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5,\n      file = \"fits/b05.02\")\n\n\n\nCodeprint(b5.2)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Divorce ~ 1 + Marriage_s \n   Data: d (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nPopulation-Level Effects: \n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      9.68      0.25     9.19    10.17 1.00     5003     4432\nMarriage_s     0.63      0.25     0.15     1.11 1.00     5641     4331\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.75      0.19     1.43     2.17 1.00     5148     3934\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nI’ll skip plotting this for now\nMultiple regression notation\nModel with\nmu <- a + bM*M + bA*A\n\nNotice how bA doesn’t move, only grows a bit more uncertain, while bM is only associated with divorce when age at marriage is missing from the model. You can interpret these distributions as saying:\n\n\nOnce we know median age at marriage for a State, there is little or no additional predictive power in also knowing the rate of marriage in that State. In that weird notation, \\(D \\perp M|A\\). This tests the implication of the second DAG from earlier.\n\n\nSince the first DAG did not imply this result, it is out\n\nDR: This seems like a very weak test and a poor strategy for making inferences about causality (more so running with those inferences to underly future modeling). Even when age is present in the model, the compatibility intervals for the marriage rate coefficient include rather large coefficients in either direction.\nFitting the (multivariate) model\n‘Priors for each slope’11\n\nCodeb5.3 <- \n  brm(data = d, \n      family = gaussian,\n      Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s,\n      prior = c(prior(normal(10, 10), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(uniform(0, 10), class = sigma)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5,\n      file = \"fits/b05.03\")\n\n\n\nCodeprint(b5.3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s \n   Data: d (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept               9.69      0.22     9.27    10.13 1.00     5542     4118\nMarriage_s             -0.19      0.30    -0.78     0.40 1.00     4042     4202\nMedianAgeMarriage_s    -1.23      0.31    -1.82    -0.62 1.00     4021     4081\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.53      0.16     1.25     1.88 1.00     5026     3391\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nmcmc_plot(b5.3) seems to produce a ggplot object:\n\nCode(\nxxx <- mcmc_plot(b5.3) + ggtitle(\"My fancy bayesplot-based coefficient plot\") +\n  theme_bw() +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank()\n  )\n)\n\n\n\n\nWith the bayesplot::mcmc_intervals function you can specify what you want more precisely.\n\nCode# install.packages(\"bayesplot\", dependencies = T)\nlibrary(bayesplot)\n\npost <- posterior_samples(b5.3)\n\ncolor_scheme_set(\"red\")\nmcmc_intervals(post[, 1:4], \n               prob = .65,\n  prob_outer = 0.95,\n               point_est = \"mean\") + #Kurz and the default use 'median'\n  ggtitle(\"My fancy bayesplot-based coefficient plot\") +\n  theme_bw() +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank())\n\n\n\n\nNote; this approach required you to work with the posterior_samples() instead of the brmsfit object. Just to be different, I set point_est = \"mean\" instead of median, and prob = .65 for the ‘inner interval’ and prob_outer = 0.95.\n\nThe tidybayes::stat_pointinterval() function offers a third way, this time with a more ground-up ggplot2 workflow.\n\n… I will return to that later, perhaps. I think Willem would prefer the control that ‘doing it yourself with ggplot’ offers\nPlotting multivariate posteriors.\n\n\nPredictor residual plots. These plots show the outcome against residual predictor values. …\n\n\n\n\nPosterior prediction plots. These show model-based predictions against raw data, or otherwise display the error in prediction. They are tools for checking fit and assessing predictions. …\n\n\n\n\nCounterfactual plots. These show the implied predictions for imaginary experiments. These plots allow you to explore the causal implications of manipulating one or more variables.\n\n\nPredictor residual plots\nPredictions and residuals: outcomes vs\n‘How well does the “surprise part”12 of one feature predict the outcome’“?\n\nTo get ready to make our residual plots, we’ll predict Marriage_s with MedianAgeMarriage_s.\n\n13\n\nCodeb5.4 <- \n  brm(data = d,\n      family = gaussian,\n      Marriage_s ~ 1 + MedianAgeMarriage_s,\n      prior = c(prior(normal(0, 10), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(uniform(0, 10), class = sigma)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5,\n      file = \"fits/b05.04\")\n\nprint(b5.4)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Marriage_s ~ 1 + MedianAgeMarriage_s \n   Data: d (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              -0.00      0.10    -0.20     0.20 1.00     5731     4560\nMedianAgeMarriage_s    -0.71      0.10    -0.91    -0.51 1.00     5623     4288\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.72      0.08     0.59     0.89 1.00     5449     4270\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nWith fitted(), we compute the expected values for each state (with the exception of Nevada).14\n\n\nCodef <- \n  fitted(b5.4) %>%\n  as_tibble() %>%\n  bind_cols(d)\n\nhead(f)\n\n# A tibble: 6 × 19\n  Estimate Est.E…¹     Q2.5  Q97.5 Locat…² Loc   Popul…³ Media…⁴ Marri…⁵ Marri…⁶\n     <dbl>   <dbl>    <dbl>  <dbl> <fct>   <fct>   <dbl>   <dbl>   <dbl>   <dbl>\n1    0.431   0.119  0.199    0.667 Alabama AL       4.78    25.3    20.2    1.27\n2    0.489   0.124  0.245    0.732 Alaska  AK       0.71    25.2    26      2.93\n3    0.144   0.104 -0.0560   0.350 Arizona AZ       6.33    25.8    20.3    0.98\n4    1.01    0.177  0.657    1.36  Arkans… AR       2.92    24.3    26.4    1.7 \n5   -0.430   0.117 -0.660   -0.197 Califo… CA      37.2     26.8    19.1    0.39\n6    0.202   0.106 -0.00519  0.409 Colora… CO       5.03    25.7    23.5    1.24\n# … with 9 more variables: Divorce <dbl>, Divorce.SE <dbl>, WaffleHouses <int>,\n#   South <int>, Slaves1860 <int>, Population1860 <int>, PropSlaves1860 <dbl>,\n#   MedianAgeMarriage_s <dbl>, Marriage_s <dbl>, and abbreviated variable names\n#   ¹​Est.Error, ²​Location, ³​Population, ⁴​MedianAgeMarriage, ⁵​Marriage,\n#   ⁶​Marriage.SE\n\n\nAfter a little data processing, we can make Figure 5.3.\n\nCodef %>% \n  ggplot(aes(x = MedianAgeMarriage_s, y = Marriage_s)) +\n  geom_point(size = 2, shape = 1, color = \"firebrick4\") +\n  geom_segment(aes(xend = MedianAgeMarriage_s, yend = Estimate), \n               size = 1/4) +\n  geom_line(aes(y = Estimate), \n            color = \"firebrick4\") +\n  coord_cartesian(ylim = range(d$Marriage_s)) +\n  theme_bw() +\n  theme(panel.grid = element_blank())     \n\n\n\n\nSkipped a bunch here; come back to it\n\nThe trick with simulating counterfactuals is to realize that when we manipulate some variable X, we break the causal influence of other variables on X.\n\n\nNow we can use sim, which you met in the previous chapter, to simulate observations from model m5.3_A. But this time we’ll tell it to simulate both M and D, in that order. Why in that order? Because we have to simulate the influence of A on M before we simulate the joint influence of A and M on D. The vars argument to sim tells it both which observables to simulate and in which order.\n\n\nCode# prep data 5.21\nsim_dat <- data.frame( A=A_seq )\n# simulate M and then D, using A_seq\ns <- sim( m5.3_A , data=sim_dat , vars=c(\"M\",\"D\") )\n\n\n\nCodeplot( sim_dat$A , colMeans(s$D) , ylim=c(-2,2) , type=\"l\" ,\nxlab=\"manipulated A\" , ylab=\"counterfactual D\" )\nshade( apply(s$D,2,PI) , sim_dat$A )\nmtext( \"Total counterfactual effect of A on D\" )\n\n\nCounterfactual plots\n\n\nPlot the model predicted outcome for a range of ‘possible’ values of predictors (which may not occur in the data)\nE.g. plot predicted divorce rates against the median age of marriage in a state, assuming all these states have the average marriage rate\n\n(This is sort of like the ‘estimated marginal means’ thing we often plot)\n\n\n\n\nWe can also plot ‘direct and indirect effects’\nIf we really believe the causal structure we laid out…\n\n(1) Pick a variable to manipulate, the intervention variable.\n(2) Define the range of values to set the intervention variable to.\n(3) For each value of the intervention variable, and for each sample in posterior, use the causal model to simulate the values of other variables, including the outcome.\n\n\nTo estimate the influence of A on M, all we need is to regress A on M. There are no other variables in the DAG creating an association between A and M. We can just add this regression to the quap model\n\n\ndefine a range of values for A … this time we’ll tell it to simulate both M and D, in that order. Why in that order? Because we have to simulate the influence of A on M before we simulate the joint influence of A and M on D. The vars argument to sim tells it both which observables to simulate and in which order.\n\n\nPosterior prediction plots\n“Check the model fit against the observed data”\nDR: And do what with this? Couldn’t any such response run the risk of overfitting?"
  },
  {
    "objectID": "chapters/5_spurious_waffles.html#simulating-the-various-problems-spurious-association-masking",
    "href": "chapters/5_spurious_waffles.html#simulating-the-various-problems-spurious-association-masking",
    "title": "25  Ch 5. The Many Variables & The Spurious Waffles",
    "section": "\n25.1 (Simulating) the various problems (Spurious association, masking, …)",
    "text": "25.1 (Simulating) the various problems (Spurious association, masking, …)\nDR: This seems like an important check on our work. Such simulations may have a high type-1 error, yielding ‘false negatives’ (where we see ‘hmm, the results look fine, no bias here’) by coincidence. But they should have a low or zero type-2 error, if done right. Where we can simulate a ‘biased case’, we know such a bias is a potential threat.\nMasked relationship\n\ntends to arise when there are two predictor variables that are correlated with one another. However, one of these is positively correlated with the outcome and the other is negatively correlated with it\n\nE.g.,\n\nOutcome: ENGAGE ‘self-reported engagement’\ntime_in_EA: Positive relation to ENGAGE\nInterest_Global_health: Negative relation to ENGAGE\ntime_in_EA, Interest_Global_health: Positive relationships\n\nSo, a simple model of ENGAGE ~ time_in_EA might find a zero (or ‘too small’) relationship, suggesting time_in_EA has little or no (causal) impact on engagement. However in a model like ENGAGE ~ time_in_EA + Interest_Global_health we might see the latter has a negative relationship to the outcome and the former a positive one.\n\n\nWhy did adding neocortex and body mass to the same model lead to stronger associations for both? This is a context in which there are two variables correlated with the outcome, but one is positively correlated with it and the other is negatively correlated with it. In addition, both of the explanatory variables are positively correlated with one another. Try a simple pairs(~K + M + N , dcc ) plot to appreciate this pattern of correlation. The result of this pattern is that the variables tend to cancel one another out.\n\n\n\nBeginning on the left, the first possibility is that body mass (M) influences neocortex percent (N). Both then influence kilocalories in milk (K). Second, in the middle, neocortex could instead influence body mass. The two variables still end up correlated in the sample. Finally, on the right, there could be an unobserved variable U that influences both M and N, producing a correlation between them. In this book, I’ll circle variables that are unobserved. One of the threats to causal inference is that there are potentially many unobserved variables thatinfluence an outcome or the predictors.\n\n\nWhich of these graphs is right? We can’t tell from the data alone, because these graphs imply the same set of conditional independencies. In this case, there are no conditional independencies—each DAG above implies that all pairs of variables are associated, regardless of what we condition on. A set of DAGs with the same conditional independencies is known as a Markov equivalence set.\n\n\nthe data alone can never tell you which causal model is correct\n\nEstimating the masked milk model\nLet’s load the Hindle and Milligan (2011) milk data.\n\nCodelibrary(rethinking)\ndata(milk)\nd <- milk\n\n\nUnload rethinking and load brms.\n\nCoderm(milk)\ndetach(package:rethinking, unload = T)\nlibrary(brms)\n\n\nYou might inspect the data like this.\n\nCoded %>% \n  select(kcal.per.g, mass, neocortex.perc) %>% \n  pairs(col = \"firebrick4\")\n\n\n\n\nHard to make out the relationship\nSimple univariable milk model, with ‘realistic priors’\n\n\n\nPriors15 NAs: 16\n\n\nCodedcc <- \n  d %>%\n  tidyr::drop_na(ends_with(\"perc\"))              \n\n\nBut anyway, let’s inspect the parameter summary.\n\nCodeprint(b5.5, digits = 3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: kcal.per.g ~ 1 + neocortex.perc \n   Data: d (Number of observations: 17) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nPopulation-Level Effects: \n               Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept         0.358     0.563   -0.740    1.461 1.000     5006     3613\nneocortex.perc    0.004     0.008   -0.012    0.021 1.000     5053     3672\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.194     0.041    0.133    0.293 1.001     3123     3496\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\nTo get the brms answer to what McElreath did with coef(), we can use the fixef() function.\n\nI guess this considers the estimated difference in predicted kcal.per.g for a species with 76 vs 55 neocortex.perc; these are the extreme values in the data perhaps?\n\nCodefixef(b5.5)[2] * (76 - 55)\n\n[1] 0.09273912\n\n\nYes, indeed, “that’s less than 0.1 kilocalories” (p. 137).\nJust for kicks, we’ll superimpose 50% intervals atop 95% intervals for the next few plots. Here’s Figure 5.7, top left.\n\nCodend <- tibble(neocortex.perc = 54:80)\n\nfitted(b5.5, \n       newdata = nd,\n       probs = c(.025, .975, .25, .75)) %>% #fits quantiles of predicted values \n  as_tibble() %>%\n  bind_cols(nd) %>% #bind to original data\n  \n  ggplot(aes(x = neocortex.perc, y = Estimate)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), #ribbon boundaries\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = Q25, ymax = Q75),\n              stat = \"identity\",\n              fill = \"firebrick4\", color = \"firebrick4\", alpha = 1/5, size = 1/2) +\n  geom_point(data = dcc, \n             aes(y = kcal.per.g),\n             size = 2, color = \"firebrick4\") +\n  ylab(\"kcal.per.g\") +\n  coord_cartesian(xlim = range(dcc$neocortex.perc), \n                  ylim = range(dcc$kcal.per.g)) +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\n\nLet’s make the log_mass variable.\n\nCodedcc <-\n  dcc %>%\n  mutate(log_mass = log(mass))\n\n\nNow we use log_mass as the new sole predictor.\n\nCodeb5.6 <- \n  brm(data = dcc, \n      family = gaussian,\n      kcal.per.g ~ 1 + log_mass,\n      prior = c(prior(normal(0, 100), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(uniform(0, 1), class = sigma)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      control = list(adapt_delta = 0.9),\n      seed = 5,\n      file = \"fits/b05.06\")\n\n\n\nCodeprint(b5.6, digits = 3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: kcal.per.g ~ 1 + log_mass \n   Data: dcc (Number of observations: 17) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept    0.705     0.057    0.593    0.818 1.001     4340     3421\nlog_mass    -0.031     0.024   -0.079    0.016 1.000     4210     3298\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.183     0.039    0.127    0.271 1.002     3993     3696\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nMake Figure 5.7, top right.\n\nCodend <- tibble(log_mass = seq(from = -2.5, to = 5, length.out = 30))\n\nfitted(b5.6, \n       newdata = nd,\n       probs = c(.025, .975, .25, .75)) %>%\n  as_tibble() %>%\n  bind_cols(nd) %>% \n  \n  ggplot(aes(x = log_mass, y = Estimate)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = Q25, ymax = Q75),\n              stat = \"identity\",\n              fill = \"firebrick4\", color = \"firebrick4\", alpha = 1/5, size = 1/2) +\n  geom_point(data = dcc, \n             aes(y = kcal.per.g),\n             size = 2, color = \"firebrick4\") +\n  ylab(\"kcal.per.g\") +\n  coord_cartesian(xlim = range(dcc$log_mass), \n                  ylim = range(dcc$kcal.per.g)) +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\n\nFinally, we’re ready to fit with both predictors included in the “joint model.” Here’s the statistical formula:\n\\[\\begin{align*}\n\\text{kcal.per.g}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i   & = \\alpha + \\beta_1 \\text{neocortex.perc}_i + \\beta_2 \\log (\\text{mass}_i) \\\\\n\\alpha  & \\sim \\operatorname{Normal}(0, 100) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\beta_2 & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\sigma  & \\sim \\operatorname{Cauchy}(0, 1).\n\\end{align*}\\]\n\nCodeb5.7 <- \n  brm(data = dcc, \n      family = gaussian,\n      kcal.per.g ~ 1 + neocortex.perc + log_mass,\n      prior = c(prior(normal(0, 100), class = Intercept),\n                prior(normal(0, 1), class = b),\n        prior(cauchy(0, 1), class = sigma)), \n    iter = 4000, warmup = 2000, chains = 4, cores = 4,\n      control = list(adapt_delta = 0.999),\n      seed = 5,\n      file = \"fits/b05.07\")\n\n\n\nCodeprint(b5.7, digits = 3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: kcal.per.g ~ 1 + neocortex.perc + log_mass \n   Data: dcc (Number of observations: 17) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nPopulation-Level Effects: \n               Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept        -1.093     0.579   -2.243    0.075 1.000     3653     3994\nneocortex.perc    0.028     0.009    0.010    0.046 1.001     3515     3871\nlog_mass         -0.097     0.028   -0.151   -0.041 1.003     3274     3638\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.140     0.031    0.096    0.214 1.001     3159     3923\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nMake Figure 5.7, bottom left.\n\nCodend <- \n  tibble(neocortex.perc = 54:80 %>% as.double(),\n         log_mass       = mean(dcc$log_mass))\n\np1 <-\n  b5.7 %>%\n  fitted(newdata = nd, \n         probs = c(.025, .975, .25, .75)) %>%\n  as_tibble() %>%\n  bind_cols(nd) %>% \n\n  ggplot(aes(x = neocortex.perc, y = Estimate)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = Q25, ymax = Q75),\n              stat = \"identity\",\n              fill = \"firebrick4\", color = \"firebrick4\", alpha = 1/5, size = 1/2) +\n  geom_point(data = dcc, \n             aes(y = kcal.per.g),\n             size = 2, color = \"firebrick4\") +\n  ylab(\"kcal.per.g\") +\n  coord_cartesian(xlim = range(dcc$neocortex.perc), \n                  ylim = range(dcc$kcal.per.g))\n\n\nNow make Figure 5.7, bottom right, and combine.\n\nCodend <- \n  tibble(log_mass       = seq(from = -2.5, to = 5, length.out = 30),\n         neocortex.perc = mean(dcc$neocortex.perc))\n\np2 <-\n  b5.7 %>%\n  fitted(newdata = nd,\n         probs = c(.025, .975, .25, .75)) %>%\n  as_tibble() %>%\n  bind_cols(nd) %>% \n\n  ggplot(aes(x = log_mass, y = Estimate)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = Q25, ymax = Q75),\n              stat = \"identity\",\n              fill = \"firebrick4\", color = \"firebrick4\", alpha = 1/5, size = 1/2) +\n  geom_point(data = dcc, \n             aes(y = kcal.per.g),\n             size = 2, color = \"firebrick4\") +\n  ylab(\"kcal.per.g\") +\n  coord_cartesian(xlim = range(dcc$log_mass), \n                  ylim = range(dcc$kcal.per.g))\n\n(p1 | p2) &\n  theme_bw() &\n  theme(panel.grid = element_blank())\n\nError in p1 | p2: operations are possible only for numeric, logical or complex types\n\n\n\nWhat [this regression model did was] ask if species that have high neocortex percent for their body mass have higher milk energy. Likewise, the model [asked] if species with high body mass for their neocortex percent have higher milk energy. Bigger species, like apes, have milk with less energy. But species with more neocortex tend to have richer milk. The fact that these two variables, body size and neocortex, are correlated across species makes it hard to see these relationships, unless we statistically account for both. (pp. 140–141, emphasis in the original)\n\n\n25.1.1 DR: Simulating the fork, considering if ‘random effects control’ addresses it\nHere is an example relevant to my recent/ongoing work.\nLet\n\\(Z\\): A measure of ’which “chunk” (recruitment group) people are in \\(X\\): … ’Which treatment (video) they got” \\(Y\\): Their attitude towards 80k\nWe are interested in the impact of the treatment on their attitude, i.e., whether \\(X \\rightarrow Y\\) is ‘part of the causal model.’\nBut suppose the true causal model is ‘the fork’, as seen in the picture below (from McElreath):\n\nHere, if we ‘stratify on’ \\(Z\\), i.e., we look ‘within each chunk’, we should see no relationship between treatments and 80k-attitudes (as \\(X \\perp Y | Z\\)). However, if we pool across all chunks, and some chunks tend to have some treatments more than others, we may falsely diagnose an ‘impact of X on Y’.\nBut what if we ‘control for X’ in a (linear or generalized linear) model:\n\nWith ‘fixed effect’ adjustments (‘dummies’) for each chunk? or\n\nWith ‘chunk’ as a random effect?\n\n(DR: I’m pretty sure this is equivalent, for our inferential purposes, to a simulation of a model where \\(X\\) does impact Y, but \\(Z\\) impacts both, leading to what econometricians call ‘omitted variable bias’ if we leave out \\(Z\\).)\n\nDrawing and setting up the ‘fork’ DAGlibrary(patchwork)\n\n\ngg_dag <- function(d) {\n  \n  d %>% \n    ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n    geom_dag_point(color = \"firebrick\", alpha = 1/4, size = 10) +\n    geom_dag_text(color = \"firebrick\") +\n    geom_dag_edges(edge_color = \"firebrick\") +\n    scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n    scale_y_continuous(NULL, breaks = NULL, expand = c(0.2, 0.2)) +\n    theme_bw() +\n    theme(panel.grid = element_blank())\n  \n}\n\n\ndag_coords <-\n  tibble(name = c(\"X\", \"Y\", \"Z\", \"u\"),\n         x    = c(1, 3, 2, 2),\n         y    = c(2, 2, 1, 2))\n\nfork <-\n  ggdag::dagify(X ~ Z,\n         Y ~ Z,\n         coords = dag_coords) %>%\n  gg_dag()\n\nfork_descendant <-\n  ggdag::dagify(Y ~ u,\n         Z ~ u,\n    X ~Z,\n         coords = dag_coords) %>%\n  gg_dag()\n\nfork + fork_descendant\n\n\n\n\nSuppose the DAG above on the left holds, as described above.\n\n\n\n\n\n\nThe DAG on the right is probably closer to what we really mean but it seems equivalent.\n\n\n\n\n\nThe DAG on the right is probably closer to what we really mean; the ‘recruit chunk’ (\\(Z\\)) is not itself causing attitudes to 80k (\\(Y\\)); recruitment into this chunk is driven by some unobservable term \\(u\\), which has a direct impact on this attitude. But I suspect this will be equivalent, with \\(Z\\) thus acting as a ‘proxy for \\(u\\)’.\nWe should explore this with daggitty below\n\nCodefork <- dagitty( \"dag {\nZ -> X\nZ -> Y\n}\")\n\nadjustmentSets( fork , exposure=\"X\" , outcome=\"Y\" )\n\n{ Z }\n\nCodefork_descendant <-\n  dagitty( \"dag {\nZ -> X\nu -> Y\nu -> Z\n}\")\n\nadjustmentSets( fork_descendant , exposure=\"X\" , outcome=\"Y\" )\n\n{ u }\n{ Z }\n\n\nNote the adjustment set {Z} works in both cases.\n\n\n\nFollowing the code structure Kurz for the present case. I’ll treat the outcome ‘\\(Y\\)’ (attitude to 80k) and the video treatment (\\(X\\)) as continuous for now.17 However, to enable meaningful random-effects mixed models, we need to make the ‘recruitment chunk’ (\\(Z\\)) categorical.\n\nNow simulate data consistent with that DAG.\n\n\nCoden <- 10000 #DR: I increased this to be a more reliable simulation\n\nset.seed(5)\n\nfork_sample <- \n  tibble(u = runif(n, min = 0, max = 1)) %>%   #latent love of 80k thing\n  mutate(\n    Z =  case_when(\n      u < 0.2 ~  sample(c(1,2,3,4), 1, prob = c(0.85, 0.05, 0.05, 0.05)),\n      u > 0.2 & u < 0.4 ~ sample(c(1,2,3,4), 1, prob = c(0.05, 0.85, 0.05, 0.05)),\n      u > 0.4 & u < 0.8 ~ sample(c(1,2,3,4), 1, prob = c(0.05, 0.05, 0.85, 0.05)),\n      u > 0.8 ~  sample(c(1,2,3,4), 1, prob = c(0.05, 0.05, 0.05, 0.85))\n    )\n  ) %>% \n  mutate(X = rnorm(n, mean = 1.5*Z-2, sd = 1)) %>%  \n  mutate(Y = rnorm(n, mean = 1.5*u, sd = 1))  \n\n\nPairs plots\n\nCodepairs(fork_sample, col = \"firebrick4\")\n\n\n\n\nNext we estimate three models: the naive model, the model with a fixed-effect for Z, and the model with a random effect for Z.\n\nCodenaive_on_fork <- \n  brm(data = fork_sample, \n      family = gaussian,\n      Y ~ 1 + X,\n      prior = c(prior(normal(0, 0.2), class = Intercept),\n                prior(normal(0, 0.5), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 5,\n      sample_prior = T,\n      file = \"fits/naive_on_fork\")\n\nfe_control_on_fork <- \n  update(naive_on_fork, \n    newdata = fork_sample,\n         formula = Y ~ 1 + X + Z,\n         seed = 5,\n         file = \"fits/fe_control_on_fork\")\n\n\nfe_control_on_fork <- \n  update(naive_on_fork, \n    newdata = fork_sample,\n         formula = Y ~ 1 + X + Z,\n         seed = 5,\n         file = \"fits/fe_control_on_fork\")\n\n\nre_control_on_fork <- brm(\n  formula = Y ~ 1 + X + (1 | Z),\n  family = gaussian(),\n  data = fork_sample,\n  control = list(adapt_delta = .99, max_treedepth = 15),\n  prior =  c(set_prior(\"normal(0, 1)\", class = \"Intercept\"),\n                 set_prior(\"normal(0, 1)\", class = \"b\"),\n                 set_prior(\"exponential(10)\", class = \"sd\")),\n  chains = 4,\n  cores = 4,\n  iter = 2000,\n  warmup = 500,\n       #backend = \"cmdstanr\",\n       #threads = threading(4),\n  seed = 1010,\n  silent = 2 #cmdstanr seems to ignore requests for silence\n  )\n\n\nCompare the coefficients.\n\nCodefixef(naive_on_fork) %>% round(digits = 2)\n\n          Estimate Est.Error Q2.5 Q97.5\nIntercept     0.40      0.02 0.37  0.43\nX             0.18      0.01 0.17  0.19\n\nCodefixef(fe_control_on_fork) %>% round(digits = 2)\n\n          Estimate Est.Error  Q2.5 Q97.5\nIntercept    -0.29      0.04 -0.36 -0.22\nX             0.01      0.01 -0.01  0.03\nZ             0.39      0.02  0.36  0.43\n\nCodefixef(re_control_on_fork) %>% round(digits = 2)\n\n          Estimate Est.Error  Q2.5 Q97.5\nIntercept     0.66      0.20  0.26  1.05\nX             0.01      0.01 -0.01  0.03"
  },
  {
    "objectID": "chapters/5_spurious_waffles.html#categorical-variables",
    "href": "chapters/5_spurious_waffles.html#categorical-variables",
    "title": "25  Ch 5. The Many Variables & The Spurious Waffles",
    "section": "Categorical variables",
    "text": "Categorical variables\nDR: This section is rather straightforward. The key – and highly useful – insight: Remove the intercept when modeling with categorical features.\nThis yields a specific estimate for each category or combination of categories. Why?\n\nThere is no ‘base category’, making interpretation easier.\nIt avoids the challenges (particular to Bayesian approaches) of including an ‘additive adjustment coefficient relative to the base group’, which naturally (and misleadingly) implies more variance in the non-base groups.\n\nDownside: the coefficients can’t be interpreted as ‘effects’ or adjustments, potentially making (direct) interpretation statistical inference more difficult in some contexts.\nRemedy to this: Explicitly model/test these differences through simulation, easily done in a Bayesian framework\n\n\n\n\n\n\nDR question: Does this apply to frequentist analysis too?\n\n\n\n\n\nIs it Bayesian specific? How does this relate to ‘contrast-coding’?\n\n\n\n\nMany readers will already know that variables like this, routinely called factors, can easily be included in linear models. But what is not widely understood is how these variables are included in a model… Knowing how the machine works removes a lot of this difficulty. (p. 153, emphasis in the original)\n\n\nWe’ll practice with milk.\n\n\nCodelibrary(rethinking)\ndata(milk)\nd <- milk\n\nrm(milk)\ndetach(package:rethinking, unload = T)\nlibrary(brms)\n\n\n\nWith the tidyverse, we can peek at clade with distinct() in the place of base R unique().\n\n\nCoded %>%\n  distinct(clade)\n\n             clade\n1    Strepsirrhine\n2 New World Monkey\n3 Old World Monkey\n4              Ape\n\n\n\nAs clade has 4 categories, let’s use if_else() to convert these to 4 dummy variables.\n\n\nCoded <- \n  d %>%\n  mutate(clade_nwm = if_else(clade == \"New World Monkey\", 1, 0),\n         clade_owm = if_else(clade == \"Old World Monkey\", 1, 0),\n         clade_s   = if_else(clade == \"Strepsirrhine\", 1, 0),\n         clade_ape = if_else(clade == \"Ape\", 1, 0))\n\n\n\nNow we’ll fit the model with three of the four dummies. In this model, clade_ape is the reference category captured by the intercept.\n\n\nCodeb5.16 <- \n  brm(data = d, \n      family = gaussian,\n      kcal.per.g ~ 1 + clade_nwm + clade_owm + clade_s,\n      prior = c(prior(normal(.6, 10), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(uniform(0, 10), class = sigma)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5,\n      file = \"fits/b05.16\")\n\n\n\nCodeprint(b5.16)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: kcal.per.g ~ 1 + clade_nwm + clade_owm + clade_s \n   Data: d (Number of observations: 29) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.55      0.04     0.46     0.63 1.00     4607     4237\nclade_nwm     0.17      0.06     0.05     0.29 1.00     5072     4453\nclade_owm     0.24      0.07     0.10     0.37 1.00     5092     4527\nclade_s      -0.04      0.07    -0.18     0.10 1.00     5067     4500\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.13      0.02     0.10     0.17 1.00     4846     4300\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n25.1.2 Adding regular predictor variables.\n\nIf we wanted to fit the model including perc.fat as an additional predictor, the basic statistical formula would be\n\n\\[\n\\mu_i = \\alpha + \\beta_\\text{clade_nwm} \\text{clade_nwm}_i + \\beta_\\text{clade_owm} \\text{clade_owm}_i + \\beta_\\text{clade_s} \\text{clade_s}_i + \\beta_\\text{perc.fat} \\text{perc.fat}_i.\n\\]\n\nThe corresponding formula argument within brm() would be kcal.per.g ~ 1 + clade_nwm + clade_owm + clade_s + perc.fat.\n\n\n25.1.3 Another approach: Unique intercepts.\n\n“Another way to conceptualize categorical variables is to construct a vector of intercept parameters, one parameter for each category” (p. 158). Using the code below, there’s no need to transform d$clade into d$clade_id. The advantage of this approach is the indices in the model summary are more descriptive than a[1] through a[4].\n\n\nCodeb5.16_alt <- \n  brm(data = d, \n      family = gaussian,\n      kcal.per.g ~ 0 + clade,\n      prior = c(prior(normal(.6, 10), class = b),\n                prior(uniform(0, 10), class = sigma)),\n      iter = 2000, warmup = 500, chains = 4, cores = 4,\n      seed = 5,\n      file = \"fits/b05.16_alt\")\n\n\n\nCodeprint(b5.16_alt)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: kcal.per.g ~ 0 + clade \n   Data: d (Number of observations: 29) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ncladeApe                0.54      0.04     0.46     0.63 1.00     6979     3927\ncladeNewWorldMonkey     0.71      0.04     0.63     0.80 1.00     7110     4200\ncladeOldWorldMonkey     0.79      0.05     0.69     0.89 1.00     7056     4131\ncladeStrepsirrhine      0.51      0.06     0.39     0.63 1.00     7116     4299\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.13      0.02     0.10     0.18 1.00     4960     4064\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nSee? This is much easier than trying to remember which one was which in an arbitrary numeric index.\n\nRethinking: Continuous countries. – DR: See the famous ‘log NAICs’ case in economics, where a prominent (?) paper’s results used the logarithm of an arbitrary (‘North American Industry Classification System’) code in the model."
  },
  {
    "objectID": "chapters/5_spurious_waffles.html#further-notes-on-the-use-of-dags-etc",
    "href": "chapters/5_spurious_waffles.html#further-notes-on-the-use-of-dags-etc",
    "title": "25  Ch 5. The Many Variables & The Spurious Waffles",
    "section": "Further notes on the use of DAGS etc",
    "text": "Further notes on the use of DAGS etc"
  }
]