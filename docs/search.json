[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rethink Priorities on surveys, experiments, and data analysis; methodology, protocols and templates",
    "section": "",
    "text": "As always, focus on ‘what repeatedly comes up in our work’, linking the practical cases↩︎"
  },
  {
    "objectID": "chapters/coding_data.html",
    "href": "chapters/coding_data.html",
    "title": "2  Coding and data management",
    "section": "",
    "text": "Integrate further content from getting, cleaning, and using data (Reinstein,"
  },
  {
    "objectID": "chapters/coding_data.html#coding-and-organisational-issues",
    "href": "chapters/coding_data.html#coding-and-organisational-issues",
    "title": "2  Coding and data management",
    "section": "2.1 Coding and organisational issues",
    "text": "2.1 Coding and organisational issues\n\nData protection (e.g., EA Survey data pre-2021 is not publicly shareable!)\nGood data management\nReproducability\nGit and github\nR and Rmd\nHow to leave comments and collaborate?\ntrackdown to convert to Gdoc for feedback\nFolder structure, use of packages; esp Renv\nFunctions etc pulled from dr-rstuff repo\nI (DR) love lower_snake_case\n\n\n\n\nAutomation and ‘dynamic documents’\n‘Soft-code’ as much as possible to avoid conflicting versions when data updates, and to make everything reproduceable and transparent\nInline code in Rmd is great but it can be a double-edged sword.\nSometimes its better to ‘do the important and complicated coding’ in a chunk before this, not in the inline code itself because\n\nthe ‘bookdown’ doesn’t show the code generating the inline computation … so a separate chunk makes it more transparent for external readers\ninline code isn’t spaced well and its hard to read and debug.\n\n\n\n2.1.1 Data management\n\nTrack it from its ‘source’; use API to grab directly from Qualtrics (etc.) if possible\nA main.R file in the root directory should run everything\nData import; external ‘dictionary’ can be helpful (see, e.g., here for EAS integrated with Google sheet; R code here brings it in\nimport, cleaning, variable creation separate from analysis (unless its a very ‘one-off-for-analysis’ thing)\n\nimport and cleaning in .R rather than .Rmd perhaps\n\n‘raw’ data in separate folder from ‘munged’ data\ncodebook package – make a codebook\nminimize ‘versions’ of the data frames … code and use ‘filter objects’ instead\n\nsee ‘lists of filters’ but actually defining the filter with quo() seems better.\n\n\n\n\n\n\nSource Code\n# Coding and data management {#coding}\n\nIntegrate further content from [getting, cleaning, and using data](https://daaronr.github.io/metrics_discussion/data-sci.html) (Reinstein,\n\n## Coding and organisational issues\n\n- Data protection (e.g., EA Survey data pre-2021 is not publicly shareable!)\n\n- Good data management\n\n- Reproducability\n\n- Git and github\n\n- R and Rmd\n\n- How to leave comments and collaborate?\n\n- `trackdown` to convert to Gdoc for feedback\n\n- Folder structure, use of packages; esp `Renv`\n\n- Functions etc pulled from `dr-rstuff` repo\n\n- I (DR) love `lower_snake_case`\n\n\\\n\n### Automation and 'dynamic documents' {-}\n\n'Soft-code' as much as possible to avoid conflicting versions when data updates, and to make everything reproduceable and transparent\n\n[Inline code in Rmd](https://bookdown.org/yihui/rmarkdown-cookbook/r-code.html) is great but it can be a double-edged sword.\n\nSometimes its better to 'do the important and complicated coding' in a chunk before this, not in the inline code itself because\n\n- the 'bookdown' doesn't show the *code* generating the inline computation ... so a separate chunk makes it more transparent for external readers\n\n- inline code isn't spaced well and its hard to read and debug.\n\n\n### Data management\n\n- Track it from its 'source'; use API to grab directly from Qualtrics (etc.) if possible\n\n- A `main.R` file in the root directory should run everything\n\n- Data import; external 'dictionary' can be helpful (see, e.g., [here](https://docs.google.com/spreadsheets/d/1dWy-CZxd9lzx0bLZ5ntmCSmwGPTrwjGcKpY4ORLom8E/edit#gid=0) for EAS integrated with Google sheet; R code [here](https://github.com/rethinkpriorities/ea-data/blob/master/build/fmt_label_with_dic_dhj_ok.R) brings it in\n\n- import, cleaning, variable creation separate from analysis (unless its a very 'one-off-for-analysis' thing)\n   - import and cleaning in `.R` rather than `.Rmd` perhaps\n\n- 'raw' data in separate folder from 'munged' data\n\n- `codebook` package -- make a codebook\n\n- minimize 'versions' of the data frames ... code and use 'filter objects' instead\n  - see ['lists of filters'](https://daaronr.github.io/metrics_discussion/data-sci.html#building-results-based-on-lists-of-filters-of-the-data-set) but actually defining the filter with `quo()` seems better."
  },
  {
    "objectID": "chapters/presentation_method_discussion.html",
    "href": "chapters/presentation_method_discussion.html",
    "title": "3  Presentation, labeling and visualisations",
    "section": "",
    "text": "DR: I’m slightly concerned about misinterpretations of the smoothed thing though … those shaded regions maybe should not be interpreted as CI?↩︎"
  },
  {
    "objectID": "chapters/survey_designs_methods.html",
    "href": "chapters/survey_designs_methods.html",
    "title": "4  Survey design and item development",
    "section": "",
    "text": "This section will cover specific ‘survey’ related content, as well as some content that overlaps with experiments and trials.\nMore technical and involved discussion of overlapping content will often be deferred to the experiments and trials section, with placeholders and links.\nSchwarcz et al. (2007)"
  },
  {
    "objectID": "chapters/survey_designs_methods.html#willem-sleegers-to-incorporate",
    "href": "chapters/survey_designs_methods.html#willem-sleegers-to-incorporate",
    "title": "4  Survey design and item development",
    "section": "\n4.1 (Willem Sleegers, to incorporate)",
    "text": "4.1 (Willem Sleegers, to incorporate)\nitem development\n\nCodeknitr::include_url(\"https://willemsleegers.github.io/how-to-science/survey-design.html#item-development\")"
  },
  {
    "objectID": "chapters/survey_designs_methods.html#reinstein-misc-notes-to-incorporate",
    "href": "chapters/survey_designs_methods.html#reinstein-misc-notes-to-incorporate",
    "title": "4  Survey design and item development",
    "section": "Reinstein misc notes to incorporate",
    "text": "Reinstein misc notes to incorporate\n\nCodeknitr::include_url(\"https://daaronr.github.io/metrics_discussion/surveys.html\")"
  },
  {
    "objectID": "chapters/survey_designs_methods.html#suggested-sections",
    "href": "chapters/survey_designs_methods.html#suggested-sections",
    "title": "4  Survey design and item development",
    "section": "\n4.2 Suggested sections…",
    "text": "4.2 Suggested sections…\n\nHow to ask good survey questions\nAvoiding pitfalls\nConstructing reliable indices and scales\n\nSampling issues and representativeness\n\nElsey on “Mr P”\nReinstein on ‘sampling’ and ‘measuring a rare population’ with reference to the EA survey\n\n\nImplementation platforms and issues\nSurvey design tools (IT)\n\n\n\n\n\nSource Code\n# Survey design and item development {#surveydesign}\n\nThis section will cover specific 'survey' related content, as well as some content that overlaps with experiments and trials.\n\nMore technical and involved discussion of overlapping content will often be deferred to the [experiments and trials](#expt_trial) section, with placeholders and links.\n\n@schwarcz2007\n\n\n::: {.alert .alert-secondary}\n\n\nLink or incorporate William Elsey's work [HERE](https://willemsleegers.github.io/how-to-science/survey-design.html#item-development),\n\nand [Reinstein's sloppy notes](https://daaronr.github.io/metrics_discussion/surveys.html); see embeds below\n\n\n:::\n\n\n## (Willem Sleegers, to incorporate)\n\n[item development](https://willemsleegers.github.io/how-to-science/survey-design.html#item-development)\n\n```{r }\n\nknitr::include_url(\"https://willemsleegers.github.io/how-to-science/survey-design.html#item-development\")\n\n```\n\n## Reinstein misc notes to incorporate {-}\n\n```{r }\n\nknitr::include_url(\"https://daaronr.github.io/metrics_discussion/surveys.html\")\n\n```\n\n## Suggested sections...\n\n- How to ask good survey questions\n\n- Avoiding pitfalls\n\n- Constructing reliable indices and scales\n\n- Sampling issues and representativeness\n   - Elsey on \"Mr P\"\n   - Reinstein on ['sampling'](https://daaronr.github.io/metrics_discussion/surveys.html#survey-samplingintake) and ['measuring a rare population'](https://daaronr.github.io/metrics_discussion/surveys.html#jazz-case) with reference to the EA survey\n\n\n- Implementation platforms and issues\n\n- Survey design tools (IT)\n\n\n\n\n\nSchwarcz, Sandra, Hilary Spindler, Susan Scheer, Linda Valleroy, and Amy Lansky. 2007. “Assessing Representativeness of Sampling Methods for Reaching Men Who Have Sex with Men: A Direct Comparison of Results Obtained from Convenience and Probability Samples.” AIDS and Behavior 11 (4): 596. https://doi.org/ctgppb."
  },
  {
    "objectID": "chapters/experiments_trials_design.html",
    "href": "chapters/experiments_trials_design.html",
    "title": "5  Experiments: Qualitative and implementation design issues",
    "section": "",
    "text": "See Reinstein notes on ‘why run an experiment’ (somewhat more relevant to ‘lab experiments’)\nSee informal ‘practitioners’ discussion below, from ‘EA market testing’"
  },
  {
    "objectID": "chapters/experiments_trials_design.html#proposed-structure-of-section",
    "href": "chapters/experiments_trials_design.html#proposed-structure-of-section",
    "title": "5  Experiments: Qualitative and implementation design issues",
    "section": "5.1 Proposed structure of section",
    "text": "5.1 Proposed structure of section\n\nBasic design choices and terminology\nTypes of experiments: ‘lab and field’\nFormulating hypotheses and ‘learning and adaptation goals’\nHypothesis testing versus ‘reinforcement learning’ goals\nDesign concerns and pitfalls\n\nConfounded designs\nAttrition and failed randomization\n‘Demand effects’\nNaturalness versus cleanliness\n\nPractical and implementation issues\n\nSurvey and experiment platforms (see ‘surveys’) chapter\nField experiments, A/B and lift tests, and marketing trials\nDesigning, coding, and implementing experiments: IT issues\nFailure and success modes\nCapturing outcome data\nPre-registration and pre-analysis plans\n… See Reinstein discussion notes on the benefits and costs here"
  },
  {
    "objectID": "chapters/experiments_trials_design.html#treatment-assignment-blocking-randomization-etc",
    "href": "chapters/experiments_trials_design.html#treatment-assignment-blocking-randomization-etc",
    "title": "5  Experiments: Qualitative and implementation design issues",
    "section": "6.1 ‘Treatment’ assignment (blocking, randomization, etc)",
    "text": "6.1 ‘Treatment’ assignment (blocking, randomization, etc)\n\n6.1.1 Adaptive, sequential and dynamic designs\n\nReinstein notes and links here\nElsey: http://www.sequentialtesting.com/"
  },
  {
    "objectID": "chapters/experiments_trials_design.html#planning-diagnosing-and-adjusting-a-design",
    "href": "chapters/experiments_trials_design.html#planning-diagnosing-and-adjusting-a-design",
    "title": "5  Experiments: Qualitative and implementation design issues",
    "section": "6.2 Planning, diagnosing and adjusting a design",
    "text": "6.2 Planning, diagnosing and adjusting a design\n\n6.2.1 Specifying models, hypotheses, and statistical testing and inference approaches\n(Just a brief here, this will be covered in more detail in the sections on statistical inference, testing, and analysis"
  },
  {
    "objectID": "chapters/experiments_trials_design.html#power-analyses-and-other-diagnosands",
    "href": "chapters/experiments_trials_design.html#power-analyses-and-other-diagnosands",
    "title": "5  Experiments: Qualitative and implementation design issues",
    "section": "6.3 Power analyses (and other ‘diagnosands’)1",
    "text": "6.3 Power analyses (and other ‘diagnosands’)1\n\n6.3.1 Key resources and explainers\n(‘Curate link’ tools we found useful)\nReinstein notes and code\n\n\n6.3.2 See ‘Rethink_Priorities_Power_analysis_framework_2’ in appendix\nThis is a proposal and tools for a ‘path to do’ power testing (by Jamie Elsey; David Reinstein will weigh in too). I think this first focuses on a frequentist approach, but it’s likely to also introduce a Bayesian approach.\n\nDR: I’m putting it into an appendix to the bookdown for now. We may incorporate much of that into the present section.\nAn alternative to an ‘appendix’ is a standalone page which we can link here. This can sometimes be helpful; it allows a different format, may save time on knitting, may allow that other page to be better integrated elsewhere.\nBut for now, I’ll make it a bookdown appendix, to help get us all used to the current format and avoid disagregation.\n\n\n\n\n\nSource Code\n# Experiments: Qualitative and implementation design issues {#expt-qual-imp}\n\n- See [Reinstein notes on 'why run an experiment'](https://daaronr.github.io/metrics_discussion/why-experiment-design.html#why-run-an-experiment-or-study) (somewhat more relevant to 'lab experiments')\n\n- See informal 'practitioners' discussion [below](#eamt-qual), from 'EA market testing'\n\n::: {.alert .alert-secondary}\n \n\nNote that much 'experiment and trial' relevant content is covered in the [surveys](#surveys) section. We will put in placeholders and cross-link.\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Some quantitative issues\n\n- 'Treatment' assignment (blocking, randomization, etc)\n- Adaptive, sequential and dynamic designs\n- Planning, diagnosing and adjusting a design\n- Power analyses (and other 'diagnosands')\n\n:::\n\n\n## Proposed structure of section\n\n- Basic design choices and terminology\n\n- Types of experiments: 'lab and field'\n\n- Formulating hypotheses and 'learning and adaptation goals'\n\n- Hypothesis testing versus 'reinforcement learning' goals\n\n- Design concerns and pitfalls\n\n  - Confounded designs\n\n  - Attrition and failed randomization\n\n  - 'Demand effects'\n\n  - Naturalness versus cleanliness\n\n- Practical and implementation issues\n\n    - Survey and experiment platforms (see '[surveys](#surveys)') chapter\n\n    - Field experiments, A/B and lift tests, and marketing trials\n\n    - Designing, coding, and implementing experiments: IT issues\n\n    - Failure and success modes\n\n    - Capturing outcome data\n\n    - Pre-registration and pre-analysis plans\n\n    - ... See Reinstein discussion notes on the benefits and costs [here](https://daaronr.github.io/metrics_discussion/quant-design-power.html#the-benefits-and-costs-of-pre-registration-a-typical-discussion)\n\n# Design: quantitative issues {#expt-quant}\n\n::: {.alert .alert-secondary}\n \n\nNote that much 'experiment and trial' relevant content is covered in the [surveys](#surveys) section. We will put in placeholders and cross-link.\n\n \n:::\n\n## 'Treatment' assignment (blocking, randomization, etc)\n\n\n### Adaptive, sequential and dynamic designs\n\n- [Reinstein notes and links here](https://daaronr.github.io/metrics_discussion/quant-design-power.html#quant_design_power)\n\n- Elsey: http://www.sequentialtesting.com/\n\n\n## Planning, diagnosing and adjusting a design\n\n### Specifying models, hypotheses, and statistical testing and inference approaches\n\n(Just a brief here, this will be covered in more detail in the sections on statistical inference, testing, and analysis\n\n## Power analyses (and other 'diagnosands')^[Note: this will very much overlap the discussion of power analysis for *surveys*. Perhaps we put the simpler introduction there, and the more involved details here?\n]\n\n\n### Key resources and explainers\n\n('Curate link' tools we found useful)\n\n[Reinstein notes and code](https://daaronr.github.io/metrics_discussion/power.html)\n\n\n\n### See ['Rethink_Priorities_Power_analysis_framework_2' in  appendix](#power-workflow)\n\nThis is a proposal and tools for a 'path to do' power testing (by Jamie Elsey; David Reinstein will weigh in too). I think this first focuses on a frequentist approach, but it's likely to also introduce a Bayesian approach.\n\n::: {.alert .alert-secondary}\n \n\nDR: I'm putting it into an appendix to the bookdown for now. We may incorporate much of that into the present section.\n\nAn alternative to an 'appendix' is a standalone page which we can link here. This can sometimes be helpful; it allows a different format, may save time on knitting, may allow that other page to be better integrated elsewhere.\n\nBut for now, I'll make it a bookdown appendix, to help get us all used to the current format and avoid disagregation.\n\n \n:::"
  },
  {
    "objectID": "chapters/qualitative-design-issues_plus.html",
    "href": "chapters/qualitative-design-issues_plus.html",
    "title": "6  “Qualitative” design issues",
    "section": "",
    "text": "Here we link to (and can help build) the ‘EA Market testing Gitbook’ for building a non-technical and applied discussion. We may add more technical and detailed material on these issues below (in the present bookdown).\n\n‘EA Market testing gitbook’"
  },
  {
    "objectID": "chapters/qualitative-design-issues_plus.html#further-technical-discussion-and-code-can-be-added-here",
    "href": "chapters/qualitative-design-issues_plus.html#further-technical-discussion-and-code-can-be-added-here",
    "title": "6  “Qualitative” design issues",
    "section": "6.2 Further technical discussion and code can be added here",
    "text": "6.2 Further technical discussion and code can be added here\n\n\n\n\nSource Code\n# \"Qualitative\" design issues {-#eamt-qual}\n\n\n## 'EA market testing' resource on qualitative design issues\n\n::: {.alert .alert-secondary}\n\nHere we link to (and can help build) the ['EA Market testing Gitbook'](https://effective-giving-marketing.gitbook.io/untitled/methodological-discussion/qualitative-design-issues) for building a non-technical and applied discussion.\nWe may add more technical and detailed material on these issues below (in the present bookdown).\n\n:::\n\n**'EA Market testing gitbook'**\n\n\n## Further technical discussion and code can be added here"
  },
  {
    "objectID": "chapters/basic_stats.html",
    "href": "chapters/basic_stats.html",
    "title": "\n7  Models, Testing, Inference, Descriptives\n",
    "section": "",
    "text": "1"
  },
  {
    "objectID": "chapters/basic_stats.html#conceptual",
    "href": "chapters/basic_stats.html#conceptual",
    "title": "\n7  Models, Testing, Inference, Descriptives\n",
    "section": "\n8.1 Conceptual discussion of statistics, probability and inference",
    "text": "8.1 Conceptual discussion of statistics, probability and inference\nFrequentist, Bayesian, ‘randomization inference’, ‘likelihood-ist’>\n\nDR: I am not saying this should be a major focus. We probably don’t want to get too deep here. However, if we do end up discussing these issues, I propose we put it or link it here."
  },
  {
    "objectID": "chapters/basic_stats.html#hypothesis-testing-statistical-comparisons-and-inferences",
    "href": "chapters/basic_stats.html#hypothesis-testing-statistical-comparisons-and-inferences",
    "title": "\n7  Models, Testing, Inference, Descriptives\n",
    "section": "\n8.2 Hypothesis testing, statistical comparisons and inferences",
    "text": "8.2 Hypothesis testing, statistical comparisons and inferences\n\n8.2.1 ‘Common statistical tests are linear models’\n\nMany of the ‘univariate’ tests presented below can be extended to multiple-variable models (e.g., regression coefficients).\nFurther discussion, examples, and tables comparing the statistics by Oska Fentem in his Notion here."
  },
  {
    "objectID": "chapters/basic_stats.html#randomization-and-permutation-based-tests-and-inference",
    "href": "chapters/basic_stats.html#randomization-and-permutation-based-tests-and-inference",
    "title": "\n7  Models, Testing, Inference, Descriptives\n",
    "section": "\n8.3 Randomization and permutation-based tests and inference",
    "text": "8.3 Randomization and permutation-based tests and inference\n2\nBasic description: (still looking for the best source for this)\n\nDiscussion of the difference between randomization inference and bootstrapping here\n\n\n\nBootstrapped p-values are about uncertainty over the specific sample of the population you drew, while randomization inference p-values are about uncertainty over which units within your sample are assigned to the treatment.\n\nThe infer package vignette gives a good walk-through; this package is useful for doing these tests (some also recommend the coin package).3\n\nWe use this, and give some explanation, in the 2021 EAS donations post - see bookdown (see folds within)\n\nWe use permutation tests for testing whether the median (and mean) of planned donations exceeded/fell short of the mean for actual donations, but using the data from different years’ surveys (without connected individuals). …\n\nThe code we use for these tests is permalinked here E\nWhy use these techniques?\n\nTractable for testing differences in medians\nFairly easy to explain, fairly intuitive\nDo not depend on strong assumptions about underlying distributions or ‘large sample asymptotics’\nSome statisticians and Econometricians (e.g., Athey and Imbens) argue for their value and robustness; it also seems close to what a lot of ‘data science’ people do (they love simulation)\n\n\nDR concerns about ‘population vs super-population’, possibly misinformed:\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n… we nearly always want to make inferences about the population that the treatment and control groups are taken from (even thinking about a hypothetical super-population), not about the impact on the sampled groups themselves. So, with this in mind, when would I still want to use randomization inference."
  },
  {
    "objectID": "chapters/basic_stats.html#particular-experimetrics-issues",
    "href": "chapters/basic_stats.html#particular-experimetrics-issues",
    "title": "\n7  Models, Testing, Inference, Descriptives\n",
    "section": "\n8.4 Particular ‘experimetrics’ issues",
    "text": "8.4 Particular ‘experimetrics’ issues\nShould we include controls (covariates) in analyzing ‘treatment effects’ from randomized experiments?\n\n“in the conventional sampling paradigm… Controlling for observable heterogeneity using a regression model” is required for the assumptions to be justified with this approach. With the randomisation approach it makes more sense to put data into strata by covariates, analyse within-group experiments and average results.” - (?) Athey and Imbens\n\n:::"
  },
  {
    "objectID": "chapters/basic_stats.html#equivalence-tests-and-simple-frequentist-approaches",
    "href": "chapters/basic_stats.html#equivalence-tests-and-simple-frequentist-approaches",
    "title": "\n7  Models, Testing, Inference, Descriptives\n",
    "section": "\n9.1 Equivalence tests and simple frequentist approaches",
    "text": "9.1 Equivalence tests and simple frequentist approaches\nFrom Wikipedia (accessed 12 Apr 2022)\n\nIn equivalence tests, the null hypothesis is defined as an effect large enough to be deemed interesting, specified by an equivalence bound. The alternative hypothesis is any effect that is less extreme than said equivalence bound. The observed data is statistically compared against the equivalence bounds. If the statistical test indicates the observed data is surprising, assuming that true effects are at least as extreme as the equivalence bounds, a Neyman-Pearson approach to statistical inferences can be used to reject effect sizes larger than the equivalence bounds with a pre-specified Type 1 error rate.\n\n\nSeveral tests exist for equivalence analyses; however, more recently the Two-one-sided t-tests (TOST) procedure has been garnering considerable attention. As outlined below, this approach is an adaptation of the widely known t-test.\n\n\n“A very simple equivalence testing approach is the ‘two-one-sided t-tests’ (TOST) procedure. [Schuirmann, 1987] In the TOST procedure an upper (\\(\\Delta U\\)) and lower (-\\(\\Delta L\\)) equivalence bound is specified based on the smallest effect size of interest (e.g., a positive or negative difference of \\(d = 0.3\\)). Two composite null hypotheses are tested: \\(H_{01}: \\Delta \\leq –\\Delta L\\) and \\(H_{02}: \\Delta \\geq \\Delta U\\) When both these one-sided tests can be statistically rejected, we can conclude that \\(–\\Delta L < \\Delta < \\Delta U\\), or that the observed effect falls within the equivalence bounds … .[Searman et al, 1998]” [Lakens 2017]\n\nSemi-aside: Unresolved discussion of ‘Bayes factors’ (Nik and Reinstein)"
  },
  {
    "objectID": "chapters/basic_stats.html#simple-bayes",
    "href": "chapters/basic_stats.html#simple-bayes",
    "title": "\n7  Models, Testing, Inference, Descriptives\n",
    "section": "\n9.2 Some simple Bayesian testing examples",
    "text": "9.2 Some simple Bayesian testing examples\nWe return to consider a very simple and common case of interest:\n\nWe have a population divided into two treatment conditions (e.g., ‘Impact’ vs ‘Emotion’ language in an email donation appeal)\n\nWe wish to focus on a binary outcome (e.g., ‘whether clicked to donate’), which may be rare in each treatment.\n\nWe want to understand the impact of the treatment condition on the binary outcome, putting probability bounds on our belief about this\n\n\n\n\nThis is a classic example for Bayesian inference. Some good discussions and vignettes:\nDiscussions and walk-throughs\n\nI have a memory that McElreath goes through this, but I cannot find it\nJamie Elsey’s code example: bayes_odds_ratio.R, which uses the brms interface to Stan\nReinstein and Dickerson’s work using bayesian_test_me on ‘dual process’ donation data\n\n\nReady packages\n\n\nbayesAB is easy to apply, allows flexible specification of priors (and has tools to help you visualize these), and generates very clear output and graphs. See vignette\n\nIt (seems to) save the simulation data for you to summarize as you like\nLimitations: Requires raw data for each treatment (not just summarized data), cannot address more than two groups, does not (?) share the ‘raw code doing the work’ for tweaking\n\n\n\nBayesianFirstAid::bayes.prop.test is also easy to use (see vignette), and it works with either vectors or counts.\n\nIt uses a uniform prior, but you can get it to spit out the rjags code and then adjust this\n… or adjust to multiple groups\nThe plots it give you by default are pretty messy, but it also preserves the simulation data, so you could make your own plots"
  },
  {
    "objectID": "chapters/basic_stats.html#simulation_to_bayes",
    "href": "chapters/basic_stats.html#simulation_to_bayes",
    "title": "\n7  Models, Testing, Inference, Descriptives\n",
    "section": "\n9.3 A single framework? “Significance and equivalence testing” with randomization inference/simulation; building to Bayes",
    "text": "9.3 A single framework? “Significance and equivalence testing” with randomization inference/simulation; building to Bayes\n\n\n\nThis is an outline/discussion. I (DR) try to implement and demonstrate this in eamt_data_analysis/oftw_giving_season_upsell_2021_trial/oftw_upsell_input_first_analysis.Rmd\nNote: there may be good ways to hijack all sorts of existing tools, such as the bayesAB package or BayesianFirstAid\n\n\nSuppose we see a ‘small difference’ between treatment groups and it is ‘not significant in standard tests’ (tests not shown here yet).\nHow can we: - Put meaningful bounds on this? - Statistically ‘rule out large effects’? - Do this in the same context as our ‘null hypothesis tests’? - Do and communicate this in a way that is acceptable to a range of audiences?\n(This parallels the analysis done in HERE, which includes dome further explanation of the methods)\n\nI (David) propose taking the following approach:\n\nConstruct (by simulation or analytical formulae) the ‘probability of “some function of our data” given a range of true parameters, i.e., given a range of relevant ’true rates of (relative) incidence’ under each treatment (1/2, 3/4, equal, 4/3, 2, etc). This includes (and perhaps focuses on) the case where ‘the true rates of incidence are equal to one another and equal to the average empirical incidence in our sample.’\n\nWhat “function of our data”?\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nThe exact proportional difference in incidence rates that we see\nA proportional difference in incidence rates as large as we see (in absolute value) or larger (I believe this is the ‘two-tailed frequentist p-value’)\nA proportional difference in incidence rates as large as we see (in absolute value) or smaller (this gives us a sense of ‘how unlikely is a large true effect … can we probabilistically rule out a big difference’)\nA proportional difference in incidence rates as large or larger in favor of the Treatment\nA proportional difference in incidence rates as large as we see in favor of the Control\n… Perhaps similar measures for other statistics such as ‘difference in counts (not proportional), or ’average amount (donated)’ (for the latter, we’d need to consider distinct true distributions of contributions)\n\n\n\n\n\nPlot the above over the parameter range to get a visual picture of the maximum likelihood parameter, and (eyeballing) the relative probability of different ranges\nFor a few important elements of the above, consider the ‘relative likelihood of the data’ under different true rates of incidence (or distributions of donations), for important comparisons such as\n\n\nA relative incidence of 1.5 versus a relative incidence of 1\n\n… If the ratio is very small we might suspect that ’no difference is far more likely than a strong difference in favor of the treatment.\n\nImplicitly considering a ‘flat prior’, integrate (average) and compare important ranges of the above\n\nE.g….\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nthe probability mass of the parameter ‘the incidence is 1.25x or higher in favor of the Treatment’ , versus 1.25x or lower (closer to zero, or higher incidence under the Control)…\n… If the ratio is very small, then, given a fairly flat prior, our posterior should put a low probability on ‘a large difference in favor of the treatment’ …\nAnd similarly, reversing the control and treatment\nAnd similarly, for both extremes relative to a middle incidence range…\n… here, if the ratio is very small, we will say “we can probablistically rule out an effect of 1.5x or larger in either direction”\n\n\n\n\n\nNote:\n\n\n\n\n\n\nNote\n\n\n\n\n\nJamie:\n“To put simply, likelihood is”the likelihood of \\(\\theta\\) having generated D” and posterior is essentially “the likelihood of θ having generated D” further multiplied by the prior distribution of θ. If the prior distribution is flat (or non-informative), likelihood is exactly the same as posterior.”\nSo in essence, if you wanted, you could just put a prior distribution on the differences over which you have simulated differences, and multiply it with your likelihood. To get a proper posterior you would need a continuous distribution for the prior over the parameter space, and probably more simulations across the parameter space as well, to fill in the shape of the posterior. I guess the key difference between what you are suggesting and just running it as a Bayesian binomial regression is that your likelihood function is produced by simulation, whereas the one in the Bayesian regression would be generated through the model assumptions of the regression.\n\n\n\n\nOptionally, repeat the above with a Bayesian tool, considering more than one ‘explicit prior’.\n\nOngoing question: Do we need or want a separate prior for ‘incidence under treatment’ and ‘incidence under control’, or can our prior simply focus on the relative incidence rates?\n\nJamie suggests: Just do a Bayesian analysis and consider the posterior distribution. You can test the sensitivity to different priors."
  },
  {
    "objectID": "chapters/basic_stats.html#empirical-bayes",
    "href": "chapters/basic_stats.html#empirical-bayes",
    "title": "\n7  Models, Testing, Inference, Descriptives\n",
    "section": "\n9.4 ‘Empirical Bayes’",
    "text": "9.4 ‘Empirical Bayes’\nReference: David Robinson “Introduction to empirical bayes examples from baseball statistics” downloaded 28 Mar 2022\n\nEmpirical Bayes estimation, where a beta distribution fit on all observations is then used to improve each individually. What’s great about this method is that as long as you have a lot of examples, you don’t need to bring in prior expectations.\n\nSome data cleaning\n\nCodep_load(Lahman)\n# Filter out pitchers\ncareer <- Batting %>%\nfilter(AB > 0) %>%\nanti_join(Pitching, by = \"playerID\") %>%\ngroup_by(playerID) %>%\nsummarize(H = sum(H), AB = sum(AB)) %>%\nmutate(average = H / AB)\n# Include names along with the player IDs\ncareer <- Master %>%\ntbl_df() %>%\ndplyr::select(playerID, nameFirst, nameLast) %>%\nunite(name, nameFirst, nameLast, sep = \" \") %>%\ninner_join(career, by = \"playerID\") %>%\ndplyr::select(-playerID)\n\n\ncareer_filtered <- career %>%\nfilter(AB > 100) #he uses 500, I think 100 is enough"
  },
  {
    "objectID": "chapters/basic_stats.html#step-1-estimate-a-beta-prior-from-all-your-data",
    "href": "chapters/basic_stats.html#step-1-estimate-a-beta-prior-from-all-your-data",
    "title": "\n7  Models, Testing, Inference, Descriptives\n",
    "section": "\n9.5 Step 1: Estimate a [Beta] prior from all your data",
    "text": "9.5 Step 1: Estimate a [Beta] prior from all your data\nWe set up a likelihood function… probability of the data given different parameters. It sums the log of ‘how likely is each row (batter) given that batting averages are drawn particular true beta distribution, and hits are drawn from this’. At bats are taken as a constant, I guess\n\nCodelibrary(stats4)\np_load(VGAM)\n\nll <- function(alpha, beta) {\nx <- career_filtered$H\ntotal <- career_filtered$AB\n\n-sum(VGAM::dbetabinom.ab(x, total, alpha, beta, log = TRUE)) #Note this is outputting the negative Log likelihood (not subtracting it from the last line)\n# 'how likely\n}\n\n\nNext we search for the ‘most likely parameters’ … the alpha and beta that maximize the above (using a particular algorithm).\n\nCode# maximum likelihood estimation\nm <- mle(ll, start = list(alpha = 1, beta = 10), method = \"L-BFGS-B\",\nlower = c(0.0001, .1))\nab <- coef(m)\nalpha0 <- ab[1]\nbeta0 <- ab[2]"
  },
  {
    "objectID": "chapters/basic_stats.html#step-2-use-that-distribution-as-a-prior-for-each-individual-estimate",
    "href": "chapters/basic_stats.html#step-2-use-that-distribution-as-a-prior-for-each-individual-estimate",
    "title": "\n7  Models, Testing, Inference, Descriptives\n",
    "section": "\n9.6 Step 2: Use that distribution as a prior for each individual estimate",
    "text": "9.6 Step 2: Use that distribution as a prior for each individual estimate\nWith the Beta prior and Binomial distribution, it turns out there is a simple rule for finding the midpoint (MAP?) of the posterior for the mean. Adjust the ‘average’ formula by … taking the numerator (hits) and add \\(\\alpha_0\\) from the Beta prior. Take the denominator (at-bats) and add \\(\\alpha_0 + \\beta_0\\).\nSo if someone got 30 hits out of 100, our midpoint estimate for their true ‘data generating’ batting average is \\(\\frac{30+\\alpha_0}{100+\\alpha_0+\\beta_0}\\)\nWe can do this across all the data, and find the ‘best and worst’ batters:\n\nCodecareer_eb <- career %>%\nmutate(eb_estimate = (H + alpha0) / (AB + alpha0 + beta0))\n\n\ncareer_eb %>% arrange(eb_estimate) %>% head(5)\n\n\n\n\n\n\n\n\n\nname\nH\nAB\naverage\neb_estimate\n\n\nBill Bergen\n516\n3028\n0.17 \n0.178\n\n\nRay Oyler\n221\n1265\n0.175\n0.191\n\n\nJohn Vukovich\n90\n559\n0.161\n0.195\n\n\nJohn Humphries\n52\n364\n0.143\n0.195\n\n\nGeorge Baker\n74\n474\n0.156\n0.196\n\n\n\nCodecareer_eb %>% arrange(-eb_estimate) %>% head(5)\n\n\n\n\n\n\n\n\n\nname\nH\nAB\naverage\neb_estimate\n\n\nRogers Hornsby\n2930\n8173\n0.358\n0.355\n\n\nShoeless Joe Jackson\n1772\n4981\n0.356\n0.35 \n\n\nEd Delahanty\n2597\n7510\n0.346\n0.342\n\n\nBilly Hamilton\n2164\n6283\n0.344\n0.34 \n\n\nWillie Keeler\n2932\n8591\n0.341\n0.338\n\n\n\n\nAs noted (and plotted) in the book, this method tends to ‘shrink’ the averages towards the overall midpoint. The lowest averages are raised a bit, and the highest averages are reduced a bit … with more ‘shrinkage’ where there are fewer observations (at-bats)."
  },
  {
    "objectID": "chapters/ml_modeling.html",
    "href": "chapters/ml_modeling.html",
    "title": "8  Prediction, machine learning, and ‘ml-style modeling’",
    "section": "",
    "text": "See Vignette: Modeling donations in EA Survey data with Tidymodels and workflow"
  },
  {
    "objectID": "chapters/ml_modeling.html#resources-and-use-cases",
    "href": "chapters/ml_modeling.html#resources-and-use-cases",
    "title": "8  Prediction, machine learning, and ‘ml-style modeling’",
    "section": "8.1 Resources and use-cases",
    "text": "8.1 Resources and use-cases\n\nOska ML notes (Notion)\nPredictive models of donations in EA Survey\n\nCode to fit models in this folder\n… uses tidymodels including workflow_set and ‘recipes’ and the parsnip package\n\nGeneral intuitive discussion: “Data Science for Business by Foster Provost and Tom Fawcett (2013)”; DR notes here and here\n\n\n\n\n\nSource Code\n# Prediction, machine learning, and 'ml-style modeling' {-#modeling}\n\n\nSee [Vignette: Modeling donations in EA Survey data with Tidymodels and workflow](modeling_vignettes/eas_ml_modeling_vignette.html) \n\n## Resources and use-cases\n\n- Oska ML notes (Notion)\n\n- [Predictive models of donations in EA Survey](https://rethinkpriorities.github.io/ea_data_public/eas_donations.html#predictive-models)\n   - Code to fit models in [this folder](https://github.com/rethinkpriorities/ea-data/tree/master/analysis/predictive)\n   - ... uses [tidymodels](https://www.tidymodels.org/) including [workflow_set](https://workflowsets.tidymodels.org/) and 'recipes' and the [parsnip](https://www.tidyverse.org/blog/2018/11/parsnip-0-0-1/) package\n\n- General intuitive discussion: \"Data Science for Business by Foster Provost and Tom Fawcett (2013)\"; DR notes [here](https://daaronr.github.io/metrics_discussion/n-ds4bs.html) and [here](https://daaronr.github.io/metrics_discussion/control-ml.html)"
  },
  {
    "objectID": "chapters/time_series_application.html",
    "href": "chapters/time_series_application.html",
    "title": "9  Time Series – application to ‘predict the influence of discrete events’ (and more) on web traffic, signups, etc",
    "section": "",
    "text": "Modeling what drives/effects/predicts …\n\n‘traffic and number of conversions over the days/weeks of this year’ (count data)\nquality of conversions\n\nParticularly in response to discrete events, media coverage, etc.*\n\n* DR: The latter makes this seem more like a causal inference problem (‘what works to boost good applications’) … than a ‘predict the future to plan around it’ problem (as in Pete’s ML notes).\n\n\n“Did any of these events (at different times) seem to increase numbers?*” From which they aim to infer (more loosely)\n“What kinds of events will, in the future, most likely increase numbers?” obviously this involves also assessing what normal variation is, where there’s a trend, seasonality etc."
  },
  {
    "objectID": "chapters/time_series_application.html#petes-notes-below",
    "href": "chapters/time_series_application.html#petes-notes-below",
    "title": "9  Time Series – application to ‘predict the influence of discrete events’ (and more) on web traffic, signups, etc",
    "section": "9.1 PETE’S NOTES BELOW",
    "text": "9.1 PETE’S NOTES BELOW\n\nDR: Not sure what/how to adapt this\n\nIn Machine Learning, Time series problems are problems that involve forecasting (extrapolating) the future based on the information of the past. Typically we have to make a chain of non-independent predictions rather than predict discrete, independent events.\n\nDR: this prediction problem should be distinguished from ‘time series econometrics’, see, e.g., Diebold’s text, which focuses on estimating fundamental structural parameters, and considers forms of ‘causality’.\n\nMost machine learning problems assume that the order of rows don’t matter and that each row is independent of each other. Since time series problems involve time-ordered rows where past events may influence future events, the rows are not independent and this independence assumption is violated.\nAnother key assumption of ML is that the training data is similar to the data being predicted (in this case, future data). This assumption must be true for time series as well.\n\n9.1.1 Decomposition\nYou can decompose a time series into four key parts:\n\nThe trend (T), where the mean is changing over time (e.g., sales generally keep increasing over time)\nSeasonality (S) (e.g., sales are higher in the holiday season)\nA non-seasonal cyclical (C) component (e.g., stock market follows “business cycles”). This is distinct from seasonality as seasonality has a fixed period (e.g., every November), whereas a cycle does not.\nA random component (e)\n\n\nDR: The ‘random component’ could be distinguised or described further; there are various types of ‘random’ terms … shifts, changes in trends, ‘moving average’ one-off terms, ‘autoregressive’ error terms\n\n\n\n9.1.2 Types of decomposition\nThere are two basic types of decomposition – addititive, where y = T + S + C + e and multiplicative, where y = T * S * C * e."
  },
  {
    "objectID": "chapters/time_series_application.html#stationary-vs.-non-stationarity",
    "href": "chapters/time_series_application.html#stationary-vs.-non-stationarity",
    "title": "9  Time Series – application to ‘predict the influence of discrete events’ (and more) on web traffic, signups, etc",
    "section": "9.2 Stationary vs. non-stationarity",
    "text": "9.2 Stationary vs. non-stationarity\nStationary time series (a) do not have a trend and also (b) do not have variance that changes over time. Non-stationary time series’ do have (a) and/or (b). Typically (a) and (b) create problems for modeling, as models have trouble extrapolating these and they tend to violate the assumption that the training data is similar to the data being predicted.\n\nDR: There can be trend-stationary series, that are stationary after including a trend. These are pretty easy to deal with. Note also that nonstationary series can be described as following a ‘random walk’\n\n\nBut I’m also not convinced that this should necessarily be a problem in a prediction problem. If the series is a random walk/nonstationary … we can still use that knowledge to make a decent prediction of where the outcome will be at time T+t given it’s value at time T.\n\n\n9.2.1 Converting to stationary\nWe can resolve these issues by converting a non-stationary series to a stationary series. This is done by differencing, where we look at the differences in the target over time rather than the actual target (y[t]-> y[t] - y[t-1]).\n\nDR: Most economic time series are in fact stationary after first-differencing. However, this is not guaranteed. You may still want to test for stationarity after first-differencing. (But my memory is that the whole tests for stationarity thing is a huge can of worms).\n\nWe can also handle exponential trends using techniques like log transformations."
  },
  {
    "objectID": "chapters/time_series_application.html#handling-features",
    "href": "chapters/time_series_application.html#handling-features",
    "title": "9  Time Series – application to ‘predict the influence of discrete events’ (and more) on web traffic, signups, etc",
    "section": "9.3 Handling Features",
    "text": "9.3 Handling Features\n\n9.3.1 Lags\nLagging is pretty key to time series. A lag is when you use the value from the previous series to forecast the next series. You can lag the target variable (e.g., use last month’s sales to predict next month’s sales) and/or you can lag independent variables. Lagged target variables often have strong explanatory power because the real world has delays (e.g., it takes a few weeks for marketing to transition to sales so marketing spend from three weeks ago may be more predictive than marketing spend of the same week) and causations that occur over time (e.g., sales from last year show that the store is more popular so there is more word of mouth and it is even more popular the next year).\nLagging always involves losing some data, as if you are using data from the previous month you won’t be able to use the first month in your training data (because there’s no previous month data for month -1). If you are using data from the three previous months, you won’t be able to train on the first three months. This may not be an issue though, because unlike with non-time series problems, more data in time series isn’t always better (since you might prefer to only be modeling the most recent trend).\nWe can mix lags of different lengths.\n\n\n9.3.2 Rolling Statistics\nLags aren’t the only thing we can do - we can also calculate rolling statistics, like the mean of a variable over the past 14 days. You can also do rolling stats on differences.\n\n\n\n9.3.3 Which Lags / Rolling Stats to Use?\n\nAssess with cross correlation function, which tests correlation of many different lags.\n\n\nooAssess with partial autocorrelation function (PACF), which gives the partial correlation of a stationary time series with its own lagged values, regressed the values of the time series at all shorter lags. Also can assess with the autocorrelation function (ACF), which does not control for other lags.\n\nchanging lag lengths based on data type, domain knowledge, and how quickly you think the series reacts to change (e.g., use shorter lags for stocks)\ncompare by backtesting\n\n\n\n9.3.4 Known in Advance vs. Not\nWhen we are trying to predict in the future, we run into an issue that the features we are using for prediction might also be unknown at prediction time. For example, our historical data might contain information about rainfall and how that connects to sales, but we can’t reliably know the rainfall three months in the future to predict future sales.\nHowever, some features are known in advance - like Christmas time may also have a big impact on sales and we always know exactly when Christmas will be.\nFor features that are not known in advance, we can still use them by either explicitly forecasting them, extrapolating them out using lags / rolling stats of sufficient size, or extrapolating from current values using differing forecast differences."
  },
  {
    "objectID": "chapters/time_series_application.html#validation",
    "href": "chapters/time_series_application.html#validation",
    "title": "9  Time Series – application to ‘predict the influence of discrete events’ (and more) on web traffic, signups, etc",
    "section": "9.4 Validation",
    "text": "9.4 Validation\nNormally for ML problems we use cross validation, where we randomly partition the data and then predict one partition using data from all the other partitions. The problem with this for time series is that this will involve using future data to predict the past, which will make for unrealistically good predictions.\nInstead, we can use backtesting where we predict a future time using a window of past times (e.g., predict Year 6 using Years 3-5, predict Year 5 using Years 2-4, predict Year 4 using Years 1-3).\nFor metrics, it’s good to compare the evaluation metric to a naive baseline model, or an intentionally minimal extrapolation."
  },
  {
    "objectID": "chapters/time_series_application.html#types-of-models",
    "href": "chapters/time_series_application.html#types-of-models",
    "title": "9  Time Series – application to ‘predict the influence of discrete events’ (and more) on web traffic, signups, etc",
    "section": "9.5 Types of Models",
    "text": "9.5 Types of Models\n\nIntegrated model - move model one step at a time, e.g., ARIMA, exponential smoothing - univariate\nForecast distance model - predict fixed distances, e.g., XGB\nTrend and decomposition model - predict a different model for each distance, e.g., FB Prophet\n\n\n\n9.5.1 ARIMA\nAutoregressive process: AR(p): fit coefficients to p lags\nMoving average process: MA(q): fit coefficients to q previous errors\nARMA model: X = f(AR, MA)\nI(d) = Differencing d times\nARIMA(p, d, q) = AR(p) + I(d) + MA(q)"
  },
  {
    "objectID": "chapters/time_series_application.html#multiseries",
    "href": "chapters/time_series_application.html#multiseries",
    "title": "9  Time Series – application to ‘predict the influence of discrete events’ (and more) on web traffic, signups, etc",
    "section": "9.6 Multiseries",
    "text": "9.6 Multiseries\nPredict a different time series for each unit (e.g., sales by store). Can use features across series (cross series features).\n\n\n\n\nSource Code\n## Time Series -- application to 'predict the influence of discrete events' (and more) on web traffic, signups, etc\n\n\n### The case at hand  (broad terms)\n\nModeling what drives/effects/predicts ...\n\n- 'traffic and number of conversions over the days/weeks of this year' (count data)\n\n- quality of conversions\n\nParticularly in response to discrete events, media coverage, etc.\\*\n\n::: {.alert .alert-secondary}\n\n\\* DR: The latter makes this seem more like a causal inference problem (‘what works to boost good applications’) …\nthan a ‘predict the future to plan around it’ problem (as in Pete’s ML notes).\n\n:::\n\n\n- \"Did any of these events (at different times) seem to increase numbers?*\"\nFrom which they aim to infer (more loosely)\n- \"What kinds of events will, in the future, most likely increase numbers?\"\n*obviously this involves also assessing what normal variation is, where there's a trend, seasonality etc. *\n\n\n### Our proposed overall strategy\n\nEssentially, this is 'time series data' (although there may be some panel and cross-sectional elements).\nWe want to model this in a way that allows for overall trends (possibly nonlinear(, seasonality, and possibly autocorrelation (AR and MA terms?).^[ Do we need to test for stationarity (and then difference and test it again?)\n]\n\n... and maybe also 'structural breaks'.\n\nWhen considering the impact of an event, we/they want to measure its 'full effect over time', taking into account its lagged effects, and possibly indirect 'autoregressive effects' (as converted people may bring in additional converts, etc.)\n\n> DR: I think we need the partner to clarify their goals a bit more.\n> Is it:\n> 1. Predict the evolution in the future to aid their planning?,\n> 2. Understand the value of specific types of media coverage and other things they might influence?,\n> 3. Or just to  the specifically mentioned event?\n> 4. Understand how the outcomes reacted to things largely *out of their control*?\n\nDR: I think a simple model 'adjusted to trends (and maybe seasonality)' might be good enough as a first pass. I'm not sure the data is rich enough to justify much more.\n\n\n### Nik's approach\n\nso I don’t have a huge amount of familiarity here  :confused: but from past cursory reads of the lit my impression’s been that there are lots of equivalencies to areas I’m more familiar with,\n\ne.g. an ARIMA(0,1,0) model is a Brownian motion is a GP with Gaussian kernel $min(x_1,x_2)$\n\n\nso from your description of them problem, there are a few salient properties / complexities:\n\nThe focal outcome is a discrete nonzero number, ie the number of apps / page views.\n\nThe focal parameter is something like being featured in the WSJ, i.e. a (0,1) indicator.\n\nMaybe the effect of this isn't one-and-done, but but gives an immediate  boost that e.g. decays exponentially through time?\n\nThe data are temporally autocorrelated, i.e. samples closer together in time are probably going to be more similar to each other than those further away\n\nThere may also be a linear trend through time separate from being featured in the WSJ, e.g. increasing general interest in 80kh, such that average interest on either side of the ‘discontinuity’ is different in a manner unrelated to 2.\n\nThere may also be nonlinear trends through time, e.g. seasonality effects.\n\nI think my first stab would be to model this with a GLM\n\nSpecifically, a Poisson likelihood w/ a log link and linear model with intercept,\n\n(DR: Poisson because its 'arrival of events'. Why a 'log link'?)\n\n- a linear coefficient on time,\n\n- and an indicator for some discrete bump corresponding to whether a point came before or after the WSJ feature.\n\n(DR: Fundamentally, there will be bumps in individual periods that may occur randomly. A shock or permanent shift after the WSJ feature need not have been *caused* by it. The question is 'how unusual is such a shock, in the context of a long time series of shocks'? And how long is this series, anyways?)\n\nThen, add a mean 0 multivariate normal error term for overdispersion to this, composing like an RBF kernel w/ a periodic kernel?\n\n(DR: Can you explain more? And does this allow for flexible autocorrelation? Do we need to 'test if its a random walk'?)\n\nThis would sorta be like ‘kriging’ / using GPs to model temporal autocorrelation and I think should satisfy 1-3 above. And then if that works focus on trying to build out to cover more nuanced dynamics. But there might be better / more established ways to treat problems like this!\n\n(I don't know what 'kriging' means; does anyone else understand it?)\n\n\n### Kim C's thoughts\n\n1. structural change analysis (e.g., like that implemented in the R package [strucchange](https://cran.r-project.org/web/packages/strucchange/strucchange.pdf)).\n\nYou can use this procedure to find \"breakpoints\" in the data where for example, is the slope changes value.\n\nIf you think the trends are nonlinear, I've  done a similar thing using GAMs, and then taking the 1st derivative of the fitted curves and checking if it intersects zero. Explained pretty well [here](https://www.frontiersin.org/articles/10.3389/fevo.2018.00149/full).\n\n\n\n\n## PETE'S NOTES BELOW\n\n::: {.alert .alert-secondary}\n\n\nDR: Not sure what/how to adapt this\n\n\n:::\n\nIn Machine Learning,  **Time series** problems are problems that involve forecasting (extrapolating) the future based on the information of the past. Typically we have to make a chain of non-independent predictions rather than predict discrete, independent events.\n\n> DR:  this prediction problem should be distinguished from 'time series econometrics', see, e.g., [Diebold's text](https://www.sas.upenn.edu/~fdiebold/Teaching706/TimeSeriesEconometrics.pdf), which focuses on estimating fundamental structural parameters, and considers forms of 'causality'.\n\nMost machine learning problems assume that the order of rows don't matter and that each row is independent of each other. Since time series problems involve time-ordered rows where past events may influence future events, the rows are not independent and this independence assumption is violated.\n\nAnother key assumption of ML is that the training data is similar to the data being predicted (in this case, future data). This assumption must be true for time series as well.\n\n\n### Decomposition\n\nYou can **decompose** a time series into four key parts:\n\n- The **trend** (T), where the mean is changing over time (e.g., sales generally keep increasing over time)\n- **Seasonality** (S) (e.g., sales are higher in the holiday season)\n- A non-seasonal **cyclical** (C) component (e.g., stock market follows \"business cycles\"). This is distinct from seasonality as seasonality has a fixed period (e.g., every November), whereas a cycle does not.\n- A **random** component (e)\n\n> DR: The 'random component' could be distinguised or described further; there are various types of 'random' terms ... shifts, changes in trends, 'moving average' one-off terms, 'autoregressive' error terms\n\n### Types of decomposition\n\nThere are two basic types of decomposition -- **addititive**, where `y = T + S + C + e` and **multiplicative**, where `y = T * S * C * e`.\n\n<!-- DR: I think this needs a bit more explanation. Why choose either?  Can't we have a model that combines aspects of each? -->\n\n## Stationary vs. non-stationarity\n\n**Stationary** time series (a) do not have a trend and also (b) do not have variance that changes over time. **Non-stationary** time series' do have (a) and/or (b). Typically (a) and (b) create problems for modeling, as models have trouble extrapolating these and they tend to violate the assumption that the training data is similar to the data being predicted.\n\n> DR: There can be *trend-stationary* series, that are stationary after including a trend. These are pretty easy to deal with.\n> Note also that nonstationary series can be described as following a 'random walk'\n\n> But I'm also not convinced that this should necessarily be a problem in a prediction problem. If the series is a random walk/nonstationary ... we can still use that knowledge to make a decent prediction of where the outcome will be at time T+t given it's value at time T.\n\n### Converting to stationary\n\nWe can resolve these issues by converting a non-stationary series to a stationary series. This is done by **differencing**, where we look at the differences in the target over time rather than the actual target (y[t]-> y[t] - y[t-1]).\n\n> DR: Most economic time series are in fact stationary after first-differencing. However, this is not guaranteed. You may still want to test for stationarity after first-differencing. (But my memory is that the whole tests for stationarity thing is a huge can of worms).\n\nWe can also handle exponential trends using techniques like log transformations.\n\n\n## Handling Features\n\n### Lags\n\nLagging is pretty key to time series. A **lag** is when you use the value from the previous series to forecast the next series. You can lag the target variable (e.g., use last month's sales to predict next month's sales) and/or you can lag independent variables. Lagged target variables often have strong explanatory power because the real world has delays (e.g., it takes a few weeks for marketing to transition to sales so marketing spend from three weeks ago may be more predictive than marketing spend of the same week) and causations that occur over time (e.g., sales from last year show that the store is more popular so there is more word of mouth and it is even more popular the next year).\n\nLagging always involves losing some data, as if you are using data from the previous month you won't be able to use the first month in your training data (because there's no previous month data for month -1). If you are using data from the three previous months, you won't be able to train on the first three months. This may not be an issue though, because unlike with non-time series problems, more data in time series isn't always better (since you might prefer to only be modeling the most recent trend).\n\nWe can mix lags of different lengths.\n\n### Rolling Statistics\n\nLags aren't the only thing we can do - we can also calculate **rolling statistics**, like the mean of a variable over the past 14 days. You can also do rolling stats on differences.\n\n<!-- DR: I think this has something to do with the choice between autoregressive and 'moving average' terms in Econometrics, but I'm not sure -->\n\n### Which Lags / Rolling Stats to Use?\n\n- Assess with **cross correlation function**, which tests correlation of many different lags.\n\n<!-- DR: 'assess' should be fleshed out a bit more, or perhaps an example should be linked? -->\n\nooAssess with **partial autocorrelation function** (PACF), which gives the partial correlation of a stationary time series with its own lagged values, regressed the values of the time series at all shorter lags. Also can assess with the **autocorrelation function** (ACF), which does not control for other lags.\n\n- changing lag lengths based on data type, domain knowledge, and how quickly you think the series reacts to change (e.g., use shorter lags for stocks)\n\n- compare by backtesting\n\n### Known in Advance vs. Not\n\nWhen we are trying to predict in the future, we run into an issue that the features we are using for prediction might also be unknown at prediction time. For example, our historical data might contain information about rainfall and how that connects to sales, but we can't reliably know the rainfall three months in the future to predict future sales.\n\nHowever, some features are known in advance - like Christmas time may also have a big impact on sales and we always know exactly when Christmas will be.\n\nFor features that are not known in advance, we can still use them by either explicitly forecasting them, extrapolating them out using lags / rolling stats of sufficient size, or extrapolating from current values using differing forecast differences.\n\n<!-- DR: maybe this 'features not known in advance' thing needs a linked example? -->\n\n\n## Validation\n\nNormally for ML problems we use cross validation, where we randomly partition the data and then predict one partition using data from all the other partitions. The problem with this for time series is that this will involve using future data to predict the past, which will make for unrealistically good predictions.\n\nInstead, we can use **backtesting** where we predict a future time using a window of past times (e.g., predict Year 6 using Years 3-5, predict Year 5 using Years 2-4, predict Year 4 using Years 1-3).\n\n\nFor metrics, it's good to compare the evaluation metric to a **naive baseline model**, or an intentionally minimal extrapolation.\n\n\n## Types of Models\n\n- **Integrated model** - move model one step at a time, e.g., ARIMA, exponential smoothing - univariate\n- **Forecast distance model** - predict fixed distances, e.g., XGB\n- **Trend and decomposition model** - predict a different model for each distance, e.g., FB Prophet\n\n<!-- DR: the above is opaque ... e.g., what does 'move model one step at a time' mean? -->\n\n### ARIMA\n\n**Autoregressive process**: AR(p): fit coefficients to p lags\n\n**Moving average process**: MA(q): fit coefficients to q previous errors\n\n**ARMA model**: X = f(AR, MA)\n\n**I(d)** = Differencing d times\n\n**ARIMA(p, d, q)** = AR(p) + I(d) + MA(q)\n\n<!-- DR: obviously, this needs a bit more explanation. Standard statistics and econometric texts cover this ... perhaps Kennedy (\n'A Guide to Econometrics'  treatment is the most practical one I know -->\n\n\n## Multiseries\n\nPredict a different time series for each unit (e.g., sales by store). Can use features across series (**cross series features**)."
  },
  {
    "objectID": "chapters/classification_model_notes.html",
    "href": "chapters/classification_model_notes.html",
    "title": "10  Classification models",
    "section": "",
    "text": "(Fill in or link here)"
  },
  {
    "objectID": "chapters/classification_model_notes.html#classification_model_disc",
    "href": "chapters/classification_model_notes.html#classification_model_disc",
    "title": "10  Classification models",
    "section": "10.1 Discussion: Assessing classification models and precision/recall",
    "text": "10.1 Discussion: Assessing classification models and precision/recall\nBelow, I try to explain “Classification problems”, “Precision and Recall”, and ROC and ROC-AUC (machine learning validation) by analogy …\nThis loosely connects to the classification models used in the ‘EA Survey Donations post/bookdown’, particularly the consideration of model performance\nSpy-detection We have been asked to come up with a method for classifying spies based on observable characteristics of a suspect population (‘population’ in the statistical sense). For example, some of them have mustaches, some wear pointed hats, some have sinister cackles and lapdogs to a greater or less or degree, and some appear to be more wealthy than others… Let us suppose that: We have previous similar suspect populations that we know to be the same in nature (‘drawn from an identical pool’) to the future suspect populations that we will need to make these predictions for We ultimately learned with certainty which of the previous suspect populations turned out to be spies (To keep intuition and calculation simple) suppose that in these populations know that half of all of these people are spies, and half are completely innocent. (This is called ‘balanced classes’).\nWe are asked to come up with a method to use the observable characteristics to classify each individual as more or less likely to be a spy. If we are given a group of individuals’ characteristics our method needs to assign a number to each one that might be interpreted as the “likelihood this person is a spy”. For example our ‘model’ we might say “anyone with a mustache is 75% likely to be a spy, and anyone without a mustache is 25% likely to be a spy.” (If half of the population wears mustaches, this would also give us an average prediction in line with the known overall 50% spy rate.) Or, let’s make the ‘model’ a bit more complicated (but to allow ‘ordering’ predictions) … we might say … we give the probability of being a spy as\n\\(0.75 - 0.25*wtdev\\)\nif they wear a moustache and\n\\(0.25 - 0.25*wtdev\\) otherwise.\nwhere \\(wtdev\\) represents some “normalized” measure of the person’s weight relative to the average weight in the sample, We predict skinnier people are slightly more likely to be spies.\nNote that, since everyone’s weight is slightly different, this will yield a slightly different projected probability for any observed individual. Thus, our model could also be seen as a “ranking” where, for any group of individuals we are given we can order them from least likely to be a spy (the fattest clean-shaven people) to most likely to be a spy (the skinniest moustache-wearer).\nHow should we evaluate a model, in absolute terms, or against other models? This depends on how the models predictions will be used and on the cost of getting it right or wrong in either direction.\nOur model might be applied by a very soft judge (think Mr. Van Driessen from B&B) who thinks it is much worse to convict an innocent person then to set free many spies. On the other hand it might be applied by a very tight-ass judge (think the gym coach) who puts almost no cost on convicting the innocent … he just wants to cath spies. How can we conceive of the ‘success’ of our model? (edited)\n\nWe could compare across different models to see if one does ‘universally better or worse’ in a sense to be defined. Suppose each model both gives a probability of being a spy for each person. As noted, this implies that each model “rank-orders” a population of individuals in terms of most to least likely being a spy.\n\n\nIn a given (test) set of data, we might see something like the number line dots thing above. The person our model claims is most likely to be a spy is on the right and the one predicted least likely is on the left. In application the liberal judge (“vDriessen”) might convict only the “top 1/3” of the distribution, while the tight-ass (“Coach”) might put 2/3 of these ‘dots’ in prison. Each have made different types of mistakes in practice. vDriessen let a bunch of spies free (and falsely convicted only a few), while the Coach put a lot of innocent people in prison (and, in practice, but not in the dots example above, also let a few spies free). (edited)\nWe might consider that our model has some probability of being applied by various types of judges. It might be that our model performs “better” than an alternative model when applied by the liberal vDriessen but “worse” when applied by the strict Coach. On the other hand, we can imagine that some models might perform better for nearly all types of judge! How could we measure this? The ROC curve can be helpful:\n\nAbove we see models that are strictly ordered from best to worse … the lines do not cross (this need not always be the case). The blue line to picks or model that is strictly better than the orange line, which is itself strictly better than a “random classifier”. A ‘random classifier’ give us no information. If we use this we can only choose some probability to convict everyone. Using this, if we increase the rate we can convict people by 2 per 100, then on average we are convicting one more spy and one more innocent person, in our example. So, by what measure is the blue classifier ‘better’ than the orange one (and both are better than the random classifier)? (edited)\nRemember, each classifier gives a ‘ranked ordering’ of the population. Suppose that we consider going down the list from the most guilty looking to the most innocent looking, and increasing the number of people that we choose to convict.\nAs we go down the ranking in general and decide to convict a larger share of the population we are adding some possibility of convicting and innocent person (the ‘cost’) to gain the ‘benefit’ of convicting some true spies. As depicted in the ROC curves above, as we do this the blue curve (classifier) is achieving a better tradeoff than the orange, green or red curve … (this is because it gives us a ‘better ordered list of who us more likely to be guilty’)…\nAs the judge using the blue curve decides to go further down the ‘blue curve list’ (paying the cost of convicting more innocent people) he also catches more spies than if he had used the ‘orange, green, or red curve list’ . The benefit to cost ratio is higher at all ‘rates of conviction’ here. (edited)\nThis is fine in a relative sense … we know blue is better here (better than all the others except for the purple ‘perfect classifier’ … which is equivalent to a tool that always ‘perfectly orders’ guilt so there is no tradeoff)\nBut:\n\nWhat if the curves cross? and\nHow do we consider the “overall” success of these classifier models? Maybe they are all doing a terrible job.\n\n(gotta run now, will try to get back to this … for question 2 the ‘area under the curve’ seems like a decent measure that somehow ‘averages’ the gain in insight over the random classifier for all possible types of judge, if I understand correctly)\nI have been told that the following is the case:\n\nGiven randomly chosen examples from class 1 and 2, the AUC is the probability that our classifier gives a larger probability of assigning the class 1 example to class 1 (true positive) than the class 2 example (false positive)\n\nI am reinterpreting this more simply as…\n\nCompare a randomly chosen spy (case \\(i\\) where \\(y_i=1\\)) to a randomly chosen innocent person (case \\(i\\) where \\(y_i=0\\)) … (in the testing data). Recall that a ‘classifier’ (at least typically, e.g., as in a logistic regression) assigns to each case \\(i\\) some probability \\(\\hat(p_i)\\) of being a spy, based on its features. The AUC tells me “the probability that my classifier assigns a larger probability of guilt \\(\\hat(p_i)\\) to the randomly-chosen spy than to the randomly-chosen innocent person.”\n\nAn analogy has also been made to the Wilcoxon Rank-sum measure.\n\n\n\n\nSource Code\n# Classification models\n\n(Fill in or link here)\n\n## Discussion: Assessing classification models and precision/recall  {#classification_model_disc}\n\nBelow, I try to explain \"Classification problems\", \"Precision and Recall\", and ROC and ROC-AUC (machine learning validation) by analogy ...\n\nThis loosely connects to the classification models used in the ['EA Survey Donations post/bookdown'](), particularly the [consideration of model performance](https://rethinkpriorities.github.io/ea_data_public/eas_donations.html#model-perf)\n\n**Spy-detection** We have been asked to come up with a method for classifying spies based on observable characteristics of a suspect population ('population' in the statistical sense). For example, some of them have mustaches, some wear pointed hats, some have sinister cackles and lapdogs to a greater or less or degree, and some appear to be more wealthy than others... Let us suppose that: We have previous similar suspect populations that we know to be the same in nature ('drawn from an identical pool') to the future suspect populations that we will need to make these predictions for We ultimately learned with certainty which of the previous suspect populations turned out to be spies (To keep intuition and calculation simple) suppose that in these populations know that half of all of these people are spies, and half are completely innocent. (This is called 'balanced classes').\n\nWe are asked to come up with a method to use the observable characteristics to classify each individual as more or less likely to be a spy. If we are given a group of individuals' characteristics our method needs to assign a number to each one that might be interpreted as the \"likelihood this person is a spy\". For example our 'model' we might say \"anyone with a mustache is 75% likely to be a spy, and anyone without a mustache is 25% likely to be a spy.\" (If half of the population wears mustaches, this would also give us an average prediction in line with the known overall 50% spy rate.) Or, let's make the 'model' a bit more complicated (but to allow 'ordering' predictions) ... we might say ... we give the probability of being a spy as\n\n$0.75 - 0.25*wtdev$\n\nif they wear a moustache and\n\n$0.25 - 0.25*wtdev$ otherwise.\n\nwhere $wtdev$ represents some \"normalized\" measure of the person's weight relative to the average weight in the sample, We predict skinnier people are slightly more likely to be spies.\n\nNote that, since everyone's weight is slightly different, this will yield a slightly different projected probability for any observed individual. Thus, our model could also be seen as a \"ranking\" where, for any group of individuals we are given we can order them from least likely to be a spy (the fattest clean-shaven people) to most likely to be a spy (the skinniest moustache-wearer).\n\nHow should we evaluate a model, in absolute terms, or against other models? This depends on how the models predictions will be used and on the cost of getting it right or wrong in either direction.\n\nOur model might be applied by a very soft judge (think Mr. Van Driessen from B&B) who thinks it is much worse to convict an innocent person then to set free many spies. On the other hand it might be applied by a very tight-ass judge (think the gym coach) who puts almost no cost on convicting the innocent ... he just wants to cath spies. How can we conceive of the 'success' of our model? (edited)\n\n![](https://www.researchgate.net/profile/Bahman-Zohuri/publication/350592160/figure/fig2/AS:1008253420969985@1617397756511/Predictions-Ranked-in-Ascending-order-of-Logistic-Regression-Score.png)\n\nWe could compare across different models to see if one does 'universally better or worse' in a sense to be defined. Suppose each model both gives a probability of being a spy for each person. As noted, this implies that each model \"rank-orders\" a population of individuals in terms of most to least likely being a spy.\n\n![](https://static.wikia.nocookie.net/beavisandbutthead/images/7/72/Beavis_he_said_anus.jpg/revision/latest/top-crop/width/360/height/450?cb=20110922015543){width=\"20%\"}\n\n![](https://static.wikia.nocookie.net/villains/images/8/8d/Buzzcut.png/revision/latest?cb=20130320041141){width=\"20%\"}\n\nIn a given (test) set of data, we might see something like the number line dots thing above. The person our model claims is most likely to be a spy is on the right and the one predicted least likely is on the left. In application the liberal judge (\"vDriessen\") might convict only the \"top 1/3\" of the distribution, while the tight-ass (\"Coach\") might put 2/3 of these 'dots' in prison. Each have made different types of mistakes in practice. vDriessen let a bunch of spies free (and falsely convicted only a few), while the Coach put a lot of innocent people in prison (and, in practice, but not in the dots example above, also let a few spies free). (edited)\n\nWe might consider that our model has some probability of being applied by various types of judges. It might be that our model performs \"better\" than an alternative model when applied by the liberal vDriessen but \"worse\" when applied by the strict Coach. On the other hand, we can imagine that some models might perform better for nearly all types of judge! How could we measure this? The ROC curve can be helpful:\n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/Roc_curve.svg/440px-Roc_curve.svg.png)\n\nAbove we see models that are strictly ordered from best to worse ... the lines do not cross (this need not always be the case). The blue line to picks or model that is strictly better than the orange line, which is itself strictly better than a \"random classifier\". A 'random classifier' give us no information. If we use this we can only choose some probability to convict everyone. Using this, if we increase the rate we can convict people by 2 per 100, then on average we are convicting one more spy and one more innocent person, in our example. So, by what measure is the blue classifier 'better' than the orange one (and both are better than the random classifier)? (edited)\n\nRemember, each classifier gives a 'ranked ordering' of the population. Suppose that we consider going down the list from the most guilty looking to the most innocent looking, and increasing the number of people that we choose to convict.\n\nAs we go down the ranking in general and decide to convict a larger share of the population we are adding some possibility of convicting and innocent person (the 'cost') to gain the 'benefit' of convicting some true spies. As depicted in the ROC curves above, as we do this the blue curve (classifier) is achieving a better tradeoff than the orange, green or red curve ... (this is because it gives us a 'better ordered list of who us more likely to be guilty')...\n\nAs the judge using the blue curve decides to go further down the 'blue curve list' (paying the cost of convicting more innocent people) he also catches more spies than if he had used the 'orange, green, or red curve list' . The benefit to cost ratio is higher at all 'rates of conviction' here. (edited)\n\nThis is fine in a relative sense ... we know blue is better here (better than all the others except for the purple 'perfect classifier' ... which is equivalent to a tool that always 'perfectly orders' guilt so there is no tradeoff)\n\nBut:\n\n-   What if the curves cross? and\n-   How do we consider the \"overall\" success of these classifier models? Maybe they are all doing a terrible job.\n\n(gotta run now, will try to get back to this ... for question 2 the 'area under the curve' seems like a decent measure that somehow 'averages' the gain in insight over the random classifier for all possible types of judge, if I understand correctly)\n\nI have been told that the following is the case:\n\n> Given randomly chosen examples from class 1 and 2, the AUC is the probability that our classifier gives a larger probability of assigning the class 1 example to class 1 (true positive) than the class 2 example (false positive)\n\nI am reinterpreting this more simply as...\n\n> Compare a randomly chosen spy (case $i$ where $y_i=1$) to a randomly chosen innocent person (case $i$ where $y_i=0$) ... (in the testing data). Recall that a 'classifier' (at least typically, e.g., as in a logistic regression) assigns to each case $i$ some probability $\\hat(p_i)$ of being a spy, based on its features. The AUC tells me \"the *probability* that my classifier assigns a larger probability of guilt $\\hat(p_i)$ to the randomly-chosen spy than to the randomly-chosen innocent person.\"\n\nAn analogy has also been made to the Wilcoxon Rank-sum measure."
  },
  {
    "objectID": "chapters/causal_inf.html",
    "href": "chapters/causal_inf.html",
    "title": "11  Causal Inference",
    "section": "",
    "text": "Basic ideas and frameworks (simple, potential outcomes, DAGs)\nPitfalls and mistakes (layman’s terms)\nThe experimental ideal\nNon-experimental approaches to causal inference\nDealing with attrition\nSome stuff to integrate from here perhaps."
  },
  {
    "objectID": "chapters/causal_inf.html#useful-resources",
    "href": "chapters/causal_inf.html#useful-resources",
    "title": "11  Causal Inference",
    "section": "Useful resources:",
    "text": "Useful resources:\nCausal Inference: What If. R and Stata code for Exercises\nThe effect – bookdown, looks great, love nick HK\nCausal inference: the Mixtape\nUseful R Packages for Causal Inference in the Social Sciences\nApplied causal analysis with R\nMostly Harmless Econometrics (R code – where was it?)\n\n\n\n\nSource Code\n# Causal Inference {-#causal}\n\n\n- Basic ideas and frameworks (simple, potential outcomes, DAGs)\n- Pitfalls and mistakes (layman's terms)\n- The experimental ideal\n- Non-experimental approaches to causal inference\n- Dealing with attrition\n\n\nSome stuff to integrate from [here](https://daaronr.github.io/metrics_discussion/caus-inf-obs.html#caus_inf_obs) perhaps.\n\n\n## Useful resources: {-}\n\n[Causal Inference: What If. R and Stata code for Exercises](https://remlapmot.github.io/cibookex-r/)\n\n[The effect](https://www.theeffectbook.net/) -- bookdown, looks great, love nick HK\n\n[Causal inference: the Mixtape](https://www.scunning.com/mixtape.html)\n\n[Useful R Packages for Causal Inference in the Social Sciences](https://www.dhidalgo.me/post/useful-r-packages-for-causal-inference-in-the-social-sciences/)\n\n[Applied causal analysis with R](https://bookdown.org/paul/applied-causal-analysis/)\n\nMostly Harmless Econometrics (R code -- where was it?)"
  },
  {
    "objectID": "chapters/fermi.html",
    "href": "chapters/fermi.html",
    "title": "\n12  Monte-Carlo ‘Fermi Estimation’ Approaches\n",
    "section": "",
    "text": "‘Fermi estimation’ is essentially a more formal approach to this, carefully defining and explaining each element of the ‘model’ equation\nWhen we explicitly define (and justify) a probability distribution over each variable in the model, and compute (often through simulation) the overall uncertainty of the outputs (predictions, estimates, etc), we call this “Monte Carlo Fermi Estimation” (at least David Reinstein thinks this is what it’s called).\nDavid Reinstein and Sam Nolan explain this approach, advocating its use in GiveWell models and beyond, lay out some building blocks,\n… and embed some tools and work-in-progress on this HERE, also embedded below.\nOverview\n\nThe basic ideas\nCausal and Guesstimate\nCode-based tools\n\n\nCodeknitr::include_url(\"https://effective-giving-marketing.gitbook.io/innovations-in-givewell-esque-ceas/\")\n\n\n\n\nDR: We may want to look for ways to explicitly incorporate and integrate these approaches into our data analysis work in R, etc.\n\n\n\n\n\nSource Code\n# Monte-Carlo 'Fermi Estimation' Approaches {-#fermi}\n\n\n\n'BOTEC': Back of the envelope calculations are central to RP's work\n\n'Fermi estimation' is essentially a more formal approach to this, carefully defining and explaining each element of the 'model' equation\n\nWhen we explicitly define (and justify) a probability distribution over each variable in the model, and compute (often through simulation) the overall uncertainty of the outputs (predictions, estimates, etc), we call this \"Monte Carlo Fermi Estimation\" (at least David Reinstein thinks this is what it's called).\n\n\nDavid Reinstein and Sam Nolan explain this approach, advocating its use in GiveWell models and beyond, lay out some building blocks,\n\n... and embed some tools and work-in-progress on this [HERE](https://effective-giving-marketing.gitbook.io/innovations-in-givewell-esque-ceas/),  also embedded below.\n\n\n**Overview**\n\n- The basic ideas\n- Causal and Guesstimate\n- Code-based tools\n\n\n```{r}\nknitr::include_url(\"https://effective-giving-marketing.gitbook.io/innovations-in-givewell-esque-ceas/\")\n\n```\n\n::: {.alert .alert-secondary}\n\nDR: We may want to look for ways to explicitly incorporate and integrate these approaches into our data analysis work in R, etc.\n\n:::"
  },
  {
    "objectID": "chapters/other_sections.html",
    "href": "chapters/other_sections.html",
    "title": "13  Other suggested sections",
    "section": "",
    "text": "Integrate from:\nReinstein discussions here"
  },
  {
    "objectID": "chapters/other_sections.html#meta",
    "href": "chapters/other_sections.html#meta",
    "title": "13  Other suggested sections",
    "section": "13.2 (Meta-analysis)",
    "text": "13.2 (Meta-analysis)\nIncorporate and consolidate from Reinsteins meta notes and more\n\n\n\n\nSource Code\n# Other suggested sections\n\n\n## (Open and robust science: RP attitudes, discussions, resources) {#opensci}\n\nIntegrate from:\n\n\n[Reinstein discussions here](https://daaronr.github.io/metrics_discussion/robust-diag.html)\n\n\n\n## (Meta-analysis) {#meta}\n\nIncorporate and consolidate from [Reinsteins meta notes](https://daaronr.github.io/metrics_discussion/metaanalysis.html) and more"
  },
  {
    "objectID": "chapters/power_analysis_framework_2.html",
    "href": "chapters/power_analysis_framework_2.html",
    "title": "Power analysis workflow (Jamie Elsey)",
    "section": "",
    "text": "1"
  },
  {
    "objectID": "chapters/power_analysis_framework_2.html#discussion-and-framework",
    "href": "chapters/power_analysis_framework_2.html#discussion-and-framework",
    "title": "Power analysis workflow (Jamie Elsey)",
    "section": "Discussion and framework",
    "text": "Discussion and framework\nIntroduction and goals\nThe purpose of this document is to propose the fundamentals for a workflow for power analyses.\nAlthough this started from a Bayesian paradigm, the overarching framework is applicable to any analytic approach, and I will begin with a standard frequentist analysis.2\nThe general approach laid out here can be the basis for developing and build a library of common analyses and inference goals.3 Helpful future additions to the toolkit would include\n\nclear and flexible ways to generate hypothetical data sets, and\nadding further analytic designs.\n\nThis is not intended as an exhaustive introduction to the fundamentals of statistical power–I assume you are reasonably well-versed in some general principles of statistical inference and hypothesis testing, as well as in the basic idea of what power analysis is.4\nWe will focus solely on ‘simulation-based’ power analysis, and not on ways of mathematically (analytically) deriving power.\nDefinition of power\nIn frequentist null hypothesis significance testing (NHST), power is typically defined as the probability that we can reject the null hypothesis, if the alternative hypothesis is indeed true (i.e., power is the ‘true positive rate’). More precisely, we may express this as ‘power against a particular alternative hypothesis’.5\n\n\n\n\n\n\nFrom wikipedia: The statistical power of a binary hypothesis test…\n\n\n\n\n\nThe statistical power of a binary hypothesis test is the probability that the test correctly rejects the null hypothesis $H_{0}$ when a specific alternative hypothesis $H_{1}$ is true. It is commonly denoted by $1-$, and represents the chances of a “true positive” detection conditional on the actual existence of an effect to detect. … as the power of a test increases, the probability $$ of making a type II error by wrongly failing to reject the null hypothesis decreases.\nlink accessed 12 Apr 2022\n\n\n\nNote that some Bayesian statisticians question the validity and usefulness of this presentation of power analysis.6\nBefore we conduct an experiment (or run a survey, or collect data for an analysis), there are many hypothetical future data sets we might observe. Whether we favor a frequentist or Bayesian approach, it seems reasonable to ask: ‘Given the range of data that I might observe, how likely is it that I can make certain conclusions?’. 7\nWe can broaden the idea of power to indicate the probability that our proposed sample yields information that allows us to make some specific kind of inference about the data-generating process.\nGiven a particular underlying effect (or range of effects, or lack thereof), and a particular sample size (or range of sample sizes), what is the probability of… :8\n\n‘determining’ that there is a non-zero difference between two conditions,9\n\ndetecting some ‘smallest effect size of interest’,\na ‘false positive’, i.e., the probability of concluding that there is a difference between groups when in fact there is no difference(the standard ‘Type 1 error’, aka the ‘size’ of a test),\nachieving a desired level of precision around a particular parameter estimate, or\n finding an effect ‘likely equivalent to zero’ or ‘unlikely to be far from zero’.10\n\n\nHence, a power analysis helps frame our prospective research project in relation to the goals we would like to achieve.11 It provides us us with an estimate (and it is indeed an estimate, not a certainty!) of the probability that we will be able to make the inference we wish to make, given various factors both inside and outside of our control. (See discussion in fold.)\n\n\n\n\n\n\nNote\n\n\n\n\n\nWhat is ‘in our control’? We might think of sample size as in our control, which it generally is, but this is typically limited by practical considerations outside of our control (such as the availability of a sample or financial constraints). Conversely, effect size is often considered out of our contro;; however we can sometimes increase the ‘dosage’ we give, or try to select a particular group of participants who might be particularly susceptible to our effect of interest.\n\n\n\nGeneral workflow for simulation-based power analysis\nSimulation-based power analysis proceeds in four primary steps:\n\nGenerate a large number of ‘simulated data sets’ for the analysis.12\n\n\nThe data sets generated should be specific to the goal of the study. E.g., suppose we want to know the ‘power (of a particular design and testing procedure) to detect an effect size of 0.2 SD’. Here we should generate data that reflects this effect size (for considering the true positive and false negative rates). If we also want to measure the rate of type-2 error, the rate of false positives (if the null, e.g., an effect side of 0, holds), we should also generate data reflecting this ‘null effect’.\n\nRun the proposed analysis over the many simulated data sets (as efficiently as possible, as this can take a long time). This analysis should either return the estimand of interest, or ensure that we can easily compute it (in Step 3). The output should be kept as flexible as possible (while conserving computer memory). This will allow us to assess multiple inference goals on the same output.13\nSummarize the output returned in Step 2, computing the share of simulated data sets that meet different decision criteria or inference goals. (Example described in fold)\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nE.g., we might simulate 1000 data sets based on an effect size of 0.3 standard deviations and perform a standard t-test of the difference between treatment and control for each of these. We might then find that for 743 of 1000 of these simulated data, the test ‘rejected the null hypothesis’, suggesting a power of 74.3%.\n(Feel free to add other examples here.)\n\n\n\nThis can show the likelihood of achieving a range of goals given various sample sizes and design and testing choices, including the rates of misleading conclusions. In frequentist analyses, it is possible that steps 2 and 3 occur together (e.g., the p-value is returned along with the other output). In my experience with typical Bayesian designs, these stages are best kept separate so that we can first do the more time-consuming Step 2, and then more freely explore the various options in Step 3.\n\nAssess the output and determine whether the proposed analyses and inference goals are realistic and likely to yield informative results. If not, one may need to think of alternative design or data-collection choices (including sample sizes, treatments, and treatment assignments) or inference goals, and return to Step 1.14\n\n\nIn the sections that follow, I will present some generally useful packages and functions for power analyses, including annotated code examples with particularly useful aspects highlighted. I will do this in the context of a frequentist power analysis for between-groups (=between-subject?) analysis.15\nPossible pitfalls/misunderstandings of power analysis\nIt is important to note that power analyses are not an ‘omniscient oracle’. They might better be termed ‘power projections’ or ‘power estimates’. Even if we estimate that we have 99.9% power for detecting some a particular effect, we may have specified our simulations in an unrealistic way. When we actually run an experiment, we might see (e.g.) far more underlying variation or measurement error than we predicted, leading our analyses to be fairly uninformative. Conversely, we might be overly conservative with our power analysis; perhaps a design we thought was ‘underpowered’ actually has a high probability of producing very compelling results.16\nI raise this only to point out that our power analyses might not always be perfect or be something of a blunt instrument. We should recognise they are not guarantees but more a way of determining the basic plausibility of achieving certain goals. Some other things to consider in a similar vein are presented below, and again the intention here is merely to frame our use of power analyses and recognise what sort of things I suspect they can or can’t help us with:\n\nWhen we see real data and real interesting patterns emerge, we are likely to go further in modeling and investigating these patterns than some of the more simple analyses and comparisons we conduct in the initial power analysis. I suspect power analyses are not so good for determining all the intricate, in-depth things we might plumb in a dataset. They are probably better at assessing the tractability of a broad inference goal.\nWe might want to give some consideration to how far we wish to go in the initial data simulation step to think about all sorts of hypothetical data sets. As models become more complex, the number of different parameters that might vary - with possible effects on power - starts to balloon. E.g., even in a simple repeated-measures example, do we wish to vary not only the effect size but all sorts of different correlations from pre- to post-treatment within subjects? If we are simulating ordinal data, then power might change depending on how we initially suggest binning the outcomes, but there are infinitely many ways we might think the data might look… Discussion and consideration of how far we should go with these things is welcome and could be useful! (DR: I’m not sure what you are getting at here. Are you saying that ‘the space of designs and proposed analyses we can explore is extremely large, and we may need to make some ad-hoc choices to avoid this getting unmanageable’?)"
  },
  {
    "objectID": "chapters/power_analysis_framework_2.html#concrete-implementation-of-framework",
    "href": "chapters/power_analysis_framework_2.html#concrete-implementation-of-framework",
    "title": "Power analysis workflow (Jamie Elsey)",
    "section": "Concrete implementation of framework",
    "text": "Concrete implementation of framework\nStep 1: Generate a large number of data sets for the analysis\nConfirm that we can generate the basic data we want\nBecause ultimately we might be generating some rather large data files, we increase the memory limit alotted to R first. I don’t believe there is any real cost to increasing this memory limit, so it is wise to do so as to avoid a function iterating many times over for a long time, but ending up without sufficient space to store the outcome:\n\nCodememory.limit(100000)\n\n[1] Inf\n\n\nNow we want to generate a hypothetical data set. In this case, we will be comparing 4 groups, each shown a different message in a between-subjects design (3 active vs. a control condition with a neutral message). Willem has some nice procedures for generating repeated-measures data in his walkthrough (https://www.willemsleegers.com/posts/simulation-based-power-analyses/) using the library MASS. I think there are likely to be other useful packages or techniques for generating simulated data for more complex designs or patterns of results that we could look into. Being better able to generate valid and informative data sets is probably one of the things that would improve our capacity for running informative power analyses the most.\nIn this example, we will take advantage of the different groups to assess several different possible effect sizes. In a multiple regression, we would then be able to assess whether these different effect sizes come out as significantly different from our control group, and therefore our power to detect different sizes of effects.\n\nCode# tidyverse simply used for data wrangling and plotting\nlibrary(tidyverse)\n\n\n\nCodefour.group.datamaker <- function(sim = 1, a = 0, b = .1, c = .2, d = .4, pop=1500) {\n\n  # first a tibble (data frame) with 1500 ppts, with the different groups showing\n  # effect sizes in Cohen's d of .1, .2, and .4\n  four.groups <- tibble(a.control = rnorm(pop, a, 1),\n                        b.small = rnorm(pop, b, 1),\n                        c.medsmall = rnorm(pop, c, 1),\n                        d.medium = rnorm(pop, d, 1),\n                        sample.size = 1:pop) %>%\n\n    # turn the data into long form\n    pivot_longer(cols = 'a.control':'d.medium', names_to = 'group', values_to = 'response') %>%\n\n    # put cutpoints in the data to make it more similar to the ordinal responses we would get\n    mutate(ordinal = case_when(response < -1.5 ~ 1,\n                               response < -.5 ~ 2,\n                               response < .5 ~ 3,\n                               response < 1.5 ~ 4,\n                               response >= 1.5 ~ 5),\n\n           # for the purposes of this demo we will not analyse it as ordinal as it takes longer\n           # to run the regressions, but if you did so you would also want to make the response\n           # a factor\n           ordinal = as.factor(ordinal),\n           sim = sim)\n\n  return(four.groups)\n\n}\n\n# test that the function works to make one data set before making many!\ntest.data <- four.group.datamaker()\n\nggplot(data = test.data) +\n  geom_density(aes(x = response, fill = group), alpha = .3) +\n  theme(\n    aspect.ratio = .5\n  )\n\n\n\nCodeggplot(data = test.data) +\n  geom_histogram(aes(x = as.numeric(ordinal)), alpha = .6,\n                 position = position_dodge(), bins = 10) +\n  facet_wrap(~group, nrow=1) +\n  theme(\n    aspect.ratio = 1\n  )\n\n\n\n\nWe can see from the plots that the function appears to be working. When developing a data set for the first time, one would usually go further with some ‘diagnostic’ checks to confirm that the data is behaving as you intended. For example, in Willem’s examples, he used the mvnorm function with ‘empirical = TRUE’, so that the exact mean diffreences you specified are present in the data. This can then be confirmed wiht descriptive statistics. There might be all sorts of other diagnostics or plots we might check with other types of data that we generate.\nEfficiently generate many data sets\nNow we just need to run the function above many times over. This could be done using loops, but a very useful set of R functions in the tidyverse is the purrr package of map functions. Even better, a package called furrr is available to run such map functions in parallel to further reduce time. This doesn’t matter so much here because this will be quite quick anyway, but is important when we run the analyses over the many data sets. For furrr to do this, we need to tell it to plan for ‘multisession’, and give it a seed number:\n\nCodep_load(furrr)\nlibrary(furrr)\n\nplan(multisession)\noptions <- furrr_options(seed = 48238)\n\n\n\nCode# we will pass N = 500 simulations to the map function\nnsims <- 1:500\n\n# the map function will run our data-making function over 1000 simulations\nsim.data <- future_map_dfr(.x = nsims, .f = four.group.datamaker,\n                           a = 0, b = .1, c = .2, d = .4, .options = options)\n\n# split the simulated data into the separate simulations\nsim.data <- sim.data %>% group_by(sim) %>% group_split()\n\n\nNow, we have 500 simulated data sets representing our hypothetical outcome data, and can perform analyses on them:\n\nCodehead(sim.data[[3]])\n\n\n\n\n\n\n\n\n\nsample.size\ngroup\nresponse\nordinal\nsim\n\n\n1\na.control\n-0.395 \n3\n3\n\n\n1\nb.small\n-0.396 \n3\n3\n\n\n1\nc.medsmall\n-0.0762\n3\n3\n\n\n1\nd.medium\n0.994 \n4\n3\n\n\n2\na.control\n-0.931 \n2\n3\n\n\n2\nb.small\n-1.58  \n1\n3\n\n\n\n\nStep 2: Run the proposed analysis over the many data sets and return the estimands of interest\nBefore we run an analysis over the many data sets, we need to check that our models will return the estimands that we wish to make inferences from. For a Bayesian analysis, this step might involve dropping parts of the posterior that are not relevant (e.g., estimates of every participant intercept, which take too much space for what they are worth) and ensuring we get the parts we care about (e.g., we might insert some additional code to retrieve a posterior distribution for Cohen’s d, or simply the parameter estimate for the interaction). For Bayesian analyses I err on the side of getting as many of the main parameters as possible, because this is a very time consuming step.\nFor frequentist analyses, there are also a range of estimands we might care about. For example, we might consider returning a p-value, or the upper and lower bounds for confidence intervals, r-squared estimates etc. Again, this depends on the goals one wishes to achieve and what one wants to make inferences about.\nThe key point for any kind of analysis here is that you don’t want to forget an estimand that might be of interest and then have to re-run the entire analysis. In addition, you want to have the regression run on different sample sizes of the data, so that you can generate a power curve plot for the analysis, showing how your power changes according to increasing the sample size.\nIn the example function below, we run a simple linear regression, predicting the response from group.\n\nCodelinear.reg.maker <- function(data, breaks) {\n\n  # this function cuts the data set it is given into different sample sizes\n  cut.samples <- function(break.point, data) {\n    cut.data <- filter(data, sample.size <= break.point) %>%\n      mutate(sample.size = break.point)\n    return(cut.data)\n  }\n\n  data.cuts <- map_dfr(.x = breaks, .f = cut.samples, data = data)\n\n  # the data is split according to the sample size\n  # to feed to the regression model\n  data.cuts <- data.cuts %>% group_by(sample.size) %>% group_split()\n\n  # this function runs the regression\n  run.reg <- function(data) {\n\n    four.group.form <- as.numeric(ordinal) ~ 1 + group\n\n    four.group.reg <-\n      lm(formula = four.group.form,\n           data = data)\n\n    # we extract confidence intervals for the parameters of interest\n    ci99 <- confint(four.group.reg, level = .99)\n    ci95 <- confint(four.group.reg, level = .95)\n\n    # we create an output to show the confidence intervals around the effects\n    # and some additional inference info, e.g., 'nonzero' indicates whether\n    # the lower bound of the CI excludes 0 or not.\n    # 'width' indicates the width of the confidence interval,\n    # for assessment of precision\n    output <- tibble(group = c('small', 'medsmall', 'medium',\n                               'small', 'medsmall', 'medium'),\n                     interval = c(.99, .99, .99, .95, .95, .95),\n                     lower = c(ci99[[2,1]], ci99[[3,1]], ci99[[4,1]],\n                               ci95[[2,1]], ci95[[3,1]], ci95[[4,1]]),\n                     upper = c(ci99[[2,2]], ci99[[3,2]], ci99[[4,2]],\n                               ci95[[2,2]], ci95[[3,2]], ci95[[4,2]])) %>%\n      mutate('nonzero' = case_when(lower > 0 ~ 1,\n                                   TRUE ~ 0),\n             'width' = abs(upper - lower),\n             'sim' = data[[1, 'sim']],\n             'cell.size' = nrow(data)/4)\n\n    return(output)\n  }\n\n  # run the regression function over the different sample sizes\n  output <- map_df(.x = data.cuts, .f = run.reg)\n\n  return(output)\n}\n\n\nOnce we have made and tested that our function works as intended and returns the values we want to make inferences from, we can run it over the many simluated data sets:\n\nCodet1 <- Sys.time()\nlinreg.output <- future_map_dfr(.x = sim.data,\n                                .f = linear.reg.maker,\n                                breaks = seq(from = 150, to = 1500, by = 150))\nt2 <- Sys.time()\nt2 - t1\n\nTime difference of 15.23544 secs\n\n\nThe object ‘linreg.output’ is now a large dataframe, cataloguing whether or not certain inference thresholds were reached across the many simulations. In the third primary step, we can summarise and graphically display this information.\nStep 3: Summarise the output returned in Step 2 to determine likelihood of achieving various inferential goals\nNow, we wish to ascertain how likely we are to achieve a range of inferential goals, depending on factors such as the sample size, the underlying effect sizes, or whatever else we varied in simulating our data and running our models. For this example, this is as simple as generating a summary of the output from Step 2:\n\nCode# group the data according to group, confidence interval, and size per group\nfour.group.lin.summary <- linreg.output %>% group_by(group, interval, cell.size) %>%\n  # summarise the amount of times we get a CI greater than 0\n  summarise(.groups = 'keep',\n            'ci above 0 vs. control' = sum(nonzero)/5)  %>%\n  # change some factors for plotting\n  mutate(interval = factor(interval, levels = c('0.95', '0.99'),\n                           labels = c('95% CI', '99% CI')),\n         'Effect size' = factor(group, levels = c('small', 'medsmall', 'medium'),\n                                labels = c('Very small (.1)', 'Small (.2)', 'Medium (.4)')))\n\n\nAnd then plotting the resulting power curve:\n\nCodeggplot(data = four.group.lin.summary) +\n  scale_x_continuous(limits = c(100, 1550), breaks = seq(from = 150, to = 1500, by = 150)) +\n  scale_y_continuous(limits = c(0, 100), breaks = seq(from = 0, to = 100, by = 20)) +\n  geom_hline(aes(yintercept = 80), linetype = 'dashed', size = .33, alpha = .25) +\n  geom_hline(aes(yintercept = 90), linetype = 'dashed', size = .33, alpha = .25) +\n  geom_path(aes(x = cell.size, y = `ci above 0 vs. control`, color = `Effect size`,\n                group = `Effect size`), size = .66) +\n  geom_point(aes(x = cell.size, y = `ci above 0 vs. control`, color = `Effect size`),\n             size = 1.5) +\n  labs(y = 'Power to detect a non-zero effect',\n       x = 'Number of participants per condition (control group not included)') +\n  scale_color_manual(values = c('#c10d0d', '#7dc3c2', '#dcc55b')) +\n  facet_wrap(~interval) +\n  theme(\n    aspect.ratio = 1,\n    panel.grid.major = element_line(colour = \"white\", size = 0.33),\n    panel.grid.minor = element_line(colour = \"white\", size = 0.2),\n    panel.background = element_rect(fill = \"grey96\"),\n    axis.line = element_line(color = 'black', size = 0.375),\n    axis.ticks = element_line(color = 'black', size = 0.5),\n    text = element_text(color = 'black', family = 'Gill Sans MT', size = 9),\n    axis.text = element_text(color = 'black', family = 'Gill Sans MT', size = 7),\n    strip.background = element_blank()\n  )\n\n\n\n\nStep 4. Assess the output and determine whether the proposed analyses and inference goals are realistic and likely to yield informative results.\nIn Step 4, we use the information we have generated above to make substantive conclusions about the projected power of our experiment to detect certain effects, given certain underlying parameters. From this we can make recommendations as to experimental design.\nBased on the power curve plotted above, we can conclude that we would have a very high likelihood of detecting effects of .4 vs. a control group at even quite low sample sizes, and also a good possibility of detecting effect sizes of .2 at quite modest sample sizes. On the other hand, for the very small effect size, we would not be confident in detecting such a difference vs. a control group even at 1500 participants per group. If effect sizes of this size we what was expected in such an experiment, and it was crucial that they were detected if present, then we might consider going back to Step 1 and reconsidering our experimental design to include even more participants.\n\n\n\n\nSource Code\n# Power analysis workflow (Jamie Elsey) {#power-workflow .unnumbered}\n\n[^power_analysis_framework_2-1]\n\n[^power_analysis_framework_2-1]: By Jamie Elsley, incorporating some comments and suggestions from David Reinstein.\n\n```{r}\n#| include: false\n\nlibrary(here)\n\n# 1.  Load packages, some setup definitions -- need to run it in every qmd\nsource(here(\"code\", \"methods_setup.R\"))\n\n```\n\n## Discussion and framework\n\n### Introduction and goals\n\nThe purpose of this document is to propose the fundamentals for a workflow for power analyses.\n\nAlthough this started from a Bayesian paradigm, the overarching framework is applicable to any analytic approach, and I will begin with a standard frequentist analysis.[^power_analysis_framework_2-2]\n\n[^power_analysis_framework_2-2]: This will make the code easier to run as well. Note that as Bayesian power analyses can be very time consuming (at the moment/with our current setup), in some cases even if you ultimately might perform a Bayesian analysis, it may make some sense to run frequentist approaches first. In many cases the estimates from Bayesian and frequentist approaches will tend to converge. I (Jamie) suspect general estimates from frequentist approaches would be quite similar to those of Bayesian approaches, if the goal of the analysis (what you want to make an inference about) is the same. I will make an accompanying document with an example of a Bayesian power analysis that should be computationally feasible to accompany this.\n\nThe general approach laid out here can be the basis for developing and build a library of common analyses and inference goals.[^power_analysis_framework_2-3] Helpful future additions to the toolkit would include\n\n[^power_analysis_framework_2-3]: DR: we might want to incorporate declaredesign for this.\n\n-   clear and flexible ways to generate hypothetical data sets, and\n\n-   adding further analytic designs.\n\nThis is not intended as an exhaustive introduction to the fundamentals of statistical power--I assume you are reasonably well-versed in some general principles of statistical inference and hypothesis testing, as well as in the basic idea of what power analysis is.[^power_analysis_framework_2-4]\n\n[^power_analysis_framework_2-4]: DR: We can cover this in the earlier section, or link relevant explanations.\n\nWe will focus solely on 'simulation-based' power analysis, and not on ways of mathematically (analytically) deriving power.\n\n### Definition of power\n\nIn frequentist null hypothesis significance testing (NHST), power is typically defined as the probability that we can reject the null hypothesis, if the alternative hypothesis is indeed true (i.e., power is the 'true positive rate'). More precisely, we may express this as 'power against a *particular* alternative hypothesis'.[^power_analysis_framework_2-5]\n\n[^power_analysis_framework_2-5]: DR: may be worth putting maths here at some point, and also give a canonical reference.\n\n::: {.callout-note collapse=\"true\"}\n## From wikipedia: The statistical power of a binary hypothesis test...\n\nThe statistical power of a binary hypothesis test is the probability that the test correctly rejects the null hypothesis \\$H\\_{0}\\$ when a specific alternative hypothesis \\$H\\_{1}\\$ is true. It is commonly denoted by \\$1-\\beta\\$, and represents the chances of a \"true positive\" detection conditional on the actual existence of an effect to detect. ... as the power of a test increases, the probability \\$\\beta\\$ of making a type II error by wrongly failing to reject the null hypothesis decreases.\n\n[link accessed 12 Apr 2022](https://en.wikipedia.org/wiki/Power_of_a_test)\n:::\n\nNote that some Bayesian statisticians question the validity and usefulness of this presentation of power analysis.[^power_analysis_framework_2-6]\n\n[^power_analysis_framework_2-6]: This engages deep issues in the conception of probability. From the frequentist perspective you imagine a fixed underlying true parameter and consider the many data sets that might hypothetically arise from it - 'the true parameter is fixed and the data varies'. In Bayesian reasoning you note that you have 'uncertainty about the underlying parameter' but you  'consider the data as fixed'. A Bayesian approach does not tend to  consider 'the probability that the data arises in a particular way under a null hypothesis'; however 'Bayesian hypothesis testing' is discussed in the literature. (Presumably, it envisions 'rejecting a hypothesis' where the posterior probability that the hypothesis holds is sufficiently small).\n\n    \\\n\nBefore we conduct an experiment (or run a survey, or collect data for an analysis), there are many hypothetical future data sets we might observe. Whether we favor a frequentist or Bayesian approach, it seems reasonable to ask: *'Given the range of data that I might observe, how likely is it that I can make certain conclusions?'*. [^power_analysis_framework_2-7]\n\n[^power_analysis_framework_2-7]: The conclusion or inference one wishes to make from a data set can go far beyond simply saying something like \"there is a non-zero difference between groups\" (although this is probably the most common inference people first consider). Or a comparable statement posed in probabilistic terms...\n\n    Frequentist: 'if there had been a zero difference between these groups (H0), this data is unlikely to have been generated'.\n\n    Bayesian: 'the posterior distribution puts most of the probability mass on there being a difference between groups greater than (some moderate amount).'\n\nWe can broaden the idea of power to indicate the probability that our proposed sample yields information that allows us to make some specific kind of inference about the data-generating process.\n\nGiven a particular underlying effect (or range of effects, or lack thereof), and a particular sample size (or range of sample sizes), what is the probability of... :[^power_analysis_framework_2-8]\n\n[^power_analysis_framework_2-8]: Note that these could all be either Bayesian or frequentist, depending on how you specify your goals/inferences.\n\n-   'determining' that there is a non-zero difference between two conditions,[^power_analysis_framework_2-9]\n-   detecting some 'smallest effect size of interest',\n-   a 'false positive', i.e., the probability of concluding that there is a difference between groups when in fact there is no difference(the standard 'Type 1 error', aka the 'size' of a test),\n-   achieving a desired level of precision around a particular parameter estimate, or\n-    finding an effect 'likely equivalent to zero' or 'unlikely to be far from zero'.[^power_analysis_framework_2-10]\n\n[^power_analysis_framework_2-9]: .'Determining' is in scare quotes; ultimately, we are making probabilistic statements, either about the 'true effect size' ('we make the decision that, most probably, there is a non-zero difference') or about the likelihood of observing data like this under some conditions (frequentist: 'this data would be very unlikely if there were a zero effect').\n\n    \\\n\n[^power_analysis_framework_2-10]: This is sometimes called 'equivalence testing'. In Bayesian estimation, you can consider a 'range of practical equivalence' (ROPE). Here, if a large enough proportion of the posterior (of the difference between two groups, say) falls in this range, you conclude that 'to all intents and purposes these groups are equivalent.'\n\n    \\\n\n<!-- DR: does this encompass 'equivalence testing', which I think is very relevant? Also, should the first one be framed as ... 'when the true difference is XXX or larger'? Or is that only a frequentist concept?-->\n\nHence, a power analysis helps frame our prospective research project in relation to the goals we would like to achieve.[^power_analysis_framework_2-11] It provides us us with an estimate (and it is indeed an estimate, not a certainty!) of the probability that we will be able to make the inference we wish to make, given various factors both inside and outside of our control. (See discussion in fold.)\n\n[^power_analysis_framework_2-11]: See the 'diagnosands' of the `declaredesign` framework.\n\n::: {.callout-note collapse=\"true\"}\nWhat is 'in our control'? We might think of sample size as *in* our control, which it generally is, but this is typically limited by practical considerations outside of our control (such as the availability of a sample or financial constraints). Conversely, effect size is often considered *out* of our contro;; however we can sometimes increase the 'dosage' we give, or try to select a particular group of participants who might be particularly susceptible to our effect of interest.\n:::\n\n### General workflow for simulation-based power analysis\n\nSimulation-based power analysis proceeds in four primary steps:\n\n1.  Generate a large number of 'simulated data sets' for the analysis.[^power_analysis_framework_2-12]\n\n[^power_analysis_framework_2-12]: These may be generated by draws based on a canonical distribution, like the Gaussian ('normal') with particular parameters. Alternately, if prior 'similar' data is available (e.g., from outcomes in the year prior to a field experiment), we may prefer to generate it by resampling from this.\n\nThe data sets generated should be specific to the goal of the study. E.g., suppose we want to know the 'power (of a particular design and testing procedure) to detect an effect size of 0.2 SD'. Here we should generate data that reflects this effect size (for considering the true positive and false negative rates). If we also want to measure the rate of type-2 error, the rate of false positives (if the null, e.g., an effect side of 0, holds), we should also generate data reflecting this 'null effect'.\n\n2.  Run the proposed analysis over the many simulated data sets (as efficiently as possible, as this can take a long time). This analysis should either return the estimand of interest, or ensure that we can easily compute it (in Step 3). The output should be kept as flexible as possible (while conserving computer memory). This will allow us to assess multiple inference goals on the same output.[^power_analysis_framework_2-13]\n\n3.  Summarize the output returned in Step 2, computing the share of simulated data sets that meet different decision criteria or inference goals. (Example described in fold)\n\n[^power_analysis_framework_2-13]: E.g., in a Bayesian analysis, we can get the full posterior distribution for an analysis so it can be assessed and summarised in many ways in step 3.\n\n::: {.callout-note collapse=\"true\"}\nE.g., we might simulate 1000 data sets based on an effect size of 0.3 standard deviations and perform a standard t-test of the difference between treatment and control for each of these. We might then find that for 743 of 1000 of these simulated data, the test 'rejected the null hypothesis', suggesting a power of 74.3%.\n\n(Feel free to add other examples here.)\n:::\n\nThis can show the likelihood of achieving a range of goals given various sample sizes and design and testing choices, including the rates of *misleading* conclusions. In frequentist analyses, it is possible that steps 2 and 3 occur together (e.g., the p-value is returned along with the other output). In my experience with typical Bayesian designs, these stages are best kept separate so that we can first do the more time-consuming Step 2, and then more freely explore the various options in Step 3.\n\n4.  Assess the output and determine whether the proposed analyses and inference goals are realistic and likely to yield informative results. If not, one may need to think of alternative design or data-collection choices (including sample sizes, treatments, and treatment assignments) or inference goals, and return to Step 1.[^power_analysis_framework_2-14]\n\n[^power_analysis_framework_2-14]: DR: I removed 'ways of generating data' because I thought it could be misleading. It seems wrong to first say, e.g., 'we assumed the a standard normal distribution of outcomes with a standard deviation equal to that observed in prior trials'... and then say 'but that didn't have enough power so lets assume a more concentrated distribution'. What I think you meant was consider other ways of actually *collecting* data or setting up the experiment that would lead to a reasonable expectation of a different data generating process, and then simulate and diagnose this *new* approach. Note that DD seems to have good tools for comparing and considering design variations.\n\nIn the sections that follow, I will present some generally useful packages and functions for power analyses, including annotated code examples with particularly useful aspects highlighted. I will do this in the context of a frequentist power analysis for between-groups (=between-subject?) analysis.[^power_analysis_framework_2-15]\n\n[^power_analysis_framework_2-15]: We give this example is primarily because between-groups analyses run much more quickly than within- subjects analyses. Thus, these can be run without a timewasting headache on your own computer! We hope to add (and link or connect) further examples of common designs in future.\n\n### Possible pitfalls/misunderstandings of power analysis\n\nIt is important to note that power analyses are not an 'omniscient oracle'. They might better be termed 'power projections' or 'power estimates'. Even if we estimate that we have 99.9% power for detecting some a particular effect, we may have specified our simulations in an unrealistic way. When we actually run an experiment, we might see (e.g.) far more underlying variation or measurement error than we predicted, leading our analyses to be fairly uninformative. Conversely, we might be overly conservative with our power analysis; perhaps a design we thought was 'underpowered' actually has a high probability of producing very compelling results.[^power_analysis_framework_2-16]\n\n[^power_analysis_framework_2-16]: DR: I changed the wording here because if we diagnose the design as 'underpowered' we probably would choose not to run it in that form.\n\nI raise this only to point out that our power analyses might not always be perfect or be something of a blunt instrument. We should recognise they are not guarantees but more a way of determining the basic plausibility of achieving certain goals. Some other things to consider in a similar vein are presented below, and again the intention here is merely to frame our use of power analyses and recognise what sort of things I suspect they can or can't help us with:\n\n-   When we see real data and real interesting patterns emerge, we are likely to go further in modeling and investigating these patterns than some of the more simple analyses and comparisons we conduct in the initial power analysis. I suspect power analyses are not so good for determining all the intricate, in-depth things we might plumb in a dataset. They are probably better at assessing the tractability of a broad inference goal.\n\n-   We might want to give some consideration to how far we wish to go in the initial data simulation step to think about all sorts of hypothetical data sets. As models become more complex, the number of different parameters that might vary - with possible effects on power - starts to balloon. E.g., even in a simple repeated-measures example, do we wish to vary not only the effect size but all sorts of different correlations from pre- to post-treatment within subjects? If we are simulating ordinal data, then power might change depending on how we initially suggest binning the outcomes, but there are infinitely many ways we might think the data might look... Discussion and consideration of how far we should go with these things is welcome and could be useful! (DR: I'm not sure what you are getting at here. Are you saying that 'the space of designs and proposed analyses we can explore is extremely large, and we may need to make some ad-hoc choices to avoid this getting unmanageable'?)\n\n## Concrete implementation of framework\n\n### Step 1: Generate a large number of data sets for the analysis\n\n#### Confirm that we can generate the basic data we want {.unnumbered}\n\nBecause ultimately we might be generating some rather large data files, we increase the memory limit alotted to R first. I don't believe there is any real cost to increasing this memory limit, so it is wise to do so as to avoid a function iterating many times over for a long time, but ending up without sufficient space to store the outcome:\n\n```{r change-memory-limit, message = FALSE, warning = FALSE}\nmemory.limit(100000)\n```\n\nNow we want to generate a hypothetical data set. In this case, we will be comparing 4 groups, each shown a different message in a between-subjects design (3 active vs. a control condition with a neutral message). Willem has some nice procedures for generating repeated-measures data in his walkthrough (<https://www.willemsleegers.com/posts/simulation-based-power-analyses/>) using the library MASS. **I think there are likely to be other useful packages or techniques for generating simulated data for more complex designs or patterns of results that we could look into. Being better able to generate valid and informative data sets is probably one of the things that would improve our capacity for running informative power analyses the most.**\n\nIn this example, we will take advantage of the different groups to assess several different possible effect sizes. In a multiple regression, we would then be able to assess whether these different effect sizes come out as significantly different from our control group, and therefore our power to detect different sizes of effects.\n\n```{r load-tidyverse, message = FALSE, warning = FALSE}\n\n# tidyverse simply used for data wrangling and plotting\nlibrary(tidyverse)\n\n```\n\n```{r datamaker-function, message = FALSE, warning = FALSE}\n\nfour.group.datamaker <- function(sim = 1, a = 0, b = .1, c = .2, d = .4, pop=1500) {\n\n  # first a tibble (data frame) with 1500 ppts, with the different groups showing\n  # effect sizes in Cohen's d of .1, .2, and .4\n  four.groups <- tibble(a.control = rnorm(pop, a, 1),\n                        b.small = rnorm(pop, b, 1),\n                        c.medsmall = rnorm(pop, c, 1),\n                        d.medium = rnorm(pop, d, 1),\n                        sample.size = 1:pop) %>%\n\n    # turn the data into long form\n    pivot_longer(cols = 'a.control':'d.medium', names_to = 'group', values_to = 'response') %>%\n\n    # put cutpoints in the data to make it more similar to the ordinal responses we would get\n    mutate(ordinal = case_when(response < -1.5 ~ 1,\n                               response < -.5 ~ 2,\n                               response < .5 ~ 3,\n                               response < 1.5 ~ 4,\n                               response >= 1.5 ~ 5),\n\n           # for the purposes of this demo we will not analyse it as ordinal as it takes longer\n           # to run the regressions, but if you did so you would also want to make the response\n           # a factor\n           ordinal = as.factor(ordinal),\n           sim = sim)\n\n  return(four.groups)\n\n}\n\n# test that the function works to make one data set before making many!\ntest.data <- four.group.datamaker()\n\nggplot(data = test.data) +\n  geom_density(aes(x = response, fill = group), alpha = .3) +\n  theme(\n    aspect.ratio = .5\n  )\n\nggplot(data = test.data) +\n  geom_histogram(aes(x = as.numeric(ordinal)), alpha = .6,\n                 position = position_dodge(), bins = 10) +\n  facet_wrap(~group, nrow=1) +\n  theme(\n    aspect.ratio = 1\n  )\n\n```\n\nWe can see from the plots that the function appears to be working. When developing a data set for the first time, one would usually go further with some 'diagnostic' checks to confirm that the data is behaving as you intended. For example, in Willem's examples, he used the mvnorm function with 'empirical = TRUE', so that the exact mean diffreences you specified are present in the data. This can then be confirmed wiht descriptive statistics. There might be all sorts of other diagnostics or plots we might check with other types of data that we generate.\n\n#### Efficiently generate many data sets {.unnumbered}\n\nNow we just need to run the function above many times over. This could be done using loops, but a very useful set of R functions in the tidyverse is the purrr package of *map functions*. Even better, a package called furrr is available to run such map functions in parallel to further reduce time. This doesn't matter so much here because this will be quite quick anyway, but is important when we run the analyses over the many data sets. For furrr to do this, we need to tell it to plan for 'multisession', and give it a seed number:\n\n```{r load-furr, message = FALSE, warning = FALSE}\np_load(furrr)\nlibrary(furrr)\n\nplan(multisession)\noptions <- furrr_options(seed = 48238)\n```\n\n```{r run-datamaker-function, message = FALSE, warning = FALSE}\n\n# we will pass N = 500 simulations to the map function\nnsims <- 1:500\n\n# the map function will run our data-making function over 1000 simulations\nsim.data <- future_map_dfr(.x = nsims, .f = four.group.datamaker,\n                           a = 0, b = .1, c = .2, d = .4, .options = options)\n\n# split the simulated data into the separate simulations\nsim.data <- sim.data %>% group_by(sim) %>% group_split()\n\n```\n\nNow, we have 500 simulated data sets representing our hypothetical outcome data, and can perform analyses on them:\n\n```{r check-data, message = FALSE, warning = FALSE}\nhead(sim.data[[3]])\n```\n\n### Step 2: Run the proposed analysis over the many data sets and return the estimands of interest\n\nBefore we run an analysis over the many data sets, we need to check that our models will return the estimands that we wish to make inferences from. For a Bayesian analysis, this step might involve dropping parts of the posterior that are not relevant (e.g., estimates of every participant intercept, which take too much space for what they are worth) and ensuring we get the parts we care about (e.g., we might insert some additional code to retrieve a posterior distribution for Cohen's d, or simply the parameter estimate for the interaction). For Bayesian analyses I err on the side of getting as many of the main parameters as possible, because this is a very time consuming step.\n\nFor frequentist analyses, there are also a range of estimands we might care about. For example, we might consider returning a p-value, or the upper and lower bounds for confidence intervals, r-squared estimates etc. Again, this depends on the goals one wishes to achieve and what one wants to make inferences about.\n\nThe key point for any kind of analysis here is that you don't want to forget an estimand that might be of interest and then have to re-run the entire analysis. In addition, you want to have the regression run on different sample sizes of the data, so that you can generate a power curve plot for the analysis, showing how your power changes according to increasing the sample size.\n\nIn the example function below, we run a simple linear regression, predicting the response from group.\n\n```{r regression-function, message = FALSE, warning = FALSE}\n\nlinear.reg.maker <- function(data, breaks) {\n\n  # this function cuts the data set it is given into different sample sizes\n  cut.samples <- function(break.point, data) {\n    cut.data <- filter(data, sample.size <= break.point) %>%\n      mutate(sample.size = break.point)\n    return(cut.data)\n  }\n\n  data.cuts <- map_dfr(.x = breaks, .f = cut.samples, data = data)\n\n  # the data is split according to the sample size\n  # to feed to the regression model\n  data.cuts <- data.cuts %>% group_by(sample.size) %>% group_split()\n\n  # this function runs the regression\n  run.reg <- function(data) {\n\n    four.group.form <- as.numeric(ordinal) ~ 1 + group\n\n    four.group.reg <-\n      lm(formula = four.group.form,\n           data = data)\n\n    # we extract confidence intervals for the parameters of interest\n    ci99 <- confint(four.group.reg, level = .99)\n    ci95 <- confint(four.group.reg, level = .95)\n\n    # we create an output to show the confidence intervals around the effects\n    # and some additional inference info, e.g., 'nonzero' indicates whether\n    # the lower bound of the CI excludes 0 or not.\n    # 'width' indicates the width of the confidence interval,\n    # for assessment of precision\n    output <- tibble(group = c('small', 'medsmall', 'medium',\n                               'small', 'medsmall', 'medium'),\n                     interval = c(.99, .99, .99, .95, .95, .95),\n                     lower = c(ci99[[2,1]], ci99[[3,1]], ci99[[4,1]],\n                               ci95[[2,1]], ci95[[3,1]], ci95[[4,1]]),\n                     upper = c(ci99[[2,2]], ci99[[3,2]], ci99[[4,2]],\n                               ci95[[2,2]], ci95[[3,2]], ci95[[4,2]])) %>%\n      mutate('nonzero' = case_when(lower > 0 ~ 1,\n                                   TRUE ~ 0),\n             'width' = abs(upper - lower),\n             'sim' = data[[1, 'sim']],\n             'cell.size' = nrow(data)/4)\n\n    return(output)\n  }\n\n  # run the regression function over the different sample sizes\n  output <- map_df(.x = data.cuts, .f = run.reg)\n\n  return(output)\n}\n\n\n```\n\nOnce we have made and tested that our function works as intended and returns the values we want to make inferences from, we can run it over the many simluated data sets:\n\n```{r run-regression, message = FALSE, warning = FALSE}\n\nt1 <- Sys.time()\nlinreg.output <- future_map_dfr(.x = sim.data,\n                                .f = linear.reg.maker,\n                                breaks = seq(from = 150, to = 1500, by = 150))\nt2 <- Sys.time()\nt2 - t1\n\n```\n\nThe object 'linreg.output' is now a large dataframe, cataloguing whether or not certain inference thresholds were reached across the many simulations. In the third primary step, we can summarise and graphically display this information.\n\n### Step 3: Summarise the output returned in Step 2 to determine likelihood of achieving various inferential goals\n\nNow, we wish to ascertain how likely we are to achieve a range of inferential goals, depending on factors such as the sample size, the underlying effect sizes, or whatever else we varied in simulating our data and running our models. For this example, this is as simple as generating a summary of the output from Step 2:\n\n```{r summarise-output, message = FALSE, warning = FALSE}\n\n# group the data according to group, confidence interval, and size per group\nfour.group.lin.summary <- linreg.output %>% group_by(group, interval, cell.size) %>%\n  # summarise the amount of times we get a CI greater than 0\n  summarise(.groups = 'keep',\n            'ci above 0 vs. control' = sum(nonzero)/5)  %>%\n  # change some factors for plotting\n  mutate(interval = factor(interval, levels = c('0.95', '0.99'),\n                           labels = c('95% CI', '99% CI')),\n         'Effect size' = factor(group, levels = c('small', 'medsmall', 'medium'),\n                                labels = c('Very small (.1)', 'Small (.2)', 'Medium (.4)')))\n\n```\n\nAnd then plotting the resulting power curve:\n\n```{r plot-power-curve, message=FALSE, warning=FALSE}\n\nggplot(data = four.group.lin.summary) +\n  scale_x_continuous(limits = c(100, 1550), breaks = seq(from = 150, to = 1500, by = 150)) +\n  scale_y_continuous(limits = c(0, 100), breaks = seq(from = 0, to = 100, by = 20)) +\n  geom_hline(aes(yintercept = 80), linetype = 'dashed', size = .33, alpha = .25) +\n  geom_hline(aes(yintercept = 90), linetype = 'dashed', size = .33, alpha = .25) +\n  geom_path(aes(x = cell.size, y = `ci above 0 vs. control`, color = `Effect size`,\n                group = `Effect size`), size = .66) +\n  geom_point(aes(x = cell.size, y = `ci above 0 vs. control`, color = `Effect size`),\n             size = 1.5) +\n  labs(y = 'Power to detect a non-zero effect',\n       x = 'Number of participants per condition (control group not included)') +\n  scale_color_manual(values = c('#c10d0d', '#7dc3c2', '#dcc55b')) +\n  facet_wrap(~interval) +\n  theme(\n    aspect.ratio = 1,\n    panel.grid.major = element_line(colour = \"white\", size = 0.33),\n    panel.grid.minor = element_line(colour = \"white\", size = 0.2),\n    panel.background = element_rect(fill = \"grey96\"),\n    axis.line = element_line(color = 'black', size = 0.375),\n    axis.ticks = element_line(color = 'black', size = 0.5),\n    text = element_text(color = 'black', family = 'Gill Sans MT', size = 9),\n    axis.text = element_text(color = 'black', family = 'Gill Sans MT', size = 7),\n    strip.background = element_blank()\n  )\n\n```\n\n### Step 4. Assess the output and determine whether the proposed analyses and inference goals are realistic and likely to yield informative results.\n\nIn Step 4, we use the information we have generated above to make substantive conclusions about the projected power of our experiment to detect certain effects, given certain underlying parameters. From this we can make recommendations as to experimental design.\n\nBased on the power curve plotted above, we can conclude that we would have a very high likelihood of detecting effects of .4 vs. a control group at even quite low sample sizes, and also a good possibility of detecting effect sizes of .2 at quite modest sample sizes. On the other hand, for the very small effect size, we would not be confident in detecting such a difference vs. a control group even at 1500 participants per group. If effect sizes of this size we what was expected in such an experiment, and it was crucial that they were detected if present, then we might consider going back to Step 1 and reconsidering our experimental design to include even more participants."
  },
  {
    "objectID": "from_ea_market_testing/binary_trial_computations_redacted_ed.html",
    "href": "from_ea_market_testing/binary_trial_computations_redacted_ed.html",
    "title": "\n14  Inference and rough equivalence testing with binomial outcomes\n",
    "section": "",
    "text": "This work has been moved from the EA market testing repo, with ‘identifying’ content redacted\nThe context for this trial is discussed here (private) and will be publicly shared here when we gain permission."
  },
  {
    "objectID": "from_ea_market_testing/binary_trial_computations_redacted_ed.html#input-from-gsheet-wip",
    "href": "from_ea_market_testing/binary_trial_computations_redacted_ed.html#input-from-gsheet-wip",
    "title": "\n14  Inference and rough equivalence testing with binomial outcomes\n",
    "section": "\n14.1 Input from Gsheet (WIP)",
    "text": "14.1 Input from Gsheet (WIP)\n\nCode#library(googlesheets4)\n#read_sheet(\"XXX\")\n#Need authorization here; how to do it? Note -- we need those cells to be anonymized \n\n\nNote: subject lines were different … the impact email got more opens\n‘Giving Season contributions - website’ for donations … because CtA is a donation button on the website"
  },
  {
    "objectID": "from_ea_market_testing/binary_trial_computations_redacted_ed.html#randomization-inference-approach",
    "href": "from_ea_market_testing/binary_trial_computations_redacted_ed.html#randomization-inference-approach",
    "title": "\n14  Inference and rough equivalence testing with binomial outcomes\n",
    "section": "\n14.2 Randomization inference approach",
    "text": "14.2 Randomization inference approach"
  },
  {
    "objectID": "from_ea_market_testing/binary_trial_computations_redacted_ed.html#how-likely-is-the-imbalance-of-unique-emails-due-to-chance",
    "href": "from_ea_market_testing/binary_trial_computations_redacted_ed.html#how-likely-is-the-imbalance-of-unique-emails-due-to-chance",
    "title": "\n14  Inference and rough equivalence testing with binomial outcomes\n",
    "section": "\n14.3 How likely is the ‘imbalance of unique emails’ due to chance?",
    "text": "14.3 How likely is the ‘imbalance of unique emails’ due to chance?\nWe’ve 1345 unique emails in the dataset for ‘control’ treatment 1 and 1190 unique emails for treatment 2, if I did the quick calculations correctly. (Todo– integrate the data here.)\nWe know some emails are repeated. To analyze this correctly I should bring in the actual distribution of ‘number of times an email is in the data’ and simulate assignment. (Or maybe this is a ‘Poisson’ thing?)\nAs a (perhaps incorrect) first pass I could consider ‘legitimate unique email shows up in the treatment group’ as a random binomial event. (I guess the event is something like ’email is (not) duplicated?) Then we can do the standard binomial (Chi-sq) and Fisher’s exact tests.\n\nCoden1 <- 1345\nn2 <- 1190\n\n(\n  binom_emails <- prop.test(n1, n1+n2, correct=FALSE)\n)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  n1 out of n1 + n2, null probability 0.5\nX-squared = 9.4773, df = 1, p-value = 0.00208\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5111129 0.5499385\nsample estimates:\n       p \n0.530572 \n\nCode(\n  fisher_emails <- fisher.test(matrix(c(n1, n2, n2, n1), ncol=2))\n)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  matrix(c(n1, n2, n2, n1), ncol = 2)\np-value = 1.512e-05\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 1.142247 1.428639\nsample estimates:\nodds ratio \n  1.277394 \n\n\nIf the above tests are reasonable, this imbalance is very unlikely to occur by chance.\nNote: if we think this is opens, this is a demonstration that one email is opened more."
  },
  {
    "objectID": "from_ea_market_testing/binary_trial_computations_redacted_ed.html#how-likely-are-proportions-this-similar-under-different-size-true-effect-sizes",
    "href": "from_ea_market_testing/binary_trial_computations_redacted_ed.html#how-likely-are-proportions-this-similar-under-different-size-true-effect-sizes",
    "title": "\n14  Inference and rough equivalence testing with binomial outcomes\n",
    "section": "\n14.4 How likely are ‘proportions this similar’ under different size ‘true effect sizes’?1\n",
    "text": "14.4 How likely are ‘proportions this similar’ under different size ‘true effect sizes’?1\n\nIn the next section I started an off-the-cuff simulation approach, in folded code. But I believe that the analytical form here with code and a simulation here should be applicable.\n\n14.4.1 Difference between two binomial random variables.\nReprinting and discussing from Stackexchange post\n“derive the distribution of the difference between two binomial random variables.”\n\nI can give you an answer for the pmf of X-Y. From there |X - Y| is straightforward.\n\n\nSo we start with\n\\(X \\sim Bin(n_1, p_1)\\)\n\\(Y \\sim Bin(n_2, p_2)\\)\nWe are looking for the probability mass function of \\(Z=X-Y\\)\n\nNote: in our case we (arguably) care about the difference in ‘proportion of incidences’, but as we also have similar sample sizes (\\(n_1 \\approx n_2\\)) this is basically a normalization\n\nFirst note that the min and max of the support of Z must be \\((-n_2, n_1)\\) since that covers the most extreme cases (\\(X=0\\) and \\(Y=n_2\\)) and (\\(X=n_1\\) and \\(Y=0\\)).\nThen we need a modification of the binomial pmf so that it can cope with values outside of its support.\n\\(m(k, n, p) = \\binom {n} {k} p^k (1-p)^{n-k}\\) when \\(k \\leq n\\) and 0 otherwise.\n\nNote that this “modified binomial” is what the statistics::dbinom function in R returns.\n\nThen we need to define two cases\n\n\\(Z \\geq 0\\)\n\\(Z \\lt 0\\)\n\nIn the first case\n\\(p(z) = \\sum_{i=0}^{n_1} m(i+z, n_1, p_1) m(i, n_2, p_2)\\)\nsince this covers all the ways in which X-Y could equal z.\n\nBecause \\((i+z) - i = z\\), of course … ‘sum up’ the probability of each of these ‘co-occurances’\n\nFor example when z=1 this is reached when X=1 and Y=0 and X=2 and Y=1 and X=3 and Y=4 and so on. It also deals with cases that could not happen because of the values of \\(n_1\\) and \\(n_2\\). For example if \\(n_2 = 4\\) then we cannot get Z=1 as a combination of X=4 and Y=5. In this case thanks to our modified binomial pmf the probablity is zero.\nFor the second case we just reverse the roles. For example if z=-1 then this is reached when X=0 and Y=1, X=1 and Y=2 etc.\n\\(p(z) = \\sum_{i=0}^{n_2} m(i, n_1, p_1) m(i+z, n_2, p_2)\\)\nPut them together and that’s your pmf.\n\n\\(f(z)=\\)\n\\[\\begin{cases}\n    \\sum_{i=0}^{n_1} m(i+z, n_1, p_1) m(i, n_2, p_2),& \\text{if } z\\geq 0\\\\\n    \\sum_{i=0}^{n_2} m(i, n_1, p_1) m(i+z, n_2, p_2),              & \\text{otherwise}\n\\end{cases}\\]\n\nHere’s the function in R and a simulation to check it’s right (and it does work.) https://gist.github.com/ragscripts/9681819\n\nLet me try to apply it…\nDefining their code for this function: ::: {.cell}\nCodemodBin <-dbinom #DR: I just do this renaming here for consistency with the rest ... but the modBin they defined was redundant\n\ndiffBin<-function(z, n1, p1, n2, p2){\n\n  prob <- 0\n\n  if (z>=0){\n    for (i in 1:n1){\n      prob <- prob + modBin(i+z, n1, p1) * modBin(i, n2, p2)\n    }\n\n  }\n  else\n  {\n    for (i in 1:n2){\n      prob<-prob+modBin(i+z, n1, p1)*modBin(i, n2, p2)\n    }\n  }\n  return(prob)\n}\n:::\n\n14.4.2 Applying this to present data\nRather than using their example, I’ll dive right in to the present case.\nMy notes on the outcomes as of 15 Dec 2021\n\n\n\n\n\n\nNote\n\n\n\n\n\nTreatment 1 - Impact and Treatment 2 - Emotion Story tabs\nTreatment 1: We record - 8 unique emails donating, 26 donations in total, - worth 5200 USD in total - 1345 unique emails listed as getting ‘control’ treatment 1\nTreatment 2: - 6 unique emails, 28 donations so far — worth 7500 USD in total. - 1190 unique emails listed for treatment 2\nIf I believe my ‘unique emails count’, that implies an 0.59% ‘conversion’ rate for T1 - Control a 0.50% conversion rate for T2 - Emotion/Story\n\n\n\nPutting the observations into defined objects (later: do from data)\n\nCoden1 <- 1345\nn2 <- 1190\nd1 <- 8\nd2 <- 6\nz <- d1-d2\n\n\nComputation for a few ‘ad-hoc cases’ (later explore the space with vectors of values)\n\nSuppose truly equal incidence, at the mean level\n\n\nCodep1 <- (d1+d2)/(n1+n2)\n\np2 <- p1\n\n(\n  db_0 <- diffBin(z, n1, p1, n2, p2)\n)\n\n[1] 0.1024599\n\n\nThis implies there is a 10.2% chance of getting this exact difference of +2 incidences between the treatments (in one direction), if the true incidence rates were equal.\nLet’s plot this for a range of ‘incidence rate differences’ in this region. (Sorry, using the traditional plot, ggplot is better).\n\nCodes <- seq(-10*z, 10*z)\np<-sapply(s, function(z) diffBin(z, n1, p1, n2, p2))\nplot(s,p)\n\n\n\n\nWe see a large likelihood of values in the range of the +2 difference observed, and a low likelihood of a difference of 10 or more in either direction.\n\n14.4.3 Adaptation: ‘of this magnitude or smaller’\n\nCodeltmag_diffBin <- function(z, n1, p1, n2, p2){\n  prob <- 0\n  z_n <- -z #negative value\n\n  for (i in z_n:z){     #sum for all integer differences between observed value and its negative, inclusive\n    prob <- prob + diffBin(i, n1, p1, n2, p2)\n    }\n\n  return(prob)\n}\n\n\nNow, a similar computation as above, but for ‘this big or smaller in magnitude’:\n\nCode  (\n    mag_db_0 <- ltmag_diffBin(z, n1, p1, n2, p2)\n  )\n\n[1] 0.4908031\n\n\nThis implies there is a 49.1% chance of getting a difference no larger than this one in magnitude of +/-2 incidences between the treatments if the true incidence rates were equal.\n\nAnd finally, what we were looking for: the chance of ‘a difference this small or smaller’ as a function of the true difference…\nSet up an arbitrary vector of ‘true differences’ (to keep it simple, only change it in one direction for now …)\nBelow, I plot\nY-axis: ’how likely would a difference in donations ‘as small or smaller in magnitude’” than we see in the data against\nX-axis: if the “true difference in incidence rates” were of these magnitudes\n\nCodeoptions(scipen=999)\n\nB <- c(1, 1.5, 2, 2.5, 3, 4, 10)\n\np1 <- rep((d1+d2)/(n1+n2), length(B))\np2 <- p1*B\n\n\nas.list(ltmag_diffBin(z, n1, p1, n2, p2)*100) %>% format(digits=3, scientific=FALSE)\n\n[1] \"49.1\"             \"39.4\"             \"20.6\"             \"8.02\"            \n[5] \"2.51\"             \"0.153\"            \"0.00000000000855\"\n\nCodeprobmag <- ltmag_diffBin(z, n1, p1, n2, p2)\n\n\n#qplot(B, probmag, log  = \"x\", xlab = \"True relative incidence\", ylab =\"Prob. of difference this small\")\n\n(\n  probmag_plot <-\n    ggplot() +\n  aes(x=B, y=probmag) +\n  geom_point() +\n  scale_x_continuous(trans='log2') +\n    ylim(0,.51) +\n    xlab(\"True relative incidence rate\") +\n    ylab(\"Prob. diff. as small as obsd\")\n\n)\n\n\n\n\nHard-coded takeaways 15 Dec 2021 :\nOur data is consistent with ‘no difference’ (of course) … but its also consistent with ‘a fairly large difference in incidence’\nE.g., even if one treatment truly lead to ‘twice as many donations as the other’, we still have a 20% chance of seeing a differences as small as the one we see (of 8 versus 6)\nWe can reasonably ‘rule out’ differences of maybe 2.5x or greater\nMain point: given the rareness of donations in this context, our sample size doesn’t let us make very strong conclusions in either directions … at least not yet. I hope that combined with other evidence, we will be able to infer more\n\n\n\n\n\nSource Code\n# Inference and rough equivalence testing with binomial outcomes\n\n```{r}\n#| include: false\n\n\nlibrary(here)\n\n# 1.  Load packages, some setup definitions -- need to run it in every qmd\nsource(here(\"code\", \"methods_setup.R\"))\n\n```\n\n\n::: {.alert .alert-secondary}\n \n\nThis work has been moved from the EA market testing repo, with 'identifying' content redacted\n\n:::\n\n\nThe context for this trial is discussed [here (private)](https://app.gitbook.com/o/-MfFk4CTSGwVOPkwnRgx/s/-Mf8cHxdwePMZXRTKnEE/c/0qFJ2AUh7cZzynp3zRjk/contexts-and-environments-for-testing/one-for-the-world/pre-giving-tuesday-email-split-test) and will be publicly shared [here]() when we gain permission.\n\n## Input from Gsheet (WIP)\n\n```{r}\n#library(googlesheets4)\n#read_sheet(\"XXX\")\n#Need authorization here; how to do it? Note -- we need those cells to be anonymized \n```\n\n\n\nNote: subject lines were different ... the impact email got more opens\n\n'Giving Season contributions - website' for donations ... because CtA is a donation button on the website\n\n## Randomization inference approach \n\n## How likely is the 'imbalance of unique emails' due to chance?\n\nWe've 1345 unique emails in the dataset for ‘control’ treatment 1 and 1190 unique emails for treatment 2, if I did the quick calculations correctly. (Todo-- integrate the data here.)\n\nWe know some emails are repeated. To analyze this correctly I should bring in the actual distribution of 'number of times an email is in the data' and simulate assignment. (Or maybe this is a 'Poisson' thing?)\n\nAs a (perhaps incorrect) first pass I could consider 'legitimate unique email shows up in the treatment group' as a random binomial event. (I guess the event is something like 'email is  (not) duplicated?) Then we can do the standard binomial (Chi-sq) and Fisher's exact tests.\n\n```{r}\nn1 <- 1345\nn2 <- 1190\n\n(\n  binom_emails <- prop.test(n1, n1+n2, correct=FALSE)\n)\n\n(\n  fisher_emails <- fisher.test(matrix(c(n1, n2, n2, n1), ncol=2))\n)\n\n```\n\nIf the above tests are reasonable, this imbalance is very unlikely to occur by chance.\n\n\nNote: if we think this is opens, this is a demonstration that one email is opened more. \n\n\n\n\n## How likely are  'proportions this similar' under different size 'true effect sizes'?^[ I think this is relates to a 'power calculation', but reporting $1-power$.\n]\n\n\nIn the next section I started an off-the-cuff simulation approach, in folded code. But I believe that the *analytical* form [here](https://math.stackexchange.com/questions/562119/difference-of-two-binomial-random-variables) with code and a simulation [here](https://gist.github.com/coppeliaMLA/9681819) should be applicable.\n\n###  Difference between two binomial random variables.\n\n**Reprinting and discussing from [Stackexchange post](https://math.stackexchange.com/questions/562119/difference-of-two-binomial-random-variables)**\n\n\"derive the distribution of the difference between two binomial random variables.\"\n\n> I can give you an answer for the pmf of X-Y. From there \\|X - Y\\| is straightforward.\n\n> So we start with\n>\n> $X \\sim Bin(n_1, p_1)$\n>\n> $Y \\sim Bin(n_2, p_2)$\n>\n> We are looking for the probability mass function of $Z=X-Y$\n\n*Note:* in our case we (arguably) care about the difference in 'proportion of incidences', but as we also have similar sample sizes ($n_1 \\approx n_2$) this is basically a normalization\n\n> First note that the min and max of the support of Z must be $(-n_2, n_1)$ since that covers the most extreme cases ($X=0$ and $Y=n_2$) and ($X=n_1$ and $Y=0$).\n>\n> Then we need a modification of the binomial pmf so that it can cope with values outside of its support.\n>\n> $m(k, n, p) = \\binom {n} {k} p^k (1-p)^{n-k}$ when $k \\leq n$ and 0 otherwise.\n\nNote that this \"modified binomial\" *is* what the `statistics::dbinom` function in R returns.\n\n> Then we need to define two cases\n>\n> 1.  $Z \\geq 0$\n> 2.  $Z \\lt 0$\n>\n> In the first case\n>\n> $p(z) = \\sum_{i=0}^{n_1} m(i+z, n_1, p_1) m(i, n_2, p_2)$\n>\n> since this covers all the ways in which X-Y could equal z. \\\n\nBecause $(i+z) - i = z$, of course ... 'sum up' the probability of each of these 'co-occurances'\n\n> For example when z=1 this is reached when X=1 and Y=0 and X=2 and Y=1 and X=3 and Y=4 and so on. It also deals with cases that could not happen because of the values of $n_1$ and $n_2$. For example if $n_2 = 4$ then we cannot get Z=1 as a combination of X=4 and Y=5. In this case thanks to our modified binomial pmf the probablity is zero.\n>\n> For the second case we just reverse the roles. For example if z=-1 then this is reached when X=0 and Y=1, X=1 and Y=2 etc.\n>\n> $p(z) = \\sum_{i=0}^{n_2} m(i, n_1, p_1) m(i+z, n_2, p_2)$\n>\n> Put them together and that's your pmf.\n\n$f(z)=$\n\n```{=tex}\n\\begin{cases}\n    \\sum_{i=0}^{n_1} m(i+z, n_1, p_1) m(i, n_2, p_2),& \\text{if } z\\geq 0\\\\\n    \\sum_{i=0}^{n_2} m(i, n_1, p_1) m(i+z, n_2, p_2),              & \\text{otherwise}\n\\end{cases}\n```\n> Here's the function in R and a simulation to check it's right (and it does work.) <https://gist.github.com/ragscripts/9681819>\n\nLet me try to apply it...\n\nDefining their code for this function:\n```{r}\nmodBin <-dbinom #DR: I just do this renaming here for consistency with the rest ... but the modBin they defined was redundant\n\ndiffBin<-function(z, n1, p1, n2, p2){\n\n  prob <- 0\n\n  if (z>=0){\n    for (i in 1:n1){\n      prob <- prob + modBin(i+z, n1, p1) * modBin(i, n2, p2)\n    }\n\n  }\n  else\n  {\n    for (i in 1:n2){\n      prob<-prob+modBin(i+z, n1, p1)*modBin(i, n2, p2)\n    }\n  }\n  return(prob)\n}\n```\n\n### Applying this to present data\n\nRather than using their example, I'll dive right in to the present case.\n\nMy notes on the outcomes as of 15 Dec 2021\n\n::: {.callout-note collapse='true'}\n Treatment 1 - Impact and\nTreatment 2 - Emotion Story tabs\n\nTreatment 1: We record\n- 8 unique emails donating, 26 donations in total,\n- worth 5200 USD in total\n- 1345 unique emails listed as getting  ‘control’ treatment 1\n\nTreatment 2:\n-  6 unique emails, 28 donations so far\n—  worth 7500 USD in total.\n- 1190 unique emails listed for treatment 2\n\nIf I believe my ‘unique emails count’, that implies\nan 0.59% ‘conversion’ rate for T1 - Control\na 0.50% conversion rate for T2 - Emotion/Story\n \n \n:::\n\nPutting the observations into defined objects (later: do from data)\n\n```{r}\nn1 <- 1345\nn2 <- 1190\nd1 <- 8\nd2 <- 6\nz <- d1-d2\n\n\n```\n\nComputation for a few 'ad-hoc cases' (later explore the space with vectors of values)\n\n1. Suppose truly equal incidence, at the mean level\n\n```{r}\np1 <- (d1+d2)/(n1+n2)\n\np2 <- p1\n\n(\n  db_0 <- diffBin(z, n1, p1, n2, p2)\n)\n```\n\nThis implies there is a `r op(db_0*100)`% chance of getting this *exact* difference of +`r z` incidences between the treatments (in one direction), if the true incidence rates were equal.\n\nLet's plot this for a range of 'incidence rate differences' in this region. (Sorry, using the traditional plot, ggplot is better).\n\n```{r}\ns <- seq(-10*z, 10*z)\np<-sapply(s, function(z) diffBin(z, n1, p1, n2, p2))\nplot(s,p)\n```\n\nWe see a large likelihood of values in the range of the +`r z` difference observed, and a low likelihood of a difference of 10 or more in either direction.\n\n### Adaptation: 'of this magnitude or smaller'\n\n```{r}\nltmag_diffBin <- function(z, n1, p1, n2, p2){\n  prob <- 0\n  z_n <- -z #negative value\n\n  for (i in z_n:z){     #sum for all integer differences between observed value and its negative, inclusive\n    prob <- prob + diffBin(i, n1, p1, n2, p2)\n    }\n\n  return(prob)\n}\n```\n\nNow, a similar computation as above, but for 'this big or smaller in magnitude':\n\n```{r}\n  (\n    mag_db_0 <- ltmag_diffBin(z, n1, p1, n2, p2)\n  )\n```\n\nThis implies there is a `r op(mag_db_0*100)`% chance of getting a difference *no larger than this one in magnitude* of +/-`r z` incidences between the treatments if the true incidence rates were equal.\n\n\\\n\n**And finally, what we were looking for:** the chance of 'a difference this small or smaller' as a function of the **true difference...**\n\nSet up an arbitrary vector of 'true differences' (to keep it simple, only change it in one direction for now ...)\n\n\nBelow, I plot\n\nY-axis: ’how likely would a difference in donations ‘as small or smaller in magnitude’” than we see in the data  against\n\nX-axis: if the “true difference in incidence rates” were of these magnitudes\n\n```{r}\noptions(scipen=999)\n\nB <- c(1, 1.5, 2, 2.5, 3, 4, 10)\n\np1 <- rep((d1+d2)/(n1+n2), length(B))\np2 <- p1*B\n\n\nas.list(ltmag_diffBin(z, n1, p1, n2, p2)*100) %>% format(digits=3, scientific=FALSE)\n\n\nprobmag <- ltmag_diffBin(z, n1, p1, n2, p2)\n\n\n#qplot(B, probmag, log  = \"x\", xlab = \"True relative incidence\", ylab =\"Prob. of difference this small\")\n\n(\n  probmag_plot <-\n    ggplot() +\n  aes(x=B, y=probmag) +\n  geom_point() +\n  scale_x_continuous(trans='log2') +\n    ylim(0,.51) +\n    xlab(\"True relative incidence rate\") +\n    ylab(\"Prob. diff. as small as obsd\")\n\n)\n\n```\n\n\n\nHard-coded takeaways 15 Dec 2021 :\n\nOur data is consistent with ‘no difference’ (of course) ... but its also consistent with ‘a fairly large difference in incidence’\n\nE.g., even if one treatment truly lead to ‘twice as many donations as the other’, we still have a 20% chance of seeing a differences as small as the one we see (of 8 versus 6)\n\nWe can reasonably ‘rule out’ differences of maybe 2.5x or greater\n\n\nMain point: given the rareness of donations in this context, our sample size doesn’t let us make very strong conclusions in either directions … at least not yet. I hope that combined with other evidence, we will be able to infer more\n\n\n\n<!--\n## DR discussion of this case ... maybe obsolete\n\nWe get a 0.59% and a 0.50% conversion rate (share of emails who made at least one contribution in response) for control and treatment, respectively.\n\nHow likely would it be to have a 'difference in incidence this small or smaller' (either 0.09 pp or 0.09/0.5=0.18 proportional difference)...\n\nif the true differences in incidence were XXX or larger? (For now, assuming the actual incidence rate is 0.50 for the smaller-incidence treatment ... later we could have each of these follow a distribution.)\n\nHow?\n\nFor `num_sim` number of simulations - 'Control': $i=1..N$ draws from a binomial(0.005) - 'Treatment' N draws from a binomial(0.005\\*B) where B is the 'considered effect size' - Compute the `effect_i(B)` (the difference in share positive size ggof the simulated difference' for this simulation) - Compute: 'what share of simulations have a difference smaller in magnitude than the empirical one. I.e., \\$\\sum\\_i(\\|ES_i(B)\\| \\< \\|ES\\|) \\times \\frac{1}{N} \\$\n\nDo this for a range of considered effect sizes, e.g., $B \\in \\{0.5, 0.75, 1, 1.5, 2, 3, 4\\}$\n\n-->"
  }
]