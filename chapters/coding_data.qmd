# Coding, data  {#coding}

```{r setup}
library(pacman)
p_load(rethinkpriorities)
library(rethinkpriorities)
library(dplyr)
library(tibble)
```


## Some resources

[R for data science](https://r4ds.had.co.nz/): highly recommended

See [getting, cleaning, and using data](https://daaronr.github.io/metrics_discussion/data-sci.html) (Reinstein) ^[Todo: integrate key content.]


*Pete W's Gists curating...:*

- [Hadley Wickham's "Advanced R](https://gist.github.com/peterhurford/72dbd44e0a34e29297485a8cf679cf73) ... for the most keen

- ["Git 101" (various resources)](https://gist.github.com/peterhurford/4d43aa5d6de114c0c741ba664c9c5ff5)

## Coding and organisational issues

- Data protection (e.g., EA Survey data pre-2021 is not publicly shareable!)

- Good data management

- Reproducability

- Git and github

- R and Rmd

- How to leave comments and collaborate?

- `trackdown` to convert to Gdoc for feedback

- Folder structure, use of packages; esp `Renv`

- Functions etc pulled from `dr-rstuff` repo

- I (DR) love `lower_snake_case`

\

## Automation and 'dynamic documents' {-}

'Soft-code' as much as possible to avoid conflicting versions when data updates, and to make everything reproduceable and transparent

[Inline code in Rmd](https://bookdown.org/yihui/rmarkdown-cookbook/r-code.html) is great but it can be a double-edged sword.

Sometimes its better to 'do the important and complicated coding' in a chunk before this, not in the inline code itself because

- the 'bookdown' doesn't show the *code* generating the inline computation ... so a separate chunk makes it more transparent for external readers

- inline code isn't spaced well and its hard to read and debug.


## Data management

- Track it from its 'source'; use API to grab directly from Qualtrics (etc.) if possible

- A `main.R` file in the root directory should run everything

- Data import; external 'dictionary' can be helpful (see, e.g., [here](https://docs.google.com/spreadsheets/d/1dWy-CZxd9lzx0bLZ5ntmCSmwGPTrwjGcKpY4ORLom8E/edit#gid=0) for EAS integrated with Google sheet; R code [here](https://github.com/rethinkpriorities/ea-data/blob/master/build/fmt_label_with_dic_dhj_ok.R) brings it in

- import, cleaning, variable creation separate from analysis (unless its a very 'one-off-for-analysis' thing)
   - import and cleaning in `.R` rather than `.Rmd` perhaps

- 'raw' data in separate folder from 'munged' data

- `codebook` package -- make a codebook

- minimize 'versions' of the data frames ... code and use 'filter objects' instead
  - see ['lists of filters'](https://daaronr.github.io/metrics_discussion/data-sci.html#building-results-based-on-lists-of-filters-of-the-data-set) but actually defining the filter with `quo()` seems better.


## Standard cleaning steps

`janitor::remove_empty()` # removes empty rows and columns


## Naming columns and objects

`janitor::clean_names()` is a helpful shortcut to snake case


We sometimes input a 'dictionary' for renaming many columns at a time. ^[However, I don't think I found a tidy way to do the renaming, at least I can't remember it.]

`names(rename2020) <- eas_dictionary$true_name`




## Labelling columns

Some example code below




Put list of labels and renamings in objects in a separate location ... to avoid duplication and clutter:

```{r}

key_eas_all_labels <- c( #note these are converted to a list with as.list before assigning them
    donation_usd = "Donation (USD)",
    l_don_usd = "Log Don. (USD)",
    l_don_av_2yr = "Log Don. 'avg.'",
    ln_age = "Log age",
    don_av2_yr = "Don. 'avg'",
    donation_plan_usd = "Don. plan (USD)")
```

Variable labels are helpful

```{r}
eas_all %<>%
  labelled::set_variable_labels(.labels = as.list(key_eas_all_labels), .strict=FALSE)
```



## Simple summary tools I was not aware of


From Willem's [intro to R workshop script](https://docs.google.com/document/d/13jPMcG5m4oxinqEtlQ2tSqkQs4-JEUcC0ytz9woFG3E/edit)


```{r}

diamonds %>%
 count(cut) %>%
 mutate(pct = n / sum(n))


# Use tidystats
library(tidystats)

count_data(diamonds, cut)

diamonds %>%
 group_by(color) %>%
 count_data(cut)
describe_data(diamonds, price)

```

This one I knew, of course, the typical 'grouped summary's

```{r groupedsum}

diamonds %>%
 group_by(color) %>%
 summarize(
   M = mean(price),
   SD = sd(price),
   min = min(price)
 ) %>%
 .kable() %>% .kable_styling()

```
