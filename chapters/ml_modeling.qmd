# Prediction, machine learning, and 'ml-style modeling' {-#modeling}


See [Vignette: Modeling donations in EA Survey data with Tidymodels and workflow](modeling_vignettes/eas_ml_modeling_vignette.html)


## Prediction models for insights? {#pred-insights}


::: {.callout-note collapse="true"}

## Discussion: why build 'predictive models of donations', for example?

We focus on predicting the individual's donation in a year, focusing on the same set of outcomes used in the previous section. For this model to be useful for an *actual* prediction problem going forward, it would need to rely on 'ex-ante' characteristic that were already observable at the time of a career/EtG/pledge decision.^[An alternate project might try to predict future total EA donations in total in subsequent years and decades. This could embody both a prediction problem for individuals and uncertainties at the aggregate level. This is even further from what we are doing here, but seems worthwhile for future work, combining the EA survey with other measures and assessments.]  These might include immutable demographics, career plans, and pledges previously taken, and consider year and trend effects.

Although we have these models in mind, this is not what we are doing here. We are not posing a specific 'prediction problem' per se. Instead we are using machine learning tools built for prediction problems to generate 'data-driven insights' about factors related to EA donation behavior. Here, we do not than directly specifying all of the included components of the model (features, interaction terms, etc.). Instead we provide a large set of possible 'inputs' and use ML techniques to train models that should predict well *outside of the data they are trained on*. These models should do a good job of accomplishing the task: 'if you gave me a set of features of an EA, I should have a fairly accurate guess at what they will donate.'

The insights from these models should also be treated with caution. Again, they may not be deriving causal relationships. Furthermore, the parameters derived from model-fitting ML procedures are not in general unbiased or consistent, and it is difficult to derive proper confidence intervals for these parameters.

Still, the benefit of this exercise may be considered 'the derivation of robust and predictive relationships in the data that are mainly driven by the data itself, rather than our preconcieved ideas.' These models may also be useful building blocks towards future predictive work.
:::

:::


## Penalized regression models {#penalized-reg}

::: {.callout-note collapse="true"}

## Discussion: 'elastic net' models:

In brief, the *elastic net* models involve linear models (log-linear in
our case), i.e., '*regressions*', that carefully 'penalize' the
(squared) magnitude of coefficients, in effect shrinking these towards
zero. The penalties are specifically 'tuned' and 'validated' to maximize
the predictive power of the model. As these are essentially regression
approaches, we can report the sign and magnitude of the coefficients
used in the 'optimally tuned' predictive model. (However, we should be
careful about interpreting these parameters, and statistical inference
is challenging. See e.g., @Mullainathan2017 for a detailed discussion.)



The GLMnet approach combines 'ridge (L2 norm) and lasso (L2 norm)', tuning the mix of each, as well as tuning the penalization parameters *within* each.


Both the sum of absolute value coefficients (L1 norm) and the summed square of these (L2 norm) ma   y be penalized. Thus, these will be 'shrunk towards zero' relative to a comparable OLS (standard linear model)
 estimate. How much do we 'charge' for the absolute or squared sums? This is 'tuned', i.e., we see what combinations perform best in the cross-validation exercise.

In some cases (because of the L1 norm), coefficients may be dropped entirely, i.e., 'shrunk all the way to zero'.^[Note that the Lasso is often chosen because it sometimes drops coefficients, making the model more 'parsimonious', thus perhaps easier to describe and use. But the Lasso is *not* specifically designed as a 'variable selection procedure'. Running Lasso and then naively using the 'variables not shrunk to zero' in further standard linear models or other procedures may not be optimal. But see the 'lazy lasso' approach.]

:::


## Resources and use-cases

- Oska ML notes (Notion)

- [Predictive models of donations in EA Survey](https://rethinkpriorities.github.io/ea_data_public/eas_donations.html#predictive-models)
   - Code to fit models in [this folder](https://github.com/rethinkpriorities/ea-data/tree/master/analysis/predictive)
   - ... uses [tidymodels](https://www.tidymodels.org/) including [workflow_set](https://workflowsets.tidymodels.org/) and 'recipes' and the [parsnip](https://www.tidyverse.org/blog/2018/11/parsnip-0-0-1/) package

- General intuitive discussion: "Data Science for Business by Foster Provost and Tom Fawcett (2013)"; DR notes [here](https://daaronr.github.io/metrics_discussion/n-ds4bs.html) and [here](https://daaronr.github.io/metrics_discussion/control-ml.html)


