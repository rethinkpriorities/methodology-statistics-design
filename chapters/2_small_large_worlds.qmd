# Ch 2. Small/Large Worlds


##  2.1.

> The small world is the self-contained logical world of the model.

> The way that Bayesian models learn from evidence is arguably optimal in the small world. When their assumptions approximate reality, they also perform well in the large world. But large world performance has to be demonstrated rather than logically deduced.

> This demonstrates that there are three (out of 64) ways for a bag containing [combo of blue and white marbles] to produce the data .  The inferential power comes from comparing this count to the numbers of ways each of the other conjectures of the bag's contents could produce the same data.

> .... can be computed just by multiplying the new count by the old count. This updating approach amounts to nothing more than asserting that (1) when we have previous information suggesting there are $W_prior$ ways for a conjecture to produce a previous observation $D_{prior}$ and (2) we acquire new observations $D_{new}$ that the same conjecture can produce in $W_{new}$ ways, then (3) the number of ways the conjecture can account for both $D_{prior}$ as well as $D_{new}$ is just the product $W_{prior} \times  W_{new}$.

> This is sometimes known as the principle of indifference: When there is no reason to say that one conjecture is more plausible than another, weigh all of the conjectures equally. This book does not use nor endorse "ignorance” priors. As we’ll see in later chapters, the structure of the model and the scientific context always provide information that allows us to do better than ignorance

## 2.2. Building a model

> Designing a simple Bayesian model benefits from a design loop with three steps.
> (1) Data story: Motivate the model by narrating how the data might arise.
> (2) Update: Educate your model by feeding it the data.
> (3) Evaluate: All statistical models require supervision, leading to model revision

*DR question*: How can we 'revise the model' without overfitting or otherwise cheating in some way that overstates the confidence we should have in our results?

> The maximum height of the curve increases with each sample, meaning that fewer values of $p$ amass more plausibility as the amount of evidence increases

**Power of Bayesian inference in small-sample contexts**

> Why? In non-Bayesian statistical inference, procedures are often justified by the method's behavior at very large sample sizes, so-called asymptotic behavior. As a result, performance at small samples sizes is questionable. In contrast, Bayesian estimates are valid for any sample size. This does not mean that more data isn't helpful—it certainly is. Rather, the estimates have a clear and valid interpretation, no matter the sample size. But the price for this power is dependency upon the initial plausibilities, the prior. If the prior is a bad one, then the resulting inference will be misleading.

*DR note*: There are some frequentist/non-Bayesian procedures and tests that don't rely on large sample approximations; e.g., Fisher's exact test

### 2.2.1 - the 'globe tossing' data story


## 2.3. Components of the model


::: {.callout-note collapse="true"}
## Coding tip: `dbinom`, `pbinom`, `rbinom`, etc

> "d" in `dbinom` stands for density. Functions named in this way almost always have corresponding partners that begin with "r" for random samples and that begin with "-p" for cumulative probabilities

:::

> The distributions we assign to the observed variables typically have their own variables.


### What prior? {-}
> So where do priors come from? They are both engineering assumptions, chosen to help the machine learn, and scientific assumptions, chosen to reflect what we know about a phenomenon. The flat prior in Figure 2.5 is very common, but it is hardly ever the best prior.

> There is a school of Bayesian inference that emphasizes choosing priors based upon the personal beliefs of the analyst.  While this subjective Bayesian approach thrives in some statistics and philosophy and economics programs, it is rare in the sciences.

> If your goal is to lie with statistics, you’d be a fool to do it with priors, because such a lie would be easily uncovered. Better to use the more opaque machinery of the likelihood. Or better yet-don’t actually take this advice!—massage the data, drop some “outliers,” and otherwise engage in motivated data transformation

> because non-Bayesian procedures need to make choices that Bayesian ones do not, such as choice of estimator or likelihood penalty


## 2.4 'Making the model go'


### 2.4.1 Bayes theorem {-}
In word form:

> Posterior (probability of any given value of $p$) =

(Probability of the data [given p] $\times$ [Prior probability of p]) divided by the
'Average probability of the data'

I.e., (in my own words) 'how likely is this data *and* the particular parameter p'
divided by 'the probability of this data overall' (given any p, with the probability of each p following the prior)

> 'average probability of the data'

> Averaged over what? Averaged over the prior. It's job is just to standardize the posterior, to ensure it sums (integrates) to one. In mathematical form: Pr(W, L) = E


Probability of one Water followed by one Land:

$$Pr(W, L|p)$$
$$=E\Big( Pr(W, L|p) Big)$$

$$= \int  Pr(W,L|p) Pr(p)dp $$

> The key lesson is that the posterior is proportional to the product of the prior and the probability of the data [given the prior]. Why? Because for each specific value of p, the number of paths through the garden of forking data is the product of the prior number of paths and the new number of paths.
> A flat prior constructs a posterior that is simply proportional to the likelihood


::: {.callout-note collapse="true"}
## "Bayesian data analysis isn’t about Bayes’ theorem"

Dissing the 'HIV test false positive' thing

> Inference under any probability concept will eventually make use of Bayes’ theorem. Common introductory examples of 'Bayesian' analysis using HIV and DNA testing are not  uniquely Bayesian
:::

> Numerical techniques for computing posterior distributions: (1) Grid approximation (2) Quadratic approximation (3) Markov chain Monte Carlo (MCMC)

> Grid approximation: Basically mechanical Bayesian updating of the probability of a parameter value being in a range, dividing up the space of possible parameters into different ranges. (And then smoothing?)

> ... achieve an excellent approximation of the continuous posterior distribution by considering only a finite grid of parameter values

> in most of your real modeling, grid approximation isn't practical. The reason is that it scales very poorly, as the number of parameters increases

> Under quite general conditions, the region near the peak of the posterior distribution will be nearly Gaussian-or “normal”—in shape. This means the posterior distribution can be usefully approximated by a Gaussian distribution. A Gaussian distribution is convenient, because it can be completely described by only two numbers: the location of its center (mean) and its spread (variance) 】

> logarithm of a Gaussian distribution forms a parabola. And a parabola is a quadratic function

> For many of the most common procedures in applied statistics-linear regression, for example—the approximation works very well

> (1) Find the posterior mode

some optimization algorithm, a procedure that virtually “climbs” the posterior distribution

> (2) estimate the curvature near the peak. This curvature is sufficient to compute a quadratic approximation of the entire posterior distribution. In some cases, these calculations can be done analytically, but 】


*DR question:* But how does it do this estimate of the curvature? Does it come out of many simulations, or the optimizing hill climbing thing, or??


